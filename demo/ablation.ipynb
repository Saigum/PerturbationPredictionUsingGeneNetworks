{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Original Model Utils:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/saigum/PerturbationPredictionUsingGeneNetworks/perturb/lib/python3.10/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: The TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\n",
      "  warnings.warn(problem)\n"
     ]
    }
   ],
   "source": [
    "from gears import PertData\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import pickle\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## utils\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import sys, os\n",
    "import requests\n",
    "from torch_geometric.data import Data\n",
    "from zipfile import ZipFile\n",
    "import tarfile\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "from dcor import distance_correlation\n",
    "from multiprocessing import Pool\n",
    "import scanpy as sc\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch_geometric.nn import SGConv\n",
    "\n",
    "def parse_single_pert(i):\n",
    "    a = i.split('+')[0]\n",
    "    b = i.split('+')[1]\n",
    "    if a == 'ctrl':\n",
    "        pert = b\n",
    "    else:\n",
    "        pert = a\n",
    "    return pert\n",
    "\n",
    "def parse_combo_pert(i):\n",
    "    return i.split('+')[0], i.split('+')[1]\n",
    "\n",
    "def combine_res(res_1, res_2):\n",
    "    res_out = {}\n",
    "    for key in res_1:\n",
    "        res_out[key] = np.concatenate([res_1[key], res_2[key]])\n",
    "    return res_out\n",
    "\n",
    "def parse_any_pert(p):\n",
    "    if ('ctrl' in p) and (p != 'ctrl'):\n",
    "        return [parse_single_pert(p)]\n",
    "    elif 'ctrl' not in p:\n",
    "        out = parse_combo_pert(p)\n",
    "        return [out[0], out[1]]\n",
    "\n",
    "def np_pearson_cor(x, y):\n",
    "    xv = x - x.mean(axis=0)\n",
    "    yv = y - y.mean(axis=0)\n",
    "    xvss = (xv * xv).sum(axis=0)\n",
    "    yvss = (yv * yv).sum(axis=0)\n",
    "    result = np.matmul(xv.transpose(), yv) / np.sqrt(np.outer(xvss, yvss))\n",
    "    # bound the values to -1 to 1 in the event of precision issues\n",
    "    return np.maximum(np.minimum(result, 1.0), -1.0)\n",
    "\n",
    "def dataverse_download(url, save_path):\n",
    "    \"\"\"\n",
    "    Dataverse download helper with progress bar\n",
    "\n",
    "    Args:\n",
    "        url (str): the url of the dataset\n",
    "        path (str): the path to save the dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    if os.path.exists(save_path):\n",
    "        print_sys('Found local copy...')\n",
    "    else:\n",
    "        print_sys(\"Downloading...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        total_size_in_bytes= int(response.headers.get('content-length', 0))\n",
    "        block_size = 1024\n",
    "        progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "        with open(save_path, 'wb') as file:\n",
    "            for data in response.iter_content(block_size):\n",
    "                progress_bar.update(len(data))\n",
    "                file.write(data)\n",
    "        progress_bar.close()\n",
    "        \n",
    "def zip_data_download_wrapper(url, save_path, data_path):\n",
    "    \"\"\"\n",
    "    Wrapper for zip file download\n",
    "\n",
    "    Args:\n",
    "        url (str): the url of the dataset\n",
    "        save_path (str): the path where the file is donwloaded\n",
    "        data_path (str): the path to save the extracted dataset\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        print_sys('Found local copy...')\n",
    "    else:\n",
    "        dataverse_download(url, save_path + '.zip')\n",
    "        print_sys('Extracting zip file...')\n",
    "        with ZipFile((save_path + '.zip'), 'r') as zip:\n",
    "            zip.extractall(path = data_path)\n",
    "        print_sys(\"Done!\")  \n",
    "        \n",
    "def tar_data_download_wrapper(url, save_path, data_path):\n",
    "    \"\"\"\n",
    "    Wrapper for tar file download\n",
    "\n",
    "    Args:\n",
    "        url (str): the url of the dataset\n",
    "        save_path (str): the path where the file is donwloaded\n",
    "        data_path (str): the path to save the extracted dataset\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        print_sys('Found local copy...')\n",
    "    else:\n",
    "        dataverse_download(url, save_path + '.tar.gz')\n",
    "        print_sys('Extracting tar file...')\n",
    "        with tarfile.open(save_path  + '.tar.gz') as tar:\n",
    "            tar.extractall(path= data_path)\n",
    "        print_sys(\"Done!\")  \n",
    "        \n",
    "def get_go_auto(gene_list, data_path, data_name):\n",
    "    \"\"\"\n",
    "    Get gene ontology data\n",
    "\n",
    "    Args:\n",
    "        gene_list (list): list of gene names\n",
    "        data_path (str): the path to save the extracted dataset\n",
    "        data_name (str): the name of the dataset\n",
    "\n",
    "    Returns:\n",
    "        df_edge_list (pd.DataFrame): gene ontology edge list\n",
    "    \"\"\"\n",
    "    go_path = os.path.join(data_path, data_name, 'go.csv')\n",
    "    \n",
    "    if os.path.exists(go_path):\n",
    "        return pd.read_csv(go_path)\n",
    "    else:\n",
    "        ## download gene2go.pkl\n",
    "        if not os.path.exists(os.path.join(data_path, 'gene2go.pkl')):\n",
    "            server_path = 'https://dataverse.harvard.edu/api/access/datafile/6153417'\n",
    "            dataverse_download(server_path, os.path.join(data_path, 'gene2go.pkl'))\n",
    "        with open(os.path.join(data_path, 'gene2go.pkl'), 'rb') as f:\n",
    "            gene2go = pickle.load(f)\n",
    "\n",
    "        gene2go = {i: list(gene2go[i]) for i in gene_list if i in gene2go}\n",
    "        edge_list = []\n",
    "        for g1 in tqdm(gene2go.keys()):\n",
    "            for g2 in gene2go.keys():\n",
    "                edge_list.append((g1, g2, len(np.intersect1d(gene2go[g1],\n",
    "                   gene2go[g2]))/len(np.union1d(gene2go[g1], gene2go[g2]))))\n",
    "\n",
    "        edge_list_filter = [i for i in edge_list if i[2] > 0]\n",
    "        further_filter = [i for i in edge_list if i[2] > 0.1]\n",
    "        df_edge_list = pd.DataFrame(further_filter).rename(columns = {0: 'gene1',\n",
    "                                                                      1: 'gene2',\n",
    "                                                                      2: 'score'})\n",
    "\n",
    "        df_edge_list = df_edge_list.rename(columns = {'gene1': 'source',\n",
    "                                                      'gene2': 'target',\n",
    "                                                      'score': 'importance'})\n",
    "        df_edge_list.to_csv(go_path, index = False)        \n",
    "        return df_edge_list\n",
    "\n",
    "class GeneSimNetwork():\n",
    "    \"\"\"\n",
    "    GeneSimNetwork class\n",
    "\n",
    "    Args:\n",
    "        edge_list (pd.DataFrame): edge list of the network\n",
    "        gene_list (list): list of gene names\n",
    "        node_map (dict): dictionary mapping gene names to node indices\n",
    "\n",
    "    Attributes:\n",
    "        edge_index (torch.Tensor): edge index of the network\n",
    "        edge_weight (torch.Tensor): edge weight of the network\n",
    "        G (nx.DiGraph): networkx graph object\n",
    "    \"\"\"\n",
    "    def __init__(self, edge_list, gene_list, node_map):\n",
    "        \"\"\"\n",
    "        Initialize GeneSimNetwork class\n",
    "        \"\"\"\n",
    "\n",
    "        self.edge_list = edge_list\n",
    "        self.G = nx.from_pandas_edgelist(self.edge_list, source='source',\n",
    "                        target='target', edge_attr=['importance'],\n",
    "                        create_using=nx.DiGraph())    \n",
    "        self.gene_list = gene_list\n",
    "        for n in self.gene_list:\n",
    "            if n not in self.G.nodes():\n",
    "                self.G.add_node(n)\n",
    "        \n",
    "        edge_index_ = [(node_map[e[0]], node_map[e[1]]) for e in\n",
    "                      self.G.edges]\n",
    "        self.edge_index = torch.tensor(edge_index_, dtype=torch.long).T\n",
    "        #self.edge_weight = torch.Tensor(self.edge_list['importance'].values)\n",
    "        \n",
    "        edge_attr = nx.get_edge_attributes(self.G, 'importance') \n",
    "        importance = np.array([edge_attr[e] for e in self.G.edges])\n",
    "        self.edge_weight = torch.Tensor(importance)\n",
    "\n",
    "def get_GO_edge_list(args):\n",
    "    \"\"\"\n",
    "    Get gene ontology edge list\n",
    "    \"\"\"\n",
    "    g1, gene2go = args\n",
    "    edge_list = []\n",
    "    for g2 in gene2go.keys():\n",
    "        score = len(gene2go[g1].intersection(gene2go[g2])) / len(\n",
    "            gene2go[g1].union(gene2go[g2]))\n",
    "        if score > 0.1:\n",
    "            edge_list.append((g1, g2, score))\n",
    "    return edge_list\n",
    "        \n",
    "def make_GO(data_path, pert_list, data_name, num_workers=25, save=True):\n",
    "    \"\"\"\n",
    "    Creates Gene Ontology graph from a custom set of genes\n",
    "    \"\"\"\n",
    "\n",
    "    fname = './data/go_essential_' + data_name + '.csv'\n",
    "    if os.path.exists(fname):\n",
    "        return pd.read_csv(fname)\n",
    "\n",
    "    with open(os.path.join(data_path, 'gene2go_all.pkl'), 'rb') as f:\n",
    "        gene2go = pickle.load(f)\n",
    "    #perturbation_list\n",
    "    gene2go = {i: gene2go[i] for i in pert_list}\n",
    "\n",
    "    print('Creating custom GO graph, this can take a few minutes')\n",
    "    with Pool(num_workers) as p:\n",
    "        all_edge_list = list(\n",
    "            tqdm(p.imap(get_GO_edge_list, ((g, gene2go) for g in gene2go.keys())),\n",
    "                      total=len(gene2go.keys())))\n",
    "    edge_list = []\n",
    "    for i in all_edge_list:\n",
    "        edge_list = edge_list + i\n",
    "\n",
    "    df_edge_list = pd.DataFrame(edge_list).rename(\n",
    "        columns={0: 'source', 1: 'target', 2: 'importance'})\n",
    "    \n",
    "    if save:\n",
    "        if(data_path is not None):\n",
    "            fname = os.path.join(data_path,f\"go_essential{data_name}.csv\")\n",
    "        print(f'Saving edge_list to file {fname}')\n",
    "        df_edge_list.to_csv(fname, index=False)\n",
    "\n",
    "    return df_edge_list\n",
    "\n",
    "def get_similarity_network(network_type, adata, threshold, k,\n",
    "                           data_path, data_name, split, seed, train_gene_set_size,\n",
    "                           set2conditions, default_pert_graph=True, pert_list=None):\n",
    "    \n",
    "    if network_type == 'co-express':\n",
    "        df_out = get_coexpression_network_from_train(adata, threshold, k,\n",
    "                                                     data_path, data_name, split,\n",
    "                                                     seed, train_gene_set_size,\n",
    "                                                     set2conditions)\n",
    "    elif network_type == 'go':\n",
    "        if default_pert_graph:\n",
    "            server_path = 'https://dataverse.harvard.edu/api/access/datafile/6934319'\n",
    "            tar_data_download_wrapper(server_path, \n",
    "                                     os.path.join(data_path, 'go_essential_all'),\n",
    "                                     data_path)\n",
    "            df_jaccard = pd.read_csv(os.path.join(data_path, \n",
    "                                     'go_essential_all/go_essential_all.csv'))\n",
    "\n",
    "        else:\n",
    "            df_jaccard = make_GO(data_path, pert_list, data_name)\n",
    "\n",
    "        df_out = df_jaccard.groupby('target').apply(lambda x: x.nlargest(k + 1,\n",
    "                                    ['importance'])).reset_index(drop = True)\n",
    "\n",
    "    return df_out\n",
    "\n",
    "def get_coexpression_network_from_train(adata, threshold, k, data_path,\n",
    "                                        data_name, split, seed, train_gene_set_size,\n",
    "                                        set2conditions):\n",
    "    \"\"\"\n",
    "    Infer co-expression network from training data\n",
    "\n",
    "    Args:\n",
    "        adata (anndata.AnnData): anndata object\n",
    "        threshold (float): threshold for co-expression\n",
    "        k (int): number of edges to keep\n",
    "        data_path (str): path to data\n",
    "        data_name (str): name of dataset\n",
    "        split (str): split of dataset\n",
    "        seed (int): seed for random number generator\n",
    "        train_gene_set_size (int): size of training gene set\n",
    "        set2conditions (dict): dictionary of perturbations to conditions\n",
    "    \"\"\"\n",
    "    \n",
    "    fname = os.path.join(os.path.join(data_path, data_name), split + '_'  +\n",
    "                         str(seed) + '_' + str(train_gene_set_size) + '_' +\n",
    "                         str(threshold) + '_' + str(k) +\n",
    "                         '_co_expression_network.csv')\n",
    "    \n",
    "    if os.path.exists(fname):\n",
    "        return pd.read_csv(fname)\n",
    "    else:\n",
    "        gene_list = [f for f in adata.var.gene_name.values]\n",
    "        idx2gene = dict(zip(range(len(gene_list)), gene_list)) \n",
    "        X = adata.X\n",
    "        train_perts = set2conditions['train']\n",
    "        X_tr = X[np.isin(adata.obs.condition, [i for i in train_perts if 'ctrl' in i])]\n",
    "        gene_list = adata.var['gene_name'].values\n",
    "\n",
    "        X_tr = X_tr.toarray()\n",
    "        out = np_pearson_cor(X_tr, X_tr)\n",
    "        out[np.isnan(out)] = 0\n",
    "        out = np.abs(out)\n",
    "\n",
    "        out_sort_idx = np.argsort(out)[:, -(k + 1):]\n",
    "        out_sort_val = np.sort(out)[:, -(k + 1):]\n",
    "\n",
    "        df_g = []\n",
    "        for i in range(out_sort_idx.shape[0]):\n",
    "            target = idx2gene[i]\n",
    "            for j in range(out_sort_idx.shape[1]):\n",
    "                df_g.append((idx2gene[out_sort_idx[i, j]], target, out_sort_val[i, j]))\n",
    "\n",
    "        df_g = [i for i in df_g if i[2] > threshold]\n",
    "        df_co_expression = pd.DataFrame(df_g).rename(columns = {0: 'source',\n",
    "                                                                1: 'target',\n",
    "                                                                2: 'importance'})\n",
    "        df_co_expression.to_csv(fname, index = False)\n",
    "        return df_co_expression\n",
    "    \n",
    "def filter_pert_in_go(condition, pert_names):\n",
    "    \"\"\"\n",
    "    Filter perturbations in GO graph\n",
    "\n",
    "    Args:\n",
    "        condition (str): whether condition is 'ctrl' or not\n",
    "        pert_names (list): list of perturbations\n",
    "    \"\"\"\n",
    "\n",
    "    if condition == 'ctrl':\n",
    "        return True\n",
    "    else:\n",
    "        cond1 = condition.split('+')[0]\n",
    "        cond2 = condition.split('+')[1]\n",
    "        num_ctrl = (cond1 == 'ctrl') + (cond2 == 'ctrl')\n",
    "        num_in_perts = (cond1 in pert_names) + (cond2 in pert_names)\n",
    "        if num_ctrl + num_in_perts == 2:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "def uncertainty_loss_fct(pred, logvar, y, perts, reg = 0.1, ctrl = None,\n",
    "                         direction_lambda = 1e-3, dict_filter = None):\n",
    "    \"\"\"\n",
    "    Uncertainty loss function\n",
    "\n",
    "    Args:\n",
    "        pred (torch.tensor): predicted values\n",
    "        logvar (torch.tensor): log variance\n",
    "        y (torch.tensor): true values\n",
    "        perts (list): list of perturbations\n",
    "        reg (float): regularization parameter\n",
    "        ctrl (str): control perturbation\n",
    "        direction_lambda (float): direction loss weight hyperparameter\n",
    "        dict_filter (dict): dictionary of perturbations to conditions\n",
    "\n",
    "    \"\"\"\n",
    "    gamma = 2                     \n",
    "    perts = np.array(perts)\n",
    "    losses = torch.tensor(0.0, requires_grad=True).to(pred.device)\n",
    "    for p in set(perts):\n",
    "        if p!= 'ctrl':\n",
    "            retain_idx = dict_filter[p]\n",
    "            pred_p = pred[np.where(perts==p)[0]][:, retain_idx]\n",
    "            y_p = y[np.where(perts==p)[0]][:, retain_idx]\n",
    "            logvar_p = logvar[np.where(perts==p)[0]][:, retain_idx]\n",
    "        else:\n",
    "            pred_p = pred[np.where(perts==p)[0]]\n",
    "            y_p = y[np.where(perts==p)[0]]\n",
    "            logvar_p = logvar[np.where(perts==p)[0]]\n",
    "                         \n",
    "        # uncertainty based loss\n",
    "        losses += torch.sum((pred_p - y_p)**(2 + gamma) + reg * torch.exp(\n",
    "            -logvar_p)  * (pred_p - y_p)**(2 + gamma))/pred_p.shape[0]/pred_p.shape[1]\n",
    "                         \n",
    "        # direction loss                 \n",
    "        if p!= 'ctrl':\n",
    "            losses += torch.sum(direction_lambda *\n",
    "                                (torch.sign(y_p - ctrl[retain_idx]) -\n",
    "                                 torch.sign(pred_p - ctrl[retain_idx]))**2)/\\\n",
    "                                 pred_p.shape[0]/pred_p.shape[1]\n",
    "        else:\n",
    "            losses += torch.sum(direction_lambda *\n",
    "                                (torch.sign(y_p - ctrl) -\n",
    "                                 torch.sign(pred_p - ctrl))**2)/\\\n",
    "                                 pred_p.shape[0]/pred_p.shape[1]\n",
    "            \n",
    "    return losses/(len(set(perts)))\n",
    "\n",
    "def loss_fct(pred, y, perts, ctrl = None, direction_lambda = 1e-3, dict_filter = None):\n",
    "    \"\"\"\n",
    "    Main MSE Loss function, includes direction loss\n",
    "\n",
    "    Args:\n",
    "        pred (torch.tensor): predicted values\n",
    "        y (torch.tensor): true values\n",
    "        perts (list): list of perturbations\n",
    "        ctrl (str): control perturbation\n",
    "        direction_lambda (float): direction loss weight hyperparameter\n",
    "        dict_filter (dict): dictionary of perturbations to conditions\n",
    "\n",
    "    \"\"\"\n",
    "    gamma = 2\n",
    "    mse_p = torch.nn.MSELoss()\n",
    "    perts = np.array(perts)\n",
    "    losses = torch.tensor(0.0, requires_grad=True).to(pred.device)\n",
    "\n",
    "    for p in set(perts):\n",
    "        pert_idx = np.where(perts == p)[0]\n",
    "        \n",
    "        # during training, we remove the all zero genes into calculation of loss.\n",
    "        # this gives a cleaner direction loss. empirically, the performance stays the same.\n",
    "        if p!= 'ctrl':\n",
    "            # print(dict_filter)\n",
    "            retain_idx = dict_filter[p]\n",
    "            pred_p = pred[pert_idx][:, retain_idx]\n",
    "            y_p = y[pert_idx][:, retain_idx]\n",
    "        else:\n",
    "            pred_p = pred[pert_idx]\n",
    "            y_p = y[pert_idx]\n",
    "        losses = losses + torch.sum((pred_p - y_p)**(2 + gamma))/pred_p.shape[0]/pred_p.shape[1]\n",
    "                         \n",
    "        ## direction loss\n",
    "        if (p!= 'ctrl'):\n",
    "            losses = losses + torch.sum(direction_lambda *\n",
    "                                (torch.sign(y_p - ctrl[retain_idx]) -\n",
    "                                 torch.sign(pred_p - ctrl[retain_idx]))**2)/\\\n",
    "                                 pred_p.shape[0]/pred_p.shape[1]\n",
    "        else:\n",
    "            losses = losses + torch.sum(direction_lambda * (torch.sign(y_p - ctrl) -\n",
    "                                                torch.sign(pred_p - ctrl))**2)/\\\n",
    "                                                pred_p.shape[0]/pred_p.shape[1]\n",
    "    return losses/(len(set(perts)))\n",
    "\n",
    "def print_sys(s):\n",
    "    \"\"\"system print\n",
    "\n",
    "    Args:\n",
    "        s (str): the string to print\n",
    "    \"\"\"\n",
    "    print(s, flush = True, file = sys.stderr)\n",
    "    \n",
    "def create_cell_graph_for_prediction(X, pert_idx, pert_gene):\n",
    "    \"\"\"\n",
    "    Create a perturbation specific cell graph for inference\n",
    "\n",
    "    Args:\n",
    "        X (np.array): gene expression matrix\n",
    "        pert_idx (list): list of perturbation indices\n",
    "        pert_gene (list): list of perturbations\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if pert_idx is None:\n",
    "        pert_idx = [-1]\n",
    "    return Data(x=torch.Tensor(X).T, pert_idx = pert_idx, pert=pert_gene)\n",
    "    \n",
    "def create_cell_graph_dataset_for_prediction(pert_gene, ctrl_adata, gene_names,\n",
    "                                             device, num_samples = 300):\n",
    "    \"\"\"\n",
    "    Create a perturbation specific cell graph dataset for inference\n",
    "\n",
    "    Args:\n",
    "        pert_gene (list): list of perturbations\n",
    "        ctrl_adata (anndata): control anndata\n",
    "        gene_names (list): list of gene names\n",
    "        device (torch.device): device to use\n",
    "        num_samples (int): number of samples to use for inference (default: 300)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the indices (and signs) of applied perturbation\n",
    "    pert_idx = [np.where(p == np.array(gene_names))[0][0] for p in pert_gene]\n",
    "\n",
    "    Xs = ctrl_adata[np.random.randint(0, len(ctrl_adata), num_samples), :].X.toarray()\n",
    "    # Create cell graphs\n",
    "    cell_graphs = [create_cell_graph_for_prediction(X, pert_idx, pert_gene).to(device) for X in Xs]\n",
    "    return cell_graphs\n",
    "\n",
    "def get_coeffs(singles_expr, first_expr, second_expr, double_expr):\n",
    "    \"\"\"\n",
    "    Get coefficients for GI calculation\n",
    "\n",
    "    Args:\n",
    "        singles_expr (np.array): single perturbation expression\n",
    "        first_expr (np.array): first perturbation expression\n",
    "        second_expr (np.array): second perturbation expression\n",
    "        double_expr (np.array): double perturbation expression\n",
    "\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    results['ts'] = TheilSenRegressor(fit_intercept=False,\n",
    "                          max_subpopulation=1e5,\n",
    "                          max_iter=1000,\n",
    "                          random_state=1000)   \n",
    "    X = singles_expr\n",
    "    y = double_expr\n",
    "    results['ts'].fit(X, y.ravel())\n",
    "    Zts = results['ts'].predict(X)\n",
    "    results['c1'] = results['ts'].coef_[0]\n",
    "    results['c2'] = results['ts'].coef_[1]\n",
    "    results['mag'] = np.sqrt((results['c1']**2 + results['c2']**2))\n",
    "    \n",
    "    results['dcor'] = distance_correlation(singles_expr, double_expr)\n",
    "    results['dcor_singles'] = distance_correlation(first_expr, second_expr)\n",
    "    results['dcor_first'] = distance_correlation(first_expr, double_expr)\n",
    "    results['dcor_second'] = distance_correlation(second_expr, double_expr)\n",
    "    results['corr_fit'] = np.corrcoef(Zts.flatten(), double_expr.flatten())[0,1]\n",
    "    results['dominance'] = np.abs(np.log10(results['c1']/results['c2']))\n",
    "    results['eq_contr'] = np.min([results['dcor_first'], results['dcor_second']])/\\\n",
    "                        np.max([results['dcor_first'], results['dcor_second']])\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_GI_params(preds, combo):\n",
    "    \"\"\"\n",
    "    Get GI parameters\n",
    "\n",
    "    Args:\n",
    "        preds (dict): dictionary of predictions\n",
    "        combo (list): list of perturbations\n",
    "\n",
    "    \"\"\"\n",
    "    singles_expr = np.array([preds[combo[0]], preds[combo[1]]]).T\n",
    "    first_expr = np.array(preds[combo[0]]).T\n",
    "    second_expr = np.array(preds[combo[1]]).T\n",
    "    double_expr = np.array(preds[combo[0]+'_'+combo[1]]).T\n",
    "    \n",
    "    return get_coeffs(singles_expr, first_expr, second_expr, double_expr)\n",
    "\n",
    "def get_GI_genes_idx(adata, GI_gene_file):\n",
    "    \"\"\"\n",
    "    Optional: Reads a file containing a list of GI genes (usually those\n",
    "    with high mean expression)\n",
    "\n",
    "    Args:\n",
    "        adata (anndata): anndata object\n",
    "        GI_gene_file (str): file containing GI genes (generally corresponds\n",
    "        to genes with high mean expression)\n",
    "    \"\"\"\n",
    "    # Genes used for linear model fitting\n",
    "    GI_genes = np.load(GI_gene_file, allow_pickle=True)\n",
    "    GI_genes_idx = np.where([g in GI_genes for g in adata.var.gene_name.values])[0]\n",
    "    \n",
    "    return GI_genes_idx\n",
    "\n",
    "def get_mean_control(adata):\n",
    "    \"\"\"\n",
    "    Get mean control expression\n",
    "    \"\"\"\n",
    "    mean_ctrl_exp = adata[adata.obs['condition'] == 'ctrl'].to_df().mean()\n",
    "    return mean_ctrl_exp\n",
    "\n",
    "def get_genes_from_perts(perts):\n",
    "    \"\"\"\n",
    "    Returns list of genes involved in a given perturbation list\n",
    "    \"\"\"\n",
    "\n",
    "    if type(perts) is str:\n",
    "        perts = [perts]\n",
    "    gene_list = [p.split('+') for p in np.unique(perts)]\n",
    "    gene_list = [item for sublist in gene_list for item in sublist]\n",
    "    gene_list = [g for g in gene_list if g != 'ctrl']\n",
    "    return list(np.unique(gene_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inference\n",
    "import anndata as ad\n",
    "\n",
    "def evaluate(loader, model, uncertainty, device):\n",
    "    \"\"\"\n",
    "    Run model in inference mode using a given data loader\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    pert_cat = []\n",
    "    pred = []\n",
    "    truth = []\n",
    "    pred_de = []\n",
    "    truth_de = []\n",
    "    results = {}\n",
    "    logvar = []\n",
    "    \n",
    "    for itr, batch in enumerate(loader):\n",
    "\n",
    "        batch.to(device)\n",
    "        pert_cat.extend(batch.pert)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if uncertainty:\n",
    "                p, unc = model(batch)\n",
    "                logvar.extend(unc.cpu())\n",
    "            else:\n",
    "                p = model(batch)\n",
    "            t = batch.y\n",
    "            pred.extend(p.cpu())\n",
    "            truth.extend(t.cpu())\n",
    "            \n",
    "            # Differentially expressed genes\n",
    "            for itr, de_idx in enumerate(batch.de_idx):\n",
    "                pred_de.append(p[itr, de_idx])\n",
    "                truth_de.append(t[itr, de_idx])\n",
    "\n",
    "    # all genes\n",
    "    results['pert_cat'] = np.array(pert_cat)\n",
    "    pred = torch.stack(pred)\n",
    "    truth = torch.stack(truth)\n",
    "    results['pred']= pred.detach().cpu().numpy()\n",
    "    results['truth']= truth.detach().cpu().numpy()\n",
    "\n",
    "    pred_de = torch.stack(pred_de)\n",
    "    truth_de = torch.stack(truth_de)\n",
    "    results['pred_de']= pred_de.detach().cpu().numpy()\n",
    "    results['truth_de']= truth_de.detach().cpu().numpy()\n",
    "    \n",
    "    if uncertainty:\n",
    "        results['logvar'] = torch.stack(logvar).detach().cpu().numpy()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# def deg_score(results,adata:ad.AnnData):\n",
    "#     control_expressions = adata[adata.obs[\"condition\"]==\"ctrl\"].copy()\n",
    "#     for pert in np.unique(results[\"pert_cat\"]):\n",
    "#         p_idx = np.where(results['pert_cat'] == pert)[0]\n",
    "#         perturbed_expressions = results[\"pred\"][p_idx]\n",
    "#         perturbed_adata= ad.AnnData(X=perturbed_expressions,obs=control_expressions.obs_names,var=control_expressions.var_names)\n",
    "#         test_adata  = ad.concat(control_expressions,perturbed_adata)\n",
    "#         sc.tl.rank_genes_groups(test_adata)\n",
    "#         ## computing  the differentially expressed genes between perturbed_expressions and control, and true_expressions and control.\n",
    "#         sc.tl.rank_genes_groups(test_adata,groupby=)\n",
    "        \n",
    "#     pass\n",
    "\n",
    "def pds_score(results):\n",
    "    pass\n",
    "\n",
    "def compute_metrics(results):\n",
    "    \"\"\"\n",
    "    Given results from a model run and the ground truth, compute metrics\n",
    "\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    metrics_pert = {}\n",
    "\n",
    "    metric2fct = {\n",
    "           'mse': mse,\n",
    "           'pearson': pearsonr,\n",
    "    }\n",
    "    \n",
    "    for m in metric2fct.keys():\n",
    "        metrics[m] = []\n",
    "        metrics[m + '_de'] = []\n",
    "\n",
    "    for pert in np.unique(results['pert_cat']):\n",
    "\n",
    "        metrics_pert[pert] = {}\n",
    "        p_idx = np.where(results['pert_cat'] == pert)[0]\n",
    "            \n",
    "        for m, fct in metric2fct.items():\n",
    "            if m == 'pearson':\n",
    "                #results \n",
    "                #results['pred'] is every single possible perturbation's prediction.    \n",
    "                pred_subset_expr = results['pred_de'][p_idx]\n",
    "                truth_subset_expr = results['truth_de'][p_idx]\n",
    "                try:\n",
    "                    val = fct(results['pred_de'][p_idx].mean(0), results['truth_de'][p_idx].mean(0))[0]\n",
    "                except:\n",
    "                    # print(f\" pred{results['pred_de'][p_idx].shape}\", f\" truth :{results['truth_de'][p_idx].shape}\")\n",
    "                    val = fct(results['pred_de'][p_idx], results['truth_de'][p_idx])[0]\n",
    "                \n",
    "                \n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "            else:\n",
    "                val = fct(results['pred'][p_idx].mean(0), results['truth'][p_idx].mean(0))\n",
    "\n",
    "            metrics_pert[pert][m] = val\n",
    "            metrics[m].append(metrics_pert[pert][m])\n",
    "\n",
    "       \n",
    "        if pert != 'ctrl':\n",
    "            \n",
    "            for m, fct in metric2fct.items():\n",
    "                if m == 'pearson':\n",
    "                    try:\n",
    "                        val = fct(results['pred_de'][p_idx].mean(0), results['truth_de'][p_idx].mean(0))[0]\n",
    "                    except:\n",
    "                        # print(f\" pred{results['pred_de'][p_idx].shape}\", f\" truth :{results['truth_de'][p_idx].shape}\")\n",
    "                        val = fct(results['pred_de'][p_idx], results['truth_de'][p_idx])[0]\n",
    "                    # val = fct(results['pred_de'][p_idx].mean(0), results['truth_de'][p_idx].mean(0))[0]\n",
    "                    if np.isnan(val):\n",
    "                        val = 0\n",
    "                else:\n",
    "                    val = fct(results['pred_de'][p_idx].mean(0), results['truth_de'][p_idx].mean(0))\n",
    "                    \n",
    "                metrics_pert[pert][m + '_de'] = val\n",
    "                metrics[m + '_de'].append(metrics_pert[pert][m + '_de'])\n",
    "\n",
    "        else:\n",
    "            for m, fct in metric2fct.items():\n",
    "                metrics_pert[pert][m + '_de'] = 0\n",
    "    \n",
    "    for m in metric2fct.keys():\n",
    "        \n",
    "        metrics[m] = np.mean(metrics[m])\n",
    "        metrics[m + '_de'] = np.mean(metrics[m + '_de'])\n",
    "    \n",
    "    return metrics, metrics_pert\n",
    "\n",
    "def non_zero_analysis(adata, test_res):\n",
    "    metric2fct = {\n",
    "           'pearson': pearsonr,\n",
    "           'mse': mse\n",
    "    }\n",
    "\n",
    "    pert_metric = {}\n",
    "    \n",
    "    ## in silico modeling and upperbounding\n",
    "    pert2pert_full_id = dict(adata.obs[['condition', 'condition_name']].values)\n",
    "    geneid2name = dict(zip(adata.var.index.values, adata.var['gene_name']))\n",
    "    geneid2idx = dict(zip(adata.var.index.values, range(len(adata.var.index.values))))\n",
    "\n",
    "    # calculate mean expression for each condition\n",
    "    unique_conditions = adata.obs.condition.unique()\n",
    "    conditions2index = {}\n",
    "    for i in unique_conditions:\n",
    "        conditions2index[i] = np.where(adata.obs.condition == i)[0]\n",
    "\n",
    "    condition2mean_expression = {}\n",
    "    for i, j in conditions2index.items():\n",
    "        condition2mean_expression[i] = np.mean(adata.X[j], axis = 0)\n",
    "    pert_list = np.array(list(condition2mean_expression.keys()))\n",
    "    mean_expression = np.array(list(condition2mean_expression.values())).reshape(len(adata.obs.condition.unique()), adata.X.toarray().shape[1])\n",
    "    ctrl = mean_expression[np.where(pert_list == 'ctrl')[0]]\n",
    "    \n",
    "    gene_list = adata.var['gene_name'].values\n",
    "\n",
    "    for pert in np.unique(test_res['pert_cat']):\n",
    "        pert_metric[pert] = {}\n",
    "        \n",
    "        pert_idx = np.where(test_res['pert_cat'] == pert)[0]    \n",
    "        de_idx = [geneid2idx[i] for i in adata.uns['top_non_zero_de_20'][pert2pert_full_id[pert]]]\n",
    "\n",
    "        direc_change = np.abs(np.sign(test_res['pred'][pert_idx].mean(0)[de_idx] - ctrl[0][de_idx]) - np.sign(test_res['truth'][pert_idx].mean(0)[de_idx] - ctrl[0][de_idx]))            \n",
    "        frac_correct_direction = len(np.where(direc_change == 0)[0])/len(de_idx)\n",
    "        pert_metric[pert]['frac_correct_direction_top20_non_zero'] = frac_correct_direction\n",
    "        \n",
    "        frac_direction_opposite = len(np.where(direc_change == 2)[0])/len(de_idx)\n",
    "        pert_metric[pert]['frac_opposite_direction_top20_non_zero'] = frac_direction_opposite\n",
    "        \n",
    "        frac_direction_opposite = len(np.where(direc_change == 1)[0])/len(de_idx)\n",
    "        pert_metric[pert]['frac_0/1_direction_top20_non_zero'] = frac_direction_opposite\n",
    "        \n",
    "        mean = np.mean(test_res['truth'][pert_idx][:, de_idx], axis = 0)\n",
    "        std = np.std(test_res['truth'][pert_idx][:, de_idx], axis = 0)\n",
    "        min_ = np.min(test_res['truth'][pert_idx][:, de_idx], axis = 0)\n",
    "        max_ = np.max(test_res['truth'][pert_idx][:, de_idx], axis = 0)\n",
    "        q25 = np.quantile(test_res['truth'][pert_idx][:, de_idx], 0.25, axis = 0)\n",
    "        q75 = np.quantile(test_res['truth'][pert_idx][:, de_idx], 0.75, axis = 0)\n",
    "        q55 = np.quantile(test_res['truth'][pert_idx][:, de_idx], 0.55, axis = 0)\n",
    "        q45 = np.quantile(test_res['truth'][pert_idx][:, de_idx], 0.45, axis = 0)\n",
    "        q40 = np.quantile(test_res['truth'][pert_idx][:, de_idx], 0.4, axis = 0)\n",
    "        q60 = np.quantile(test_res['truth'][pert_idx][:, de_idx], 0.6, axis = 0)\n",
    "        \n",
    "        zero_des = np.intersect1d(np.where(min_ == 0)[0], np.where(max_ == 0)[0])\n",
    "        nonzero_des = np.setdiff1d(list(range(20)), zero_des)\n",
    "        \n",
    "        if len(nonzero_des) == 0:\n",
    "            pass\n",
    "            # pert that all de genes are 0...\n",
    "        else:            \n",
    "            pred_mean = np.mean(test_res['pred'][pert_idx][:, de_idx], axis = 0).reshape(-1,)\n",
    "            true_mean = np.mean(test_res['truth'][pert_idx][:, de_idx], axis = 0).reshape(-1,)\n",
    "           \n",
    "            in_range = (pred_mean[nonzero_des] >= min_[nonzero_des]) & (pred_mean[nonzero_des] <= max_[nonzero_des])\n",
    "            frac_in_range = sum(in_range)/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_in_range_non_zero'] = frac_in_range\n",
    "\n",
    "            in_range_5 = (pred_mean[nonzero_des] >= q45[nonzero_des]) & (pred_mean[nonzero_des] <= q55[nonzero_des])\n",
    "            frac_in_range_45_55 = sum(in_range_5)/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_in_range_45_55_non_zero'] = frac_in_range_45_55\n",
    "\n",
    "            in_range_10 = (pred_mean[nonzero_des] >= q40[nonzero_des]) & (pred_mean[nonzero_des] <= q60[nonzero_des])\n",
    "            frac_in_range_40_60 = sum(in_range_10)/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_in_range_40_60_non_zero'] = frac_in_range_40_60\n",
    "\n",
    "            in_range_25 = (pred_mean[nonzero_des] >= q25[nonzero_des]) & (pred_mean[nonzero_des] <= q75[nonzero_des])\n",
    "            frac_in_range_25_75 = sum(in_range_25)/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_in_range_25_75_non_zero'] = frac_in_range_25_75\n",
    "\n",
    "            zero_idx = np.where(std > 0)[0]\n",
    "            sigma = (np.abs(pred_mean[zero_idx] - mean[zero_idx]))/(std[zero_idx])\n",
    "            pert_metric[pert]['mean_sigma_non_zero'] = np.mean(sigma)\n",
    "            pert_metric[pert]['std_sigma_non_zero'] = np.std(sigma)\n",
    "            pert_metric[pert]['frac_sigma_below_1_non_zero'] = 1 - len(np.where(sigma > 1)[0])/len(zero_idx)\n",
    "            pert_metric[pert]['frac_sigma_below_2_non_zero'] = 1 - len(np.where(sigma > 2)[0])/len(zero_idx)\n",
    "        \n",
    "        p_idx = np.where(test_res['pert_cat'] == pert)[0]\n",
    "        for m, fct in metric2fct.items():\n",
    "            if m != 'mse':\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx] - ctrl[0][de_idx], test_res['truth'][p_idx].mean(0)[de_idx]-ctrl[0][de_idx])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_delta_top20_de_non_zero'] = val\n",
    "\n",
    "\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx], test_res['truth'][p_idx].mean(0)[de_idx])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_top20_de_non_zero'] = val\n",
    "            else:\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx] - ctrl[0][de_idx], test_res['truth'][p_idx].mean(0)[de_idx]-ctrl[0][de_idx])\n",
    "                pert_metric[pert][m + '_top20_de_non_zero'] = val\n",
    "                \n",
    "    return pert_metric\n",
    "\n",
    "def non_dropout_analysis(adata, test_res):\n",
    "    metric2fct = {\n",
    "           'pearson': pearsonr,\n",
    "           'mse': mse\n",
    "    }\n",
    "\n",
    "    pert_metric = {}\n",
    "    \n",
    "    ## in silico modeling and upperbounding\n",
    "    pert2pert_full_id = dict(adata.obs[['condition', 'condition_name']].values)\n",
    "    geneid2name = dict(zip(adata.var.index.values, adata.var['gene_name']))\n",
    "    geneid2idx = dict(zip(adata.var.index.values, range(len(adata.var.index.values))))\n",
    "\n",
    "    # calculate mean expression for each condition\n",
    "    unique_conditions = adata.obs.condition.unique()\n",
    "    conditions2index = {}\n",
    "    for i in unique_conditions:\n",
    "        conditions2index[i] = np.where(adata.obs.condition == i)[0]\n",
    "\n",
    "    condition2mean_expression = {}\n",
    "    for i, j in conditions2index.items():\n",
    "        condition2mean_expression[i] = np.mean(adata.X[j], axis = 0)\n",
    "    pert_list = np.array(list(condition2mean_expression.keys()))\n",
    "    mean_expression = np.array(list(condition2mean_expression.values())).reshape(len(adata.obs.condition.unique()), adata.X.toarray().shape[1])\n",
    "    ctrl = mean_expression[np.where(pert_list == 'ctrl')[0]]\n",
    "    \n",
    "    gene_list = adata.var['gene_name'].values\n",
    "\n",
    "    for pert in np.unique(test_res['pert_cat']):\n",
    "        pert_metric[pert] = {}\n",
    "        \n",
    "        pert_idx = np.where(test_res['pert_cat'] == pert)[0]    \n",
    "        de_idx = [geneid2idx[i] for i in adata.uns['top_non_dropout_de_20'][pert2pert_full_id[pert]]]\n",
    "        non_zero_idx = adata.uns['non_zeros_gene_idx'][pert2pert_full_id[pert]]\n",
    "        non_dropout_gene_idx = adata.uns['non_dropout_gene_idx'][pert2pert_full_id[pert]]\n",
    "             \n",
    "        direc_change = np.abs(np.sign(test_res['pred'][pert_idx].mean(0)[de_idx] - ctrl[0][de_idx]) - np.sign(test_res['truth'][pert_idx].mean(0)[de_idx] - ctrl[0][de_idx]))            \n",
    "        frac_correct_direction = len(np.where(direc_change == 0)[0])/len(de_idx)\n",
    "        pert_metric[pert]['frac_correct_direction_top20_non_dropout'] = frac_correct_direction\n",
    "        \n",
    "        frac_direction_opposite = len(np.where(direc_change == 2)[0])/len(de_idx)\n",
    "        pert_metric[pert]['frac_opposite_direction_top20_non_dropout'] = frac_direction_opposite\n",
    "        \n",
    "        frac_direction_opposite = len(np.where(direc_change == 1)[0])/len(de_idx)\n",
    "        pert_metric[pert]['frac_0/1_direction_top20_non_dropout'] = frac_direction_opposite\n",
    "        \n",
    "        direc_change = np.abs(np.sign(test_res['pred'][pert_idx].mean(0)[non_zero_idx] - ctrl[0][non_zero_idx]) - np.sign(test_res['truth'][pert_idx].mean(0)[non_zero_idx] - ctrl[0][non_zero_idx]))            \n",
    "        frac_correct_direction = len(np.where(direc_change == 0)[0])/len(non_zero_idx)\n",
    "        pert_metric[pert]['frac_correct_direction_non_zero'] = frac_correct_direction\n",
    "\n",
    "        frac_direction_opposite = len(np.where(direc_change == 2)[0])/len(non_zero_idx)\n",
    "        pert_metric[pert]['frac_opposite_direction_non_zero'] = frac_direction_opposite\n",
    "        \n",
    "        frac_direction_opposite = len(np.where(direc_change == 1)[0])/len(non_zero_idx)\n",
    "        pert_metric[pert]['frac_0/1_direction_non_zero'] = frac_direction_opposite\n",
    "        \n",
    "        direc_change = np.abs(np.sign(test_res['pred'][pert_idx].mean(0)[non_dropout_gene_idx] - ctrl[0][non_dropout_gene_idx]) - np.sign(test_res['truth'][pert_idx].mean(0)[non_dropout_gene_idx] - ctrl[0][non_dropout_gene_idx]))            \n",
    "        frac_correct_direction = len(np.where(direc_change == 0)[0])/len(non_dropout_gene_idx)\n",
    "        pert_metric[pert]['frac_correct_direction_non_dropout'] = frac_correct_direction\n",
    "        \n",
    "        frac_direction_opposite = len(np.where(direc_change == 2)[0])/len(non_dropout_gene_idx)\n",
    "        pert_metric[pert]['frac_opposite_direction_non_dropout'] = frac_direction_opposite\n",
    "        \n",
    "        frac_direction_opposite = len(np.where(direc_change == 1)[0])/len(non_dropout_gene_idx)\n",
    "        pert_metric[pert]['frac_0/1_direction_non_dropout'] = frac_direction_opposite\n",
    "        \n",
    "        mean = np.mean(test_res['truth'][pert_idx][:, de_idx], axis = 0)\n",
    "        std = np.std(test_res['truth'][pert_idx][:, de_idx], axis = 0)\n",
    "        min_ = np.min(test_res['truth'][pert_idx][:, de_idx], axis = 0)\n",
    "        max_ = np.max(test_res['truth'][pert_idx][:, de_idx], axis = 0)\n",
    "        q25 = np.quantile(test_res['truth'][pert_idx][:, de_idx], 0.25, axis = 0)\n",
    "        q75 = np.quantile(test_res['truth'][pert_idx][:, de_idx], 0.75, axis = 0)\n",
    "        q55 = np.quantile(test_res['truth'][pert_idx][:, de_idx], 0.55, axis = 0)\n",
    "        q45 = np.quantile(test_res['truth'][pert_idx][:, de_idx], 0.45, axis = 0)\n",
    "        q40 = np.quantile(test_res['truth'][pert_idx][:, de_idx], 0.4, axis = 0)\n",
    "        q60 = np.quantile(test_res['truth'][pert_idx][:, de_idx], 0.6, axis = 0)\n",
    "        \n",
    "        zero_des = np.intersect1d(np.where(min_ == 0)[0], np.where(max_ == 0)[0])\n",
    "        nonzero_des = np.setdiff1d(list(range(20)), zero_des)\n",
    "        \n",
    "        if len(nonzero_des) == 0:\n",
    "            pass\n",
    "            # pert that all de genes are 0...\n",
    "        else:            \n",
    "            pred_mean = np.mean(test_res['pred'][pert_idx][:, de_idx], axis = 0).reshape(-1,)\n",
    "            true_mean = np.mean(test_res['truth'][pert_idx][:, de_idx], axis = 0).reshape(-1,)\n",
    "           \n",
    "            in_range = (pred_mean[nonzero_des] >= min_[nonzero_des]) & (pred_mean[nonzero_des] <= max_[nonzero_des])\n",
    "            frac_in_range = sum(in_range)/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_in_range_non_dropout'] = frac_in_range\n",
    "\n",
    "            in_range_5 = (pred_mean[nonzero_des] >= q45[nonzero_des]) & (pred_mean[nonzero_des] <= q55[nonzero_des])\n",
    "            frac_in_range_45_55 = sum(in_range_5)/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_in_range_45_55_non_dropout'] = frac_in_range_45_55\n",
    "\n",
    "            in_range_10 = (pred_mean[nonzero_des] >= q40[nonzero_des]) & (pred_mean[nonzero_des] <= q60[nonzero_des])\n",
    "            frac_in_range_40_60 = sum(in_range_10)/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_in_range_40_60_non_dropout'] = frac_in_range_40_60\n",
    "\n",
    "            in_range_25 = (pred_mean[nonzero_des] >= q25[nonzero_des]) & (pred_mean[nonzero_des] <= q75[nonzero_des])\n",
    "            frac_in_range_25_75 = sum(in_range_25)/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_in_range_25_75_non_dropout'] = frac_in_range_25_75\n",
    "\n",
    "            zero_idx = np.where(std > 0)[0]\n",
    "            sigma = (np.abs(pred_mean[zero_idx] - mean[zero_idx]))/(std[zero_idx])\n",
    "            pert_metric[pert]['mean_sigma_non_dropout'] = np.mean(sigma)\n",
    "            pert_metric[pert]['std_sigma_non_dropout'] = np.std(sigma)\n",
    "            pert_metric[pert]['frac_sigma_below_1_non_dropout'] = 1 - len(np.where(sigma > 1)[0])/len(zero_idx)\n",
    "            pert_metric[pert]['frac_sigma_below_2_non_dropout'] = 1 - len(np.where(sigma > 2)[0])/len(zero_idx)\n",
    "        \n",
    "        p_idx = np.where(test_res['pert_cat'] == pert)[0]\n",
    "        for m, fct in metric2fct.items():\n",
    "            if m != 'mse':\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx] - ctrl[0][de_idx], test_res['truth'][p_idx].mean(0)[de_idx]-ctrl[0][de_idx])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_delta_top20_de_non_dropout'] = val\n",
    "\n",
    "\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx], test_res['truth'][p_idx].mean(0)[de_idx])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_top20_de_non_dropout'] = val\n",
    "            else:\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx] - ctrl[0][de_idx], test_res['truth'][p_idx].mean(0)[de_idx]-ctrl[0][de_idx])\n",
    "                pert_metric[pert][m + '_top20_de_non_dropout'] = val\n",
    "                \n",
    "    return pert_metric\n",
    "    \n",
    "def deeper_analysis(adata, test_res, de_column_prefix = 'rank_genes_groups_cov', most_variable_genes = None):\n",
    "    \n",
    "    metric2fct = {\n",
    "           'pearson': pearsonr,\n",
    "           'mse': mse\n",
    "    }\n",
    "\n",
    "    pert_metric = {}\n",
    "\n",
    "    ## in silico modeling and upperbounding\n",
    "    pert2pert_full_id = dict(adata.obs[['condition', 'condition_name']].values)\n",
    "    geneid2name = dict(zip(adata.var.index.values, adata.var['gene_name']))\n",
    "    geneid2idx = dict(zip(adata.var.index.values, range(len(adata.var.index.values))))\n",
    "\n",
    "    # calculate mean expression for each condition\n",
    "    unique_conditions = adata.obs.condition.unique()\n",
    "    conditions2index = {}\n",
    "    for i in unique_conditions:\n",
    "        conditions2index[i] = np.where(adata.obs.condition == i)[0]\n",
    "\n",
    "    condition2mean_expression = {}\n",
    "    for i, j in conditions2index.items():\n",
    "        condition2mean_expression[i] = np.mean(adata.X[j], axis = 0)\n",
    "    pert_list = np.array(list(condition2mean_expression.keys()))\n",
    "    mean_expression = np.array(list(condition2mean_expression.values())).reshape(len(adata.obs.condition.unique()), adata.X.toarray().shape[1])\n",
    "    ctrl = mean_expression[np.where(pert_list == 'ctrl')[0]]\n",
    "    \n",
    "    if most_variable_genes is None:\n",
    "        most_variable_genes = np.argsort(np.std(mean_expression, axis = 0))[-200:]\n",
    "        \n",
    "    gene_list = adata.var['gene_name'].values\n",
    "\n",
    "    for pert in np.unique(test_res['pert_cat']):\n",
    "        pert_metric[pert] = {}\n",
    "        de_idx = [geneid2idx[i] for i in adata.uns['rank_genes_groups_cov_all'][pert2pert_full_id[pert]][:20]]\n",
    "        de_idx_200 = [geneid2idx[i] for i in adata.uns['rank_genes_groups_cov_all'][pert2pert_full_id[pert]][:200]]\n",
    "        de_idx_100 = [geneid2idx[i] for i in adata.uns['rank_genes_groups_cov_all'][pert2pert_full_id[pert]][:100]]\n",
    "        de_idx_50 = [geneid2idx[i] for i in adata.uns['rank_genes_groups_cov_all'][pert2pert_full_id[pert]][:50]]\n",
    "\n",
    "        pert_idx = np.where(test_res['pert_cat'] == pert)[0]    \n",
    "        pred_mean = np.mean(test_res['pred_de'][pert_idx], axis = 0).reshape(-1,)\n",
    "        true_mean = np.mean(test_res['truth_de'][pert_idx], axis = 0).reshape(-1,)\n",
    "        \n",
    "        direc_change = np.abs(np.sign(test_res['pred'][pert_idx].mean(0) - ctrl[0]) - np.sign(test_res['truth'][pert_idx].mean(0) - ctrl[0]))            \n",
    "        frac_correct_direction = len(np.where(direc_change == 0)[0])/len(geneid2name)\n",
    "        pert_metric[pert]['frac_correct_direction_all'] = frac_correct_direction\n",
    "\n",
    "        de_idx_map = {20: de_idx,\n",
    "                      50: de_idx_50,\n",
    "                      100: de_idx_100,\n",
    "                      200: de_idx_200\n",
    "                     }\n",
    "        \n",
    "        for val in [20, 50, 100, 200]:\n",
    "            \n",
    "            direc_change = np.abs(np.sign(test_res['pred'][pert_idx].mean(0)[de_idx_map[val]] - ctrl[0][de_idx_map[val]]) - np.sign(test_res['truth'][pert_idx].mean(0)[de_idx_map[val]] - ctrl[0][de_idx_map[val]]))            \n",
    "            frac_correct_direction = len(np.where(direc_change == 0)[0])/val\n",
    "            pert_metric[pert]['frac_correct_direction_' + str(val)] = frac_correct_direction\n",
    "\n",
    "        mean = np.mean(test_res['truth_de'][pert_idx], axis = 0)\n",
    "        std = np.std(test_res['truth_de'][pert_idx], axis = 0)\n",
    "        min_ = np.min(test_res['truth_de'][pert_idx], axis = 0)\n",
    "        max_ = np.max(test_res['truth_de'][pert_idx], axis = 0)\n",
    "        q25 = np.quantile(test_res['truth_de'][pert_idx], 0.25, axis = 0)\n",
    "        q75 = np.quantile(test_res['truth_de'][pert_idx], 0.75, axis = 0)\n",
    "        q55 = np.quantile(test_res['truth_de'][pert_idx], 0.55, axis = 0)\n",
    "        q45 = np.quantile(test_res['truth_de'][pert_idx], 0.45, axis = 0)\n",
    "        q40 = np.quantile(test_res['truth_de'][pert_idx], 0.4, axis = 0)\n",
    "        q60 = np.quantile(test_res['truth_de'][pert_idx], 0.6, axis = 0)\n",
    "\n",
    "        zero_des = np.intersect1d(np.where(min_ == 0)[0], np.where(max_ == 0)[0])\n",
    "        nonzero_des = np.setdiff1d(list(range(20)), zero_des)\n",
    "        if len(nonzero_des) == 0:\n",
    "            pass\n",
    "            # pert that all de genes are 0...\n",
    "        else:            \n",
    "            \n",
    "            direc_change = np.abs(np.sign(pred_mean[nonzero_des] - ctrl[0][de_idx][nonzero_des]) - np.sign(true_mean[nonzero_des] - ctrl[0][de_idx][nonzero_des]))            \n",
    "            frac_correct_direction = len(np.where(direc_change == 0)[0])/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_correct_direction_20_nonzero'] = frac_correct_direction\n",
    "            \n",
    "            in_range = (pred_mean[nonzero_des] >= min_[nonzero_des]) & (pred_mean[nonzero_des] <= max_[nonzero_des])\n",
    "            frac_in_range = sum(in_range)/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_in_range'] = frac_in_range\n",
    "\n",
    "            in_range_5 = (pred_mean[nonzero_des] >= q45[nonzero_des]) & (pred_mean[nonzero_des] <= q55[nonzero_des])\n",
    "            frac_in_range_45_55 = sum(in_range_5)/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_in_range_45_55'] = frac_in_range_45_55\n",
    "\n",
    "            in_range_10 = (pred_mean[nonzero_des] >= q40[nonzero_des]) & (pred_mean[nonzero_des] <= q60[nonzero_des])\n",
    "            frac_in_range_40_60 = sum(in_range_10)/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_in_range_40_60'] = frac_in_range_40_60\n",
    "\n",
    "            in_range_25 = (pred_mean[nonzero_des] >= q25[nonzero_des]) & (pred_mean[nonzero_des] <= q75[nonzero_des])\n",
    "            frac_in_range_25_75 = sum(in_range_25)/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_in_range_25_75'] = frac_in_range_25_75\n",
    "\n",
    "            zero_idx = np.where(std > 0)[0]\n",
    "            sigma = (np.abs(pred_mean[zero_idx] - mean[zero_idx]))/(std[zero_idx])\n",
    "            pert_metric[pert]['mean_sigma'] = np.mean(sigma)\n",
    "            pert_metric[pert]['std_sigma'] = np.std(sigma)\n",
    "            pert_metric[pert]['frac_sigma_below_1'] = 1 - len(np.where(sigma > 1)[0])/len(zero_idx)\n",
    "            pert_metric[pert]['frac_sigma_below_2'] = 1 - len(np.where(sigma > 2)[0])/len(zero_idx)\n",
    "\n",
    "        ## correlation on delta\n",
    "        p_idx = np.where(test_res['pert_cat'] == pert)[0]\n",
    "\n",
    "        for m, fct in metric2fct.items():\n",
    "            if m != 'mse':\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)- ctrl[0], test_res['truth'][p_idx].mean(0)-ctrl[0])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "\n",
    "                pert_metric[pert][m + '_delta'] = val\n",
    "                \n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx] - ctrl[0][de_idx], test_res['truth'][p_idx].mean(0)[de_idx]-ctrl[0][de_idx])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "\n",
    "                pert_metric[pert][m + '_delta_de'] = val\n",
    "\n",
    "        ## up fold changes > 10?\n",
    "        pert_mean = np.mean(test_res['truth'][p_idx], axis = 0).reshape(-1,)\n",
    "\n",
    "        fold_change = pert_mean/ctrl\n",
    "        fold_change[np.isnan(fold_change)] = 0\n",
    "        fold_change[np.isinf(fold_change)] = 0\n",
    "        ## this is to remove the ones that are super low and the fold change becomes unmeaningful\n",
    "        fold_change[0][np.where(pert_mean < 0.5)[0]] = 0\n",
    "\n",
    "        o =  np.where(fold_change[0] > 0)[0]\n",
    "\n",
    "        pred_fc = test_res['pred'][p_idx].mean(0)[o]\n",
    "        true_fc = test_res['truth'][p_idx].mean(0)[o]\n",
    "        ctrl_fc = ctrl[0][o]\n",
    "\n",
    "        if len(o) > 0:\n",
    "            pert_metric[pert]['fold_change_gap_all'] = np.mean(np.abs(pred_fc/ctrl_fc - true_fc/ctrl_fc))\n",
    "\n",
    "\n",
    "        o = np.intersect1d(np.where(fold_change[0] <0.333)[0], np.where(fold_change[0] > 0)[0])\n",
    "\n",
    "        pred_fc = test_res['pred'][p_idx].mean(0)[o]\n",
    "        true_fc = test_res['truth'][p_idx].mean(0)[o]\n",
    "        ctrl_fc = ctrl[0][o]\n",
    "\n",
    "        if len(o) > 0:\n",
    "            pert_metric[pert]['fold_change_gap_downreg_0.33'] = np.mean(np.abs(pred_fc/ctrl_fc - true_fc/ctrl_fc))\n",
    "\n",
    "\n",
    "        o = np.intersect1d(np.where(fold_change[0] <0.1)[0], np.where(fold_change[0] > 0)[0])\n",
    "\n",
    "        pred_fc = test_res['pred'][p_idx].mean(0)[o]\n",
    "        true_fc = test_res['truth'][p_idx].mean(0)[o]\n",
    "        ctrl_fc = ctrl[0][o]\n",
    "\n",
    "        if len(o) > 0:\n",
    "            pert_metric[pert]['fold_change_gap_downreg_0.1'] = np.mean(np.abs(pred_fc/ctrl_fc - true_fc/ctrl_fc))\n",
    "\n",
    "        o = np.where(fold_change[0] > 3)[0]\n",
    "\n",
    "        pred_fc = test_res['pred'][p_idx].mean(0)[o]\n",
    "        true_fc = test_res['truth'][p_idx].mean(0)[o]\n",
    "        ctrl_fc = ctrl[0][o]\n",
    "\n",
    "        if len(o) > 0:\n",
    "            pert_metric[pert]['fold_change_gap_upreg_3'] = np.mean(np.abs(pred_fc/ctrl_fc - true_fc/ctrl_fc))\n",
    "\n",
    "        o = np.where(fold_change[0] > 10)[0]\n",
    "\n",
    "        pred_fc = test_res['pred'][p_idx].mean(0)[o]\n",
    "        true_fc = test_res['truth'][p_idx].mean(0)[o]\n",
    "        ctrl_fc = ctrl[0][o]\n",
    "\n",
    "        if len(o) > 0:\n",
    "            pert_metric[pert]['fold_change_gap_upreg_10'] = np.mean(np.abs(pred_fc/ctrl_fc - true_fc/ctrl_fc))\n",
    "\n",
    "        ## most variable genes\n",
    "        for m, fct in metric2fct.items():\n",
    "            if m != 'mse':\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[most_variable_genes] - ctrl[0][most_variable_genes], test_res['truth'][p_idx].mean(0)[most_variable_genes]-ctrl[0][most_variable_genes])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_delta_top200_hvg'] = val\n",
    "\n",
    "\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[most_variable_genes], test_res['truth'][p_idx].mean(0)[most_variable_genes])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_top200_hvg'] = val\n",
    "            else:\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[most_variable_genes], test_res['truth'][p_idx].mean(0)[most_variable_genes])\n",
    "                pert_metric[pert][m + '_top200_hvg'] = val\n",
    "\n",
    "\n",
    "        ## top 20/50/100/200 DEs\n",
    "        for m, fct in metric2fct.items():\n",
    "            if m != 'mse':\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx] - ctrl[0][de_idx], test_res['truth'][p_idx].mean(0)[de_idx]-ctrl[0][de_idx])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_delta_top20_de'] = val\n",
    "\n",
    "\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx], test_res['truth'][p_idx].mean(0)[de_idx])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_top20_de'] = val\n",
    "            else:\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx] - ctrl[0][de_idx], test_res['truth'][p_idx].mean(0)[de_idx]-ctrl[0][de_idx])\n",
    "                pert_metric[pert][m + '_top20_de'] = val\n",
    "\n",
    "        \n",
    "        for m, fct in metric2fct.items():\n",
    "            if m != 'mse':\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx_200] - ctrl[0][de_idx_200], test_res['truth'][p_idx].mean(0)[de_idx_200]-ctrl[0][de_idx_200])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_delta_top200_de'] = val\n",
    "\n",
    "\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx_200], test_res['truth'][p_idx].mean(0)[de_idx_200])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_top200_de'] = val\n",
    "            else:\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx_200] - ctrl[0][de_idx_200], test_res['truth'][p_idx].mean(0)[de_idx_200]-ctrl[0][de_idx_200])\n",
    "                pert_metric[pert][m + '_top200_de'] = val\n",
    "\n",
    "        for m, fct in metric2fct.items():\n",
    "            if m != 'mse':\n",
    "\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx_100] - ctrl[0][de_idx_100], test_res['truth'][p_idx].mean(0)[de_idx_100]-ctrl[0][de_idx_100])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_delta_top100_de'] = val\n",
    "\n",
    "\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx_100], test_res['truth'][p_idx].mean(0)[de_idx_100])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_top100_de'] = val\n",
    "            else:\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx_100] - ctrl[0][de_idx_100], test_res['truth'][p_idx].mean(0)[de_idx_100]-ctrl[0][de_idx_100])\n",
    "                pert_metric[pert][m + '_top100_de'] = val\n",
    "\n",
    "        for m, fct in metric2fct.items():\n",
    "            if m != 'mse':\n",
    "\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx_50] - ctrl[0][de_idx_50], test_res['truth'][p_idx].mean(0)[de_idx_50]-ctrl[0][de_idx_50])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_delta_top50_de'] = val\n",
    "\n",
    "\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx_50], test_res['truth'][p_idx].mean(0)[de_idx_50])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_top50_de'] = val\n",
    "            else:\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx_50] - ctrl[0][de_idx_50], test_res['truth'][p_idx].mean(0)[de_idx_50]-ctrl[0][de_idx_50])\n",
    "                pert_metric[pert][m + '_top50_de'] = val\n",
    "\n",
    "\n",
    "\n",
    "    return pert_metric\n",
    "\n",
    "def GI_subgroup(pert_metric):\n",
    "    GI_type2Score = {}\n",
    "    test_pert_list = list(pert_metric.keys())\n",
    "    for GI_type, gi_list in GIs.items():\n",
    "        intersect = np.intersect1d(gi_list, test_pert_list)\n",
    "        if len(intersect) != 0:\n",
    "            GI_type2Score[GI_type] = {}\n",
    "\n",
    "            for m in list(list(pert_metric.values())[0].keys()):\n",
    "                GI_type2Score[GI_type][m] = np.mean([pert_metric[i][m] for i in intersect if m in pert_metric[i]])\n",
    "                \n",
    "    return GI_type2Score\n",
    "\n",
    "def node_specific_batch_out(models, batch):\n",
    "    # Returns output for all node specific models as a matrix of dimension batch_size x nodes\n",
    "    outs = []\n",
    "    for idx in range(len(models)):\n",
    "        outs.append(models[idx](batch).detach().cpu().numpy()[:,idx])\n",
    "    return np.vstack(outs).T\n",
    "\n",
    "def batch_predict(loader, loaded_models, args):\n",
    "    # Prediction for node specific GNNs\n",
    "    preds = []\n",
    "    print(\"Loader size: \", len(loader))\n",
    "    for itr, batch in enumerate(loader):\n",
    "        print(itr)\n",
    "        batch = batch.to(args['device'])\n",
    "        preds.append(node_specific_batch_out(loaded_models, batch))\n",
    "\n",
    "    preds = np.vstack(preds)\n",
    "    return preds\n",
    "\n",
    "def get_high_umi_idx(gene_list):\n",
    "    # Genes used for linear model fitting\n",
    "    try:\n",
    "        high_umi = np.load('../genes_with_hi_mean.npy', allow_pickle=True)\n",
    "    except:\n",
    "        high_umi = np.load('./genes_with_hi_mean.npy', allow_pickle=True)\n",
    "    high_umi_idx = np.where([g in high_umi for g in gene_list])[0]\n",
    "    return high_umi_idx\n",
    "\n",
    "def get_mean_ctrl(adata):\n",
    "    return adata[adata.obs['condition'] == 'ctrl'].to_df().mean().reset_index(\n",
    "        drop=True)\n",
    "\n",
    "def get_single_name(g, all_perts):\n",
    "    name = g+'+ctrl'\n",
    "    if name in all_perts:\n",
    "        return name\n",
    "    else:\n",
    "        return 'ctrl+'+g\n",
    "\n",
    "def get_test_set_results_seen2(res, sel_GI_type):\n",
    "    # Get relevant test set results\n",
    "    test_pert_cats = [p for p in np.unique(res['pert_cat']) if\n",
    "                      p in GIs[sel_GI_type] or 'ctrl' in p]\n",
    "    pred_idx = np.where([t in test_pert_cats for t in res['pert_cat']])\n",
    "    out = {}\n",
    "    for key in res:\n",
    "        out[key] = res[key][pred_idx]\n",
    "    return out\n",
    "\n",
    "def get_all_vectors(all_res, mean_control, double,\n",
    "                    single1, single2, high_umi_idx):\n",
    "    # Pred\n",
    "    pred_df = pd.DataFrame(all_res['pred'])\n",
    "    pred_df['condition'] = all_res['pert_cat']\n",
    "    subset_df = pred_df[pred_df['condition'] == double].iloc[:, :-1]\n",
    "    delta_double_pred = subset_df.mean(0) - mean_control\n",
    "    single_df_1_pred = pred_df[pred_df['condition'] == single1].iloc[:, :-1]\n",
    "    single_df_2_pred = pred_df[pred_df['condition'] == single2].iloc[:, :-1]\n",
    "\n",
    "    # True\n",
    "    truth_df = pd.DataFrame(all_res['truth'])\n",
    "    truth_df['condition'] = all_res['pert_cat']\n",
    "    subset_df = truth_df[truth_df['condition'] == double].iloc[:, :-1]\n",
    "    delta_double_truth = subset_df.mean(0) - mean_control\n",
    "    single_df_1_truth = truth_df[truth_df['condition'] == single1].iloc[:, :-1]\n",
    "    single_df_2_truth = truth_df[truth_df['condition'] == single2].iloc[:, :-1]\n",
    "\n",
    "    delta_single_truth_1 = single_df_1_truth.mean(0) - mean_control\n",
    "    delta_single_truth_2 = single_df_2_truth.mean(0) - mean_control\n",
    "    delta_single_pred_1 = single_df_1_pred.mean(0) - mean_control\n",
    "    delta_single_pred_2 = single_df_2_pred.mean(0) - mean_control\n",
    "\n",
    "    return {'single_pred_1': delta_single_pred_1.values[high_umi_idx],\n",
    "            'single_pred_2': delta_single_pred_2.values[high_umi_idx],\n",
    "            'double_pred': delta_double_pred.values[high_umi_idx],\n",
    "            'single_truth_1': delta_single_truth_1.values[high_umi_idx],\n",
    "            'single_truth_2': delta_single_truth_2.values[high_umi_idx],\n",
    "            'double_truth': delta_double_truth.values[high_umi_idx]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, sizes, batch_norm=True, last_layer_act=\"linear\"):\n",
    "        \"\"\"\n",
    "        Multi-layer perceptron\n",
    "        :param sizes: list of sizes of the layers\n",
    "        :param batch_norm: whether to use batch normalization\n",
    "        :param last_layer_act: activation function of the last layer\n",
    "\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for s in range(len(sizes) - 1):\n",
    "            layers = layers + [\n",
    "                torch.nn.Linear(sizes[s], sizes[s + 1]),\n",
    "                torch.nn.BatchNorm1d(sizes[s + 1])\n",
    "                if batch_norm and s < len(sizes) - 1 else None,\n",
    "                torch.nn.ReLU()\n",
    "            ]\n",
    "\n",
    "        layers = [l for l in layers if l is not None][:-1]\n",
    "        self.activation = last_layer_act\n",
    "        self.network = torch.nn.Sequential(*layers)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class GEARS_Model(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    GEARS model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        :param args: arguments dictionary\n",
    "        \"\"\"\n",
    "\n",
    "        super(GEARS_Model, self).__init__()\n",
    "        self.args = args       \n",
    "        self.num_genes = args['num_genes']\n",
    "        self.num_perts = args['num_perts']\n",
    "        hidden_size = args['hidden_size']\n",
    "        self.uncertainty = args['uncertainty']\n",
    "        self.num_layers = args['num_go_gnn_layers']\n",
    "        self.indv_out_hidden_size = args['decoder_hidden_size']\n",
    "        self.num_layers_gene_pos = args['num_gene_gnn_layers']\n",
    "        self.no_perturb = args['no_perturb']\n",
    "        self.pert_emb_lambda = 0.2\n",
    "        \n",
    "        # perturbation positional embedding added only to the perturbed genes\n",
    "        self.pert_w = nn.Linear(1, hidden_size)\n",
    "           \n",
    "        # gene/globel perturbation embedding dictionary lookup\n",
    "        ## each gene has its own embedding .            \n",
    "        self.gene_emb = nn.Embedding(self.num_genes, hidden_size, max_norm=True)\n",
    "        ## each perturbation has its own embedding  \n",
    "        self.pert_emb = nn.Embedding(self.num_perts, hidden_size, max_norm=True)\n",
    "        \n",
    "        # transformation layer\n",
    "        self.emb_trans = nn.ReLU()\n",
    "        self.pert_base_trans = nn.ReLU()\n",
    "        self.transform = nn.ReLU()\n",
    "        \n",
    "        self.emb_trans_v2 = MLP([hidden_size, hidden_size, hidden_size], last_layer_act='ReLU')\n",
    "        self.pert_fuse = MLP([hidden_size, hidden_size, hidden_size], last_layer_act='ReLU')\n",
    "        \n",
    "        # gene co-expression GNN\n",
    "        self.G_coexpress = args['G_coexpress'].to(args['device'])\n",
    "        self.G_coexpress_weight = args['G_coexpress_weight'].to(args['device'])\n",
    "\n",
    "        self.emb_pos = nn.Embedding(self.num_genes, hidden_size, max_norm=True)\n",
    "        self.layers_emb_pos = torch.nn.ModuleList()\n",
    "        for i in range(1, self.num_layers_gene_pos + 1):\n",
    "            ## graph convolutional layers.\n",
    "            self.layers_emb_pos.append(SGConv(hidden_size, hidden_size, 1))\n",
    "        \n",
    "        ### perturbation gene ontology GNN\n",
    "        self.G_sim = args['G_go'].to(args['device'])\n",
    "        self.G_sim_weight = args['G_go_weight'].to(args['device'])\n",
    "\n",
    "        self.sim_layers = torch.nn.ModuleList()\n",
    "        for i in range(1, self.num_layers + 1):\n",
    "            self.sim_layers.append(SGConv(hidden_size, hidden_size, 1))\n",
    "        \n",
    "        # decoder shared MLP\n",
    "        self.recovery_w = MLP([hidden_size, hidden_size*2, hidden_size], last_layer_act='linear')\n",
    "        \n",
    "        # gene specific decoder\n",
    "        self.indv_w1 = nn.Parameter(torch.rand(self.num_genes,\n",
    "                                               hidden_size, 1))\n",
    "        self.indv_b1 = nn.Parameter(torch.rand(self.num_genes, 1))\n",
    "        self.act = nn.ReLU()\n",
    "        nn.init.xavier_normal_(self.indv_w1)\n",
    "        nn.init.xavier_normal_(self.indv_b1)\n",
    "        \n",
    "        # Cross gene MLP\n",
    "        self.cross_gene_state = MLP([self.num_genes, hidden_size,\n",
    "                                     hidden_size])\n",
    "        # final gene specific decoder\n",
    "        self.indv_w2 = nn.Parameter(torch.rand(1, self.num_genes,\n",
    "                                           hidden_size+1))\n",
    "        self.indv_b2 = nn.Parameter(torch.rand(1, self.num_genes))\n",
    "        nn.init.xavier_normal_(self.indv_w2)\n",
    "        nn.init.xavier_normal_(self.indv_b2)\n",
    "        \n",
    "        # batchnorms\n",
    "        self.bn_emb = nn.BatchNorm1d(hidden_size)\n",
    "        self.bn_pert_base = nn.BatchNorm1d(hidden_size)\n",
    "        self.bn_pert_base_trans = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # uncertainty mode\n",
    "        if self.uncertainty:\n",
    "            self.uncertainty_w = MLP([hidden_size, hidden_size*2, hidden_size, 1], last_layer_act='linear')\n",
    "        \n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        \"\"\"\n",
    "        x, pert_idx = data.x, data.pert_idx\n",
    "        if self.no_perturb:\n",
    "            out = x.reshape(-1,1)\n",
    "            out = torch.split(torch.flatten(out), self.num_genes)           \n",
    "            return torch.stack(out)\n",
    "        else:\n",
    "            num_graphs = len(data.batch.unique()) # each cell has its own graph\n",
    "            ## get base gene embeddings, num_batch of the same graph.\n",
    "            emb = self.gene_emb(torch.LongTensor(list(range(self.num_genes))).repeat(num_graphs, ).to(self.args['device'])) \n",
    "            emb = self.bn_emb(emb)\n",
    "            base_emb = self.emb_trans(emb)        \n",
    "\n",
    "            ## positional embeddings to differentiate each cell's embedding\n",
    "            pos_emb = self.emb_pos(torch.LongTensor(list(range(self.num_genes))).repeat(num_graphs, ).to(self.args['device']))\n",
    "            for idx, layer in enumerate(self.layers_emb_pos):\n",
    "                # pass in the positional embegginfs through a gcn with the co-expression graph.\n",
    "                pos_emb = layer(pos_emb, self.G_coexpress, self.G_coexpress_weight)\n",
    "                if idx < len(self.layers_emb_pos) - 1:\n",
    "                    # relu till the last layer.\n",
    "                    pos_emb = pos_emb.relu()\n",
    "\n",
    "            # combine base embeddings and positional embeddings\n",
    "            base_emb = base_emb + 0.2 * pos_emb\n",
    "            # pass embeddings for each cell through an mlp\n",
    "            base_emb = self.emb_trans_v2(base_emb)\n",
    "\n",
    "            ## get perturbation index and embeddings\n",
    "\n",
    "            pert_index = []\n",
    "            for idx, i in enumerate(pert_idx):\n",
    "                for j in i:\n",
    "                    if j != -1:\n",
    "                        ## idx indicates which cell, j corresponds to the perturbation number.\n",
    "                        pert_index.append([idx, j])\n",
    "            pert_index = torch.tensor(pert_index).T\n",
    "            ## perturbation embeddings for total number of perturbations to be considered.\n",
    "            pert_global_emb = self.pert_emb(torch.LongTensor(list(range(self.num_perts))).to(self.args['device']))        \n",
    "\n",
    "            ## augment global perturbation embedding with GNN\n",
    "            for idx, layer in enumerate(self.sim_layers):\n",
    "                # GCN with Perturbation graph constructed via Gene-Ontology Network\n",
    "                pert_global_emb = layer(pert_global_emb, self.G_sim, self.G_sim_weight)\n",
    "                if idx < self.num_layers - 1:\n",
    "                    pert_global_emb = pert_global_emb.relu()\n",
    "\n",
    "            ## add global perturbation embedding to each gene in each cell in the batch\n",
    "            base_emb = base_emb.reshape(num_graphs, self.num_genes, -1)\n",
    "\n",
    "            if pert_index.shape[0] != 0:\n",
    "                ### in case all samples in the batch are controls, then there is no indexing for pert_index.\n",
    "                pert_track = {}\n",
    "                for i, j in enumerate(pert_index[0]):\n",
    "                    if j.item() in pert_track:\n",
    "                        pert_track[j.item()] = pert_track[j.item()] + pert_global_emb[pert_index[1][i]]\n",
    "                    else:\n",
    "                        pert_track[j.item()] = pert_global_emb[pert_index[1][i]]\n",
    "\n",
    "                if len(list(pert_track.values())) > 0:\n",
    "                    if len(list(pert_track.values())) == 1:\n",
    "                        # circumvent when batch size = 1 with single perturbation and cannot feed into MLP\n",
    "                        emb_total = self.pert_fuse(torch.stack(list(pert_track.values()) * 2))\n",
    "                    else:\n",
    "                        emb_total = self.pert_fuse(torch.stack(list(pert_track.values())))\n",
    "\n",
    "                    for idx, j in enumerate(pert_track.keys()):\n",
    "                        base_emb[j] = base_emb[j] + emb_total[idx]\n",
    "\n",
    "            base_emb = base_emb.reshape(num_graphs * self.num_genes, -1)\n",
    "            base_emb = self.bn_pert_base(base_emb)\n",
    "\n",
    "            ## apply the first MLP\n",
    "            base_emb = self.transform(base_emb)        \n",
    "            out = self.recovery_w(base_emb)\n",
    "            out = out.reshape(num_graphs, self.num_genes, -1)\n",
    "            out = out.unsqueeze(-1) * self.indv_w1\n",
    "            w = torch.sum(out, axis = 2)\n",
    "            out = w + self.indv_b1\n",
    "\n",
    "            # Cross gene\n",
    "            cross_gene_embed = self.cross_gene_state(out.reshape(num_graphs, self.num_genes, -1).squeeze(2))\n",
    "            cross_gene_embed = cross_gene_embed.repeat(1, self.num_genes)\n",
    "\n",
    "            cross_gene_embed = cross_gene_embed.reshape([num_graphs,self.num_genes, -1])\n",
    "            cross_gene_out = torch.cat([out, cross_gene_embed], 2)\n",
    "\n",
    "            cross_gene_out = cross_gene_out * self.indv_w2\n",
    "            cross_gene_out = torch.sum(cross_gene_out, axis=2)\n",
    "            out = cross_gene_out + self.indv_b2        \n",
    "            out = out.reshape(num_graphs * self.num_genes, -1) + x.reshape(-1,1)\n",
    "            out = torch.split(torch.flatten(out), self.num_genes)\n",
    "\n",
    "            ## uncertainty head\n",
    "            if self.uncertainty:\n",
    "                out_logvar = self.uncertainty_w(base_emb)\n",
    "                out_logvar = torch.split(torch.flatten(out_logvar), self.num_genes)\n",
    "                return torch.stack(out), torch.stack(out_logvar)\n",
    "            \n",
    "            return torch.stack(out)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Have the expression as an embedding thats added to the base embedding of each cell as opposed to perturbation embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "class GEARS_EMBED(GEARS_Model):\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        self.expression_projection = nn.Linear(1,args[\"hidden_size\"])\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        \"\"\"\n",
    "        x, pert_idx = data.x, data.pert_idx\n",
    "        if self.no_perturb:\n",
    "            out = x.reshape(-1,1)\n",
    "            out = torch.split(torch.flatten(out), self.num_genes)           \n",
    "            return torch.stack(out)\n",
    "        else:\n",
    "            num_graphs = len(data.batch.unique()) # each cell has its own graph\n",
    "            ## get base gene embeddings, num_batch of the same graph.\n",
    "            emb = self.gene_emb(torch.LongTensor(list(range(self.num_genes))).repeat(num_graphs, ).to(self.args['device']))\n",
    "            emb = self.bn_emb(emb)\n",
    "            base_emb = self.emb_trans(emb)        \n",
    "            \n",
    "             # ────── 2) per‑gene mean‑centering + projection ────────────────\n",
    "            # reshape x → (B, G, 1)\n",
    "            x_cells = x.view(num_graphs, self.num_genes, 1)\n",
    "            means   = x_cells.mean(dim=0, keepdim=True)  # (1, G, 1)\n",
    "            centered= x_cells - means                    # (B, G, 1)\n",
    "            # back to (B*G, 1) for linear\n",
    "            centered = centered.view(-1, 1)\n",
    "            expr_emb = self.expression_projection(centered)  # (B*G, H)\n",
    "            # fuse expression offset into base\n",
    "            # print(f\"shapes of base embedding {base_emb.shape} \")\n",
    "            # print(f\"shapes of expression embedding {expr_emb}\")\n",
    "            base_emb = base_emb + expr_emb\n",
    "            # positional embeddings to differentiate each cell's embedding\n",
    "            pos_emb = self.emb_pos(torch.LongTensor(list(range(self.num_genes))).repeat(num_graphs, ).to(self.args['device']))\n",
    "            for idx, layer in enumerate(self.layers_emb_pos):\n",
    "                # pass in the positional embegginfs through a gcn with the co-expression graph.\n",
    "                pos_emb = layer(pos_emb, self.G_coexpress, self.G_coexpress_weight)\n",
    "                if idx < len(self.layers_emb_pos) - 1:\n",
    "                    # relu till the last layer.\n",
    "                    pos_emb = pos_emb.relu()\n",
    "                    \n",
    "            # combine base embeddings and positional embeddings\n",
    "            base_emb = base_emb  + 0.2 * pos_emb\n",
    "            # pass embeddings for each cell through an mlp\n",
    "            base_emb = self.emb_trans_v2(base_emb)\n",
    "\n",
    "            ## get perturbation index and embeddings\n",
    "\n",
    "            pert_index = []\n",
    "            for idx, i in enumerate(pert_idx):\n",
    "                for j in i:\n",
    "                    if j != -1:\n",
    "                        ## idx indicates which cell, j corresponds to the perturbation number.\n",
    "                        pert_index.append([idx, j])\n",
    "            pert_index = torch.tensor(pert_index).T\n",
    "            ## perturbation embeddings for total number of perturbations to be considered.\n",
    "            pert_global_emb = self.pert_emb(torch.LongTensor(list(range(self.num_perts))).to(self.args['device']))        \n",
    "\n",
    "            ## augment global perturbation embedding with GNN\n",
    "            for idx, layer in enumerate(self.sim_layers):\n",
    "                # GCN with Perturbation graph constructed via Gene-Ontology Network\n",
    "                pert_global_emb = layer(pert_global_emb, self.G_sim, self.G_sim_weight)\n",
    "                if idx < self.num_layers - 1:\n",
    "                    pert_global_emb = pert_global_emb.relu()\n",
    "\n",
    "            ## add global perturbation embedding to each gene in each cell in the batch\n",
    "            base_emb = base_emb.reshape(num_graphs, self.num_genes, -1)\n",
    "\n",
    "            if pert_index.shape[0] != 0:\n",
    "                ### in case all samples in the batch are controls, then there is no indexing for pert_index.\n",
    "                pert_track = {}\n",
    "                for i, j in enumerate(pert_index[0]):\n",
    "                    if j.item() in pert_track:\n",
    "                        pert_track[j.item()] = pert_track[j.item()] + pert_global_emb[pert_index[1][i]]\n",
    "                    else:\n",
    "                        pert_track[j.item()] = pert_global_emb[pert_index[1][i]]\n",
    "\n",
    "                if len(list(pert_track.values())) > 0:\n",
    "                    if len(list(pert_track.values())) == 1:\n",
    "                        # circumvent when batch size = 1 with single perturbation and cannot feed into MLP\n",
    "                        emb_total = self.pert_fuse(torch.stack(list(pert_track.values()) * 2))\n",
    "                    else:\n",
    "                        emb_total = self.pert_fuse(torch.stack(list(pert_track.values())))\n",
    "\n",
    "                    for idx, j in enumerate(pert_track.keys()):\n",
    "                        base_emb[j] = base_emb[j] + emb_total[idx]\n",
    "\n",
    "            base_emb = base_emb.reshape(num_graphs * self.num_genes, -1)\n",
    "            base_emb = self.bn_pert_base(base_emb)\n",
    "\n",
    "            ## apply the first MLP\n",
    "            base_emb = self.transform(base_emb)        \n",
    "            out = self.recovery_w(base_emb)\n",
    "            out = out.reshape(num_graphs, self.num_genes, -1)\n",
    "            out = out.unsqueeze(-1) * self.indv_w1\n",
    "            w = torch.sum(out, axis = 2)\n",
    "            out = w + self.indv_b1\n",
    "\n",
    "            # Cross gene\n",
    "            cross_gene_embed = self.cross_gene_state(out.reshape(num_graphs, self.num_genes, -1).squeeze(2))\n",
    "            cross_gene_embed = cross_gene_embed.repeat(1, self.num_genes)\n",
    "\n",
    "            cross_gene_embed = cross_gene_embed.reshape([num_graphs,self.num_genes, -1])\n",
    "            cross_gene_out = torch.cat([out, cross_gene_embed], 2)\n",
    "\n",
    "            cross_gene_out = cross_gene_out * self.indv_w2\n",
    "            cross_gene_out = torch.sum(cross_gene_out, axis=2)\n",
    "            out = cross_gene_out + self.indv_b2        \n",
    "            out = out.reshape(num_graphs * self.num_genes, -1) + x.reshape(-1,1)\n",
    "            out = torch.split(torch.flatten(out), self.num_genes)\n",
    "\n",
    "            ## uncertainty head\n",
    "            if self.uncertainty:\n",
    "                out_logvar = self.uncertainty_w(base_emb)\n",
    "                out_logvar = torch.split(torch.flatten(out_logvar), self.num_genes)\n",
    "                return torch.stack(out), torch.stack(out_logvar)\n",
    "            \n",
    "            return torch.stack(out)\n",
    "  \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Graph Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class GEARS_GAT(GEARS_Model):\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        \n",
    "        # GAT layers for co-expression GNN\n",
    "        self.layers_emb_pos = torch.nn.ModuleList()\n",
    "        for i in range(1, self.num_layers_gene_pos + 1):\n",
    "            self.layers_emb_pos.append(GATConv(args['hidden_size'], args['hidden_size'], heads=1))\n",
    "            \n",
    "        # GAT layers for perturbation similarity GNN\n",
    "        self.sim_layers = torch.nn.ModuleList()\n",
    "        for i in range(1, self.num_layers + 1):\n",
    "            self.sim_layers.append(GATConv(args['hidden_size'], args['hidden_size'], heads=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. TransformerConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import TransformerConv\n",
    "\n",
    "class GEARS_Transformer(GEARS_Model):\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        \n",
    "        # Transformer layers for co-expression GNN\n",
    "        self.layers_emb_pos = torch.nn.ModuleList()\n",
    "        for i in range(1, self.num_layers_gene_pos + 1):\n",
    "            self.layers_emb_pos.append(TransformerConv(args['hidden_size'], args['hidden_size'], heads=1))\n",
    "            \n",
    "        # Transformer layers for perturbation similarity GNN\n",
    "        self.sim_layers = torch.nn.ModuleList()\n",
    "        for i in range(1, self.num_layers + 1):\n",
    "            self.sim_layers.append(TransformerConv(args['hidden_size'], args['hidden_size'], heads=1))\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        \"\"\"\n",
    "        x, pert_idx = data.x, data.pert_idx\n",
    "        if self.no_perturb:\n",
    "            out = x.reshape(-1,1)\n",
    "            out = torch.split(torch.flatten(out), self.num_genes)           \n",
    "            return torch.stack(out)\n",
    "        else:\n",
    "            num_graphs = len(data.batch.unique()) # each cell has its own graph\n",
    "            ## get base gene embeddings, num_batch of the same graph.\n",
    "            emb = self.gene_emb(torch.LongTensor(list(range(self.num_genes))).repeat(num_graphs, ).to(self.args['device'])) \n",
    "            emb = self.bn_emb(emb)\n",
    "            base_emb = self.emb_trans(emb)        \n",
    "\n",
    "            ## positional embeddings to differentiate each cell's embedding\n",
    "            pos_emb = self.emb_pos(torch.LongTensor(list(range(self.num_genes))).repeat(num_graphs, ).to(self.args['device']))\n",
    "            for idx, layer in enumerate(self.layers_emb_pos):\n",
    "                # pass in the positional embegginfs through a gcn with the co-expression graph.\n",
    "                pos_emb = layer(pos_emb, self.G_coexpress)\n",
    "                if idx < len(self.layers_emb_pos) - 1:\n",
    "                    # relu till the last layer.\n",
    "                    pos_emb = pos_emb.relu()\n",
    "\n",
    "            # combine base embeddings and positional embeddings\n",
    "            base_emb = base_emb + 0.2 * pos_emb\n",
    "            # pass embeddings for each cell through an mlp\n",
    "            base_emb = self.emb_trans_v2(base_emb)\n",
    "\n",
    "            ## get perturbation index and embeddings\n",
    "\n",
    "            pert_index = []\n",
    "            for idx, i in enumerate(pert_idx):\n",
    "                for j in i:\n",
    "                    if j != -1:\n",
    "                        ## idx indicates which cell, j corresponds to the perturbation number.\n",
    "                        pert_index.append([idx, j])\n",
    "            pert_index = torch.tensor(pert_index).T\n",
    "            ## perturbation embeddings for total number of perturbations to be considered.\n",
    "            pert_global_emb = self.pert_emb(torch.LongTensor(list(range(self.num_perts))).to(self.args['device']))        \n",
    "\n",
    "            ## augment global perturbation embedding with GNN\n",
    "            for idx, layer in enumerate(self.sim_layers):\n",
    "                # GCN with Perturbation graph constructed via Gene-Ontology Network\n",
    "                pert_global_emb = layer(pert_global_emb, self.G_sim)\n",
    "                if idx < self.num_layers - 1:\n",
    "                    pert_global_emb = pert_global_emb.relu()\n",
    "\n",
    "            ## add global perturbation embedding to each gene in each cell in the batch\n",
    "            base_emb = base_emb.reshape(num_graphs, self.num_genes, -1)\n",
    "\n",
    "            if pert_index.shape[0] != 0:\n",
    "                ### in case all samples in the batch are controls, then there is no indexing for pert_index.\n",
    "                pert_track = {}\n",
    "                for i, j in enumerate(pert_index[0]):\n",
    "                    if j.item() in pert_track:\n",
    "                        pert_track[j.item()] = pert_track[j.item()] + pert_global_emb[pert_index[1][i]]\n",
    "                    else:\n",
    "                        pert_track[j.item()] = pert_global_emb[pert_index[1][i]]\n",
    "\n",
    "                if len(list(pert_track.values())) > 0:\n",
    "                    if len(list(pert_track.values())) == 1:\n",
    "                        # circumvent when batch size = 1 with single perturbation and cannot feed into MLP\n",
    "                        emb_total = self.pert_fuse(torch.stack(list(pert_track.values()) * 2))\n",
    "                    else:\n",
    "                        emb_total = self.pert_fuse(torch.stack(list(pert_track.values())))\n",
    "\n",
    "                    for idx, j in enumerate(pert_track.keys()):\n",
    "                        base_emb[j] = base_emb[j] + emb_total[idx]\n",
    "\n",
    "            base_emb = base_emb.reshape(num_graphs * self.num_genes, -1)\n",
    "            base_emb = self.bn_pert_base(base_emb)\n",
    "\n",
    "            ## apply the first MLP\n",
    "            base_emb = self.transform(base_emb)        \n",
    "            out = self.recovery_w(base_emb)\n",
    "            out = out.reshape(num_graphs, self.num_genes, -1)\n",
    "            out = out.unsqueeze(-1) * self.indv_w1\n",
    "            w = torch.sum(out, axis = 2)\n",
    "            out = w + self.indv_b1\n",
    "\n",
    "            # Cross gene\n",
    "            cross_gene_embed = self.cross_gene_state(out.reshape(num_graphs, self.num_genes, -1).squeeze(2))\n",
    "            cross_gene_embed = cross_gene_embed.repeat(1, self.num_genes)\n",
    "\n",
    "            cross_gene_embed = cross_gene_embed.reshape([num_graphs,self.num_genes, -1])\n",
    "            cross_gene_out = torch.cat([out, cross_gene_embed], 2)\n",
    "\n",
    "            cross_gene_out = cross_gene_out * self.indv_w2\n",
    "            cross_gene_out = torch.sum(cross_gene_out, axis=2)\n",
    "            out = cross_gene_out + self.indv_b2        \n",
    "            out = out.reshape(num_graphs * self.num_genes, -1) + x.reshape(-1,1)\n",
    "            out = torch.split(torch.flatten(out), self.num_genes)\n",
    "\n",
    "            ## uncertainty head\n",
    "            if self.uncertainty:\n",
    "                out_logvar = self.uncertainty_w(base_emb)\n",
    "                out_logvar = torch.split(torch.flatten(out_logvar), self.num_genes)\n",
    "                return torch.stack(out), torch.stack(out_logvar)\n",
    "            \n",
    "            return torch.stack(out)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. No gene-coexpression graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GEARS_No_Coexpress(GEARS_Model):\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        self.layers_emb_pos = torch.nn.ModuleList() # Empty module list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. No perturbation Coexpression Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GEARS_No_Perturb(GEARS_Model):\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        self.sim_layers = torch.nn.ModuleList()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class GEARS_SelfAttn(GEARS_Model):\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        self.cross_gene_attn = nn.MultiheadAttention(embed_dim=self.num_genes,num_heads=args[\"num_heads\"])\n",
    "    def forward(self,data):\n",
    "        x, pert_idx = data.x, data.pert_idx\n",
    "        if self.no_perturb:\n",
    "            out = x.reshape(-1,1)\n",
    "            out = torch.split(torch.flatten(out), self.num_genes)           \n",
    "            return torch.stack(out)\n",
    "        else:\n",
    "            num_graphs = len(data.batch.unique()) # each cell has its own graph\n",
    "            ## get base gene embeddings, num_batch of the same graph.\n",
    "            emb = self.gene_emb(torch.LongTensor(list(range(self.num_genes))).repeat(num_graphs, ).to(self.args['device'])) \n",
    "            emb = self.bn_emb(emb)\n",
    "            base_emb = self.emb_trans(emb)        \n",
    "\n",
    "            ## positional embeddings to differentiate each cell's embedding\n",
    "            pos_emb = self.emb_pos(torch.LongTensor(list(range(self.num_genes))).repeat(num_graphs, ).to(self.args['device']))\n",
    "            for idx, layer in enumerate(self.layers_emb_pos):\n",
    "                # pass in the positional embegginfs through a gcn with the co-expression graph.\n",
    "                pos_emb = layer(pos_emb, self.G_coexpress, self.G_coexpress_weight)\n",
    "                if idx < len(self.layers_emb_pos) - 1:\n",
    "                    # relu till the last layer.\n",
    "                    pos_emb = pos_emb.relu()\n",
    "\n",
    "            # combine base embeddings and positional embeddings\n",
    "            base_emb = base_emb + 0.2 * pos_emb\n",
    "            # pass embeddings for each cell through an mlp\n",
    "            base_emb = self.emb_trans_v2(base_emb)\n",
    "\n",
    "            ## get perturbation index and embeddings\n",
    "\n",
    "            pert_index = []\n",
    "            for idx, i in enumerate(pert_idx):\n",
    "                for j in i:\n",
    "                    if j != -1:\n",
    "                        ## idx indicates which cell, j corresponds to the perturbation number.\n",
    "                        pert_index.append([idx, j])\n",
    "            pert_index = torch.tensor(pert_index).T\n",
    "            ## perturbation embeddings for total number of perturbations to be considered.\n",
    "            pert_global_emb = self.pert_emb(torch.LongTensor(list(range(self.num_perts))).to(self.args['device']))        \n",
    "\n",
    "            ## augment global perturbation embedding with GNN\n",
    "            for idx, layer in enumerate(self.sim_layers):\n",
    "                # GCN with Perturbation graph constructed via Gene-Ontology Network\n",
    "                pert_global_emb = layer(pert_global_emb, self.G_sim, self.G_sim_weight)\n",
    "                if idx < self.num_layers - 1:\n",
    "                    pert_global_emb = pert_global_emb.relu()\n",
    "\n",
    "            ## add global perturbation embedding to each gene in each cell in the batch\n",
    "            base_emb = base_emb.reshape(num_graphs, self.num_genes, -1)\n",
    "\n",
    "            if pert_index.shape[0] != 0:\n",
    "                ### in case all samples in the batch are controls, then there is no indexing for pert_index.\n",
    "                pert_track = {}\n",
    "                for i, j in enumerate(pert_index[0]):\n",
    "                    if j.item() in pert_track:\n",
    "                        pert_track[j.item()] = pert_track[j.item()] + pert_global_emb[pert_index[1][i]]\n",
    "                    else:\n",
    "                        pert_track[j.item()] = pert_global_emb[pert_index[1][i]]\n",
    "\n",
    "                if len(list(pert_track.values())) > 0:\n",
    "                    if len(list(pert_track.values())) == 1:\n",
    "                        # circumvent when batch size = 1 with single perturbation and cannot feed into MLP\n",
    "                        emb_total = self.pert_fuse(torch.stack(list(pert_track.values()) * 2))\n",
    "                    else:\n",
    "                        emb_total = self.pert_fuse(torch.stack(list(pert_track.values())))\n",
    "\n",
    "                    for idx, j in enumerate(pert_track.keys()):\n",
    "                        base_emb[j] = base_emb[j] + emb_total[idx]\n",
    "\n",
    "            base_emb = base_emb.reshape(num_graphs * self.num_genes, -1)\n",
    "            base_emb = self.bn_pert_base(base_emb)\n",
    "\n",
    "            ## apply the first MLP\n",
    "            base_emb = self.transform(base_emb)        \n",
    "            out = self.recovery_w(base_emb)\n",
    "            out = out.reshape(num_graphs, self.num_genes, -1)\n",
    "            out = out.unsqueeze(-1) * self.indv_w1\n",
    "            w = torch.sum(out, axis = 2)\n",
    "            out = w + self.indv_b1\n",
    "\n",
    "            # Cross gene\n",
    "            outpass = out.reshape(num_graphs, self.num_genes, -1).squeeze(2)\n",
    "            cross_gene_op,cross_gene_attn = self.cross_gene_attn(outpass,outpass,outpass)\n",
    "            cross_gene_embed = self.cross_gene_state(cross_gene_op)\n",
    "            cross_gene_embed = cross_gene_embed.repeat(1, self.num_genes)\n",
    "\n",
    "            cross_gene_embed = cross_gene_embed.reshape([num_graphs,self.num_genes, -1])\n",
    "            cross_gene_out = torch.cat([out, cross_gene_embed], 2)\n",
    "\n",
    "            cross_gene_out = cross_gene_out * self.indv_w2\n",
    "            cross_gene_out = torch.sum(cross_gene_out, axis=2)\n",
    "            out = cross_gene_out + self.indv_b2        \n",
    "            out = out.reshape(num_graphs * self.num_genes, -1) + x.reshape(-1,1)\n",
    "            out = torch.split(torch.flatten(out), self.num_genes)\n",
    "\n",
    "            ## uncertainty head\n",
    "            if self.uncertainty:\n",
    "                out_logvar = self.uncertainty_w(base_emb)\n",
    "                out_logvar = torch.split(torch.flatten(out_logvar), self.num_genes)\n",
    "                return torch.stack(out), torch.stack(out_logvar)\n",
    "            \n",
    "            return torch.stack(out)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deg_score(results,adata:ad.AnnData,k:int=20):\n",
    "    control_expressions = adata[adata.obs[\"condition\"]==\"ctrl\"].copy()\n",
    "    pertwise_scores=[]\n",
    "    for pert in np.unique(results[\"pert_cat\"]):\n",
    "        print(f\"For perturbation {pert}:\")\n",
    "        p_idx = np.where(results['pert_cat'] == pert)[0]\n",
    "        perturbed_expressions = results[\"pred\"][p_idx]\n",
    "        perturbed_adata= ad.AnnData(X=perturbed_expressions,obs=control_expressions.obs_names,var=control_expressions.var_names)\n",
    "        perturbed_adata.obs[\"condition\"] = f\"ctrl+{pert}\"\n",
    "        test_adata  = ad.concat(control_expressions,perturbed_adata)\n",
    "        sc.tl.rank_genes_groups(test_adata)\n",
    "        ## computing  the differentially expressed genes between perturbed_expressions and control, and true_expressions and control.\n",
    "        sc.tl.rank_genes_groups(test_adata,groupby=\"condition\")\n",
    "        sc.tl.rank_genes_groups(adata,groupby=\"condition\",groups=[\"ctrl\",f\"ctrl+{pert}\"],key_added=f\"ctrlvs{pert}\")\n",
    "        top20pred = test_adata[\"rank_genes_groups\"][\"names\"][:k]\n",
    "        top20truth = adata[f\"ctrlvs{pert}\"][\"names\"][:k]\n",
    "        pert_score = len(set(top20pred).intersection(set(top20truth)))\n",
    "        print(f\"For {pert} deg score is {pert_score}\")\n",
    "        pertwise_scores.append(pert_score)\n",
    "    findeg_score = np.average(pertwise_scores)\n",
    "    print(f\"Final DEG score is {findeg_score}\")\n",
    "    return pertwise_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gears api\n",
    "class GEARS:\n",
    "    \"\"\"\n",
    "    GEARS base model class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pert_data, \n",
    "                 device = 'cuda',\n",
    "                 weight_bias_track = False, \n",
    "                 proj_name = 'GEARS', \n",
    "                 exp_name = 'GEARS'):\n",
    "        \"\"\"\n",
    "        Initialize GEARS model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pert_data: PertData object\n",
    "            dataloader for perturbation data\n",
    "        device: str\n",
    "            Device to run the model on. Default: 'cuda'\n",
    "        weight_bias_track: bool\n",
    "            Whether to track performance on wandb. Default: False\n",
    "        proj_name: str\n",
    "            Project name for wandb. Default: 'GEARS'\n",
    "        exp_name: str\n",
    "            Experiment name for wandb. Default: 'GEARS'\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.weight_bias_track = weight_bias_track\n",
    "        \n",
    "        if self.weight_bias_track:\n",
    "            import wandb\n",
    "            wandb.init(project=proj_name, name=exp_name)  \n",
    "            self.wandb = wandb\n",
    "        else:\n",
    "            self.wandb = None\n",
    "        \n",
    "        self.device = device\n",
    "        self.config = None\n",
    "        \n",
    "        self.dataloader = pert_data.dataloader ## \n",
    "        self.adata = pert_data.adata\n",
    "        self.node_map = pert_data.node_map\n",
    "        self.node_map_pert = pert_data.node_map_pert\n",
    "        self.data_path = pert_data.data_path\n",
    "        self.dataset_name = pert_data.dataset_name\n",
    "        self.split = pert_data.split\n",
    "        self.seed = pert_data.seed\n",
    "        self.train_gene_set_size = pert_data.train_gene_set_size\n",
    "        self.set2conditions = pert_data.set2conditions\n",
    "        self.subgroup = pert_data.subgroup\n",
    "        self.gene_list = pert_data.gene_names.values.tolist()\n",
    "        self.pert_list = pert_data.pert_names.tolist()\n",
    "        self.num_genes = len(self.gene_list)\n",
    "        self.num_perts = len(self.pert_list)\n",
    "        self.default_pert_graph = pert_data.default_pert_graph\n",
    "        self.saved_pred = {}\n",
    "        self.saved_logvar_sum = {}\n",
    "        \n",
    "        self.ctrl_expression = torch.tensor(\n",
    "            np.mean(self.adata.X[self.adata.obs.condition.values == 'ctrl'],\n",
    "                    axis=0)).reshape(-1, ).to(self.device)\n",
    "        pert_full_id2pert = dict(self.adata.obs[['condition_name', 'condition']].values)\n",
    "        self.dict_filter = {pert_full_id2pert[i]: j for i, j in\n",
    "                            self.adata.uns['non_zeros_gene_idx'].items() if\n",
    "                            i in pert_full_id2pert}\n",
    "        self.ctrl_adata = self.adata[self.adata.obs['condition'] == 'ctrl']\n",
    "        \n",
    "        gene_dict = {g:i for i,g in enumerate(self.gene_list)}\n",
    "        self.pert2gene = {p: gene_dict[pert] for p, pert in\n",
    "                          enumerate(self.pert_list) if pert in self.gene_list}\n",
    "\n",
    "\n",
    "    def tunable_parameters(self):\n",
    "        \"\"\"\n",
    "        Return the tunable parameters of the model\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Tunable parameters of the model\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return {'hidden_size': 'hidden dimension, default 64',\n",
    "                'num_go_gnn_layers': 'number of GNN layers for GO graph, default 1',\n",
    "                'num_gene_gnn_layers': 'number of GNN layers for co-expression gene graph, default 1',\n",
    "                'decoder_hidden_size': 'hidden dimension for gene-specific decoder, default 16',\n",
    "                'num_similar_genes_go_graph': 'number of maximum similar K genes in the GO graph, default 20',\n",
    "                'num_similar_genes_co_express_graph': 'number of maximum similar K genes in the co expression graph, default 20',\n",
    "                'coexpress_threshold': 'pearson correlation threshold when constructing coexpression graph, default 0.4',\n",
    "                'uncertainty': 'whether or not to turn on uncertainty mode, default False',\n",
    "                'uncertainty_reg': 'regularization term to balance uncertainty loss and prediction loss, default 1',\n",
    "                'direction_lambda': 'regularization term to balance direction loss and prediction loss, default 1'\n",
    "               }\n",
    "    \n",
    "    def model_initialize(self, hidden_size = 64,\n",
    "                         num_go_gnn_layers = 1, \n",
    "                         num_gene_gnn_layers = 1,\n",
    "                         decoder_hidden_size = 16,\n",
    "                         num_similar_genes_go_graph = 20,\n",
    "                         num_similar_genes_co_express_graph = 20,                    \n",
    "                         coexpress_threshold = 0.4,\n",
    "                         uncertainty = False, \n",
    "                         uncertainty_reg = 1,\n",
    "                         direction_lambda = 1e-1,\n",
    "                         G_go = None,\n",
    "                         G_go_weight = None,\n",
    "                         G_coexpress = None,\n",
    "                         G_coexpress_weight = None,\n",
    "                         no_perturb = False,\n",
    "                         gears_model=0,\n",
    "                         num_heads=4, \n",
    "                         **kwargs\n",
    "                        ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hidden_size: int\n",
    "            hidden dimension, default 64\n",
    "        num_go_gnn_layers: int\n",
    "            number of GNN layers for GO graph, default 1\n",
    "        num_gene_gnn_layers: int\n",
    "            number of GNN layers for co-expression gene graph, default 1\n",
    "        decoder_hidden_size: int\n",
    "            hidden dimension for gene-specific decoder, default 16\n",
    "        num_similar_genes_go_graph: int\n",
    "            number of maximum similar K genes in the GO graph, default 20\n",
    "        num_similar_genes_co_express_graph: int\n",
    "            number of maximum similar K genes in the co expression graph, default 20\n",
    "        coexpress_threshold: float\n",
    "            pearson correlation threshold when constructing coexpression graph, default 0.4\n",
    "        uncertainty: bool\n",
    "            whether or not to turn on uncertainty mode, default False\n",
    "        uncertainty_reg: float\n",
    "            regularization term to balance uncertainty loss and prediction loss, default 1\n",
    "        direction_lambda: float\n",
    "            regularization term to balance direction loss and prediction loss, default 1\n",
    "        G_go: scipy.sparse.csr_matrix\n",
    "            GO graph, default None\n",
    "        G_go_weight: scipy.sparse.csr_matrix\n",
    "            GO graph edge weights, default None\n",
    "        G_coexpress: scipy.sparse.csr_matrix\n",
    "            co-expression graph, default None\n",
    "        G_coexpress_weight: scipy.sparse.csr_matrix\n",
    "            co-expression graph edge weights, default None\n",
    "        no_perturb: bool\n",
    "            predict no perturbation condition, default False\n",
    "        gears_model: int \n",
    "            0- original model, 1- expression embedding, 2 - GAT, 3 - TransformerConv, 4- No Coexpression 5- No perturbation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        \n",
    "        self.config = {'hidden_size': hidden_size,\n",
    "                       'num_go_gnn_layers' : num_go_gnn_layers, \n",
    "                       'num_gene_gnn_layers' : num_gene_gnn_layers,\n",
    "                       'decoder_hidden_size' : decoder_hidden_size,\n",
    "                       'num_similar_genes_go_graph' : num_similar_genes_go_graph,\n",
    "                       'num_similar_genes_co_express_graph' : num_similar_genes_co_express_graph,\n",
    "                       'coexpress_threshold': coexpress_threshold,\n",
    "                       'uncertainty' : uncertainty, \n",
    "                       'uncertainty_reg' : uncertainty_reg,\n",
    "                       'direction_lambda' : direction_lambda,\n",
    "                       'G_go': G_go,\n",
    "                       'G_go_weight': G_go_weight,\n",
    "                       'G_coexpress': G_coexpress,\n",
    "                       'G_coexpress_weight': G_coexpress_weight,\n",
    "                       'device': self.device,\n",
    "                       'num_genes': self.num_genes,\n",
    "                       'num_perts': self.num_perts,\n",
    "                       'no_perturb': no_perturb,\n",
    "                       'gears_model': gears_model,\n",
    "                       'num_heads': num_heads,\n",
    "                      }\n",
    "        \n",
    "        if self.wandb:\n",
    "            self.wandb.config.update(self.config)\n",
    "        \n",
    "        if self.config['G_coexpress'] is None:\n",
    "            ## calculating co expression similarity graph\n",
    "            edge_list = get_similarity_network(network_type='co-express',\n",
    "                                               adata=self.adata,\n",
    "                                               threshold=coexpress_threshold,\n",
    "                                               k=num_similar_genes_co_express_graph,\n",
    "                                               data_path=self.data_path,\n",
    "                                               data_name=self.dataset_name,\n",
    "                                               split=self.split, seed=self.seed,\n",
    "                                               train_gene_set_size=self.train_gene_set_size,\n",
    "                                               set2conditions=self.set2conditions)\n",
    "\n",
    "            sim_network = GeneSimNetwork(edge_list, self.gene_list, node_map = self.node_map)\n",
    "            self.config['G_coexpress'] = sim_network.edge_index\n",
    "            self.config['G_coexpress_weight'] = sim_network.edge_weight\n",
    "        \n",
    "        if self.config['G_go'] is None:\n",
    "            ## calculating gene ontology similarity graph\n",
    "            edge_list = get_similarity_network(network_type='go',\n",
    "                                               adata=self.adata,\n",
    "                                               threshold=coexpress_threshold,\n",
    "                                               k=num_similar_genes_go_graph,\n",
    "                                               pert_list=self.pert_list,\n",
    "                                               data_path=self.data_path,\n",
    "                                               data_name=self.dataset_name,\n",
    "                                               split=self.split, seed=self.seed,\n",
    "                                               train_gene_set_size=self.train_gene_set_size,\n",
    "                                               set2conditions=self.set2conditions,\n",
    "                                               default_pert_graph=self.default_pert_graph)\n",
    "\n",
    "            sim_network = GeneSimNetwork(edge_list, self.pert_list, node_map = self.node_map_pert)\n",
    "            self.config['G_go'] = sim_network.edge_index\n",
    "            self.config['G_go_weight'] = sim_network.edge_weight\n",
    "            \n",
    "        if self.config[\"gears_model\"] == 0 :\n",
    "            self.model = GEARS_Model(self.config).to(self.device)\n",
    "        elif self.config[\"gears_model\"] == 1:\n",
    "            self.model = GEARS_EMBED(self.config).to(self.device)\n",
    "        elif self.config[\"gears_model\"] == 2:\n",
    "            self.model = GEARS_GAT(self.config).to(self.device)\n",
    "        elif self.config[\"gears_model\"] == 3:\n",
    "            self.model = GEARS_Transformer(self.config).to(self.device)\n",
    "        elif self.config[\"gears_model\"] == 4:\n",
    "            self.model = GEARS_No_Coexpress(self.config).to(self.device)\n",
    "        elif self.config[\"gears_model\"] == 5:\n",
    "            self.model = GEARS_No_Perturb(self.config).to(self.device)\n",
    "        elif self.config[\"gears_model\"] == 6:\n",
    "            self.model = GEARS_SelfAttn(self.config).to(self.device)            \n",
    "            \n",
    "        self.best_model = deepcopy(self.model)\n",
    "        \n",
    "    def load_pretrained(self, path):\n",
    "        \"\"\"\n",
    "        Load pretrained model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path: str\n",
    "            path to the pretrained model\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        with open(os.path.join(path, 'config.pkl'), 'rb') as f:\n",
    "            config = pickle.load(f)\n",
    "        \n",
    "        del config['device'], config['num_genes'], config['num_perts']\n",
    "        self.model_initialize(**config)\n",
    "        self.config = config\n",
    "        \n",
    "        state_dict = torch.load(os.path.join(path, 'model.pt'), map_location = torch.device('cpu'))\n",
    "        if next(iter(state_dict))[:7] == 'module.':\n",
    "            # the pretrained model is from data-parallel module\n",
    "            from collections import OrderedDict\n",
    "            new_state_dict = OrderedDict()\n",
    "            for k, v in state_dict.items():\n",
    "                name = k[7:] # remove `module.`\n",
    "                new_state_dict[name] = v\n",
    "            state_dict = new_state_dict\n",
    "        \n",
    "        self.model.load_state_dict(state_dict)\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.best_model = self.model\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        \"\"\"\n",
    "        Save the model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path: str\n",
    "            path to save the model\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        \n",
    "        if self.config is None:\n",
    "            raise ValueError('No model is initialized...')\n",
    "        \n",
    "        with open(os.path.join(path, 'config.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.config, f)\n",
    "       \n",
    "        torch.save(self.best_model.state_dict(), os.path.join(path, 'model.pt'))\n",
    "    \n",
    "    def predict(self, pert_list):\n",
    "        \"\"\"\n",
    "        Predict the transcriptome given a list of genes/gene combinations being\n",
    "        perturbed\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pert_list: list\n",
    "            list of genes/gene combiantions to be perturbed\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        results_pred: dict\n",
    "            dictionary of predicted transcriptome\n",
    "        results_logvar: dict\n",
    "            dictionary of uncertainty score\n",
    "\n",
    "        \"\"\"\n",
    "        ## given a list of single/combo genes, return the transcriptome\n",
    "        ## if uncertainty mode is on, also return uncertainty score.\n",
    "        \n",
    "        self.ctrl_adata = self.adata[self.adata.obs['condition'] == 'ctrl']\n",
    "        for pert in pert_list:\n",
    "            for i in pert:\n",
    "                if i not in self.pert_list:\n",
    "                    raise ValueError(i+ \" is not in the perturbation graph. \"\n",
    "                                        \"Please select from GEARS.pert_list!\")\n",
    "        \n",
    "        if self.config['uncertainty']:\n",
    "            results_logvar = {}\n",
    "            \n",
    "        self.best_model = self.best_model.to(self.device)\n",
    "        self.best_model.eval()\n",
    "        results_pred = {}\n",
    "        results_logvar_sum = {}\n",
    "        \n",
    "        from torch_geometric.data import DataLoader\n",
    "        for pert in pert_list:\n",
    "            try:\n",
    "                #If prediction is already saved, then skip inference\n",
    "                results_pred['_'.join(pert)] = self.saved_pred['_'.join(pert)]\n",
    "                if self.config['uncertainty']:\n",
    "                    results_logvar_sum['_'.join(pert)] = self.saved_logvar_sum['_'.join(pert)]\n",
    "                continue\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            cg = create_cell_graph_dataset_for_prediction(pert, self.ctrl_adata,\n",
    "                                                    self.pert_list, self.device)\n",
    "            loader = DataLoader(cg, 300, shuffle = False)\n",
    "            batch = next(iter(loader))\n",
    "            batch.to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if self.config['uncertainty']:\n",
    "                    p, unc = self.best_model(batch)\n",
    "                    results_logvar['_'.join(pert)] = np.mean(unc.detach().cpu().numpy(), axis = 0)\n",
    "                    results_logvar_sum['_'.join(pert)] = np.exp(-np.mean(results_logvar['_'.join(pert)]))\n",
    "                else:\n",
    "                    p = self.best_model(batch)\n",
    "                    \n",
    "            results_pred['_'.join(pert)] = np.mean(p.detach().cpu().numpy(), axis = 0)\n",
    "                \n",
    "        self.saved_pred.update(results_pred)\n",
    "        \n",
    "        if self.config['uncertainty']:\n",
    "            self.saved_logvar_sum.update(results_logvar_sum)\n",
    "            return results_pred, results_logvar_sum\n",
    "        else:\n",
    "            return results_pred\n",
    "        \n",
    "    def GI_predict(self, combo, GI_genes_file='./genes_with_hi_mean.npy'):\n",
    "        \"\"\"\n",
    "        Predict the GI scores following perturbation of a given gene combination\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        combo: list\n",
    "            list of genes to be perturbed\n",
    "        GI_genes_file: str\n",
    "            path to the file containing genes with high mean expression\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        GI scores for the given combinatorial perturbation based on GEARS\n",
    "        predictions\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        ## if uncertainty mode is on, also return uncertainty score.\n",
    "        try:\n",
    "            # If prediction is already saved, then skip inference\n",
    "            pred = {}\n",
    "            pred[combo[0]] = self.saved_pred[combo[0]]\n",
    "            pred[combo[1]] = self.saved_pred[combo[1]]\n",
    "            pred['_'.join(combo)] = self.saved_pred['_'.join(combo)]\n",
    "        except:\n",
    "            if self.config['uncertainty']:\n",
    "                pred = self.predict([[combo[0]], [combo[1]], combo])[0]\n",
    "            else:\n",
    "                pred = self.predict([[combo[0]], [combo[1]], combo])\n",
    "\n",
    "        mean_control = get_mean_control(self.adata).values  \n",
    "        pred = {p:pred[p]-mean_control for p in pred} \n",
    "\n",
    "        if GI_genes_file is not None:\n",
    "            # If focussing on a specific subset of genes for calculating metrics\n",
    "            GI_genes_idx = get_GI_genes_idx(self.adata, GI_genes_file)       \n",
    "        else:\n",
    "            GI_genes_idx = np.arange(len(self.adata.var.gene_name.values))\n",
    "            \n",
    "        pred = {p:pred[p][GI_genes_idx] for p in pred}\n",
    "        return get_GI_params(pred, combo)\n",
    "    \n",
    "    def plot_perturbation(self, query, save_file = None):\n",
    "        \"\"\"\n",
    "        Plot the perturbation graph\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query: str\n",
    "            condition to be queried\n",
    "        save_file: str\n",
    "            path to save the plot\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        import seaborn as sns\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        sns.set_theme(style=\"ticks\", rc={\"axes.facecolor\": (0, 0, 0, 0)}, font_scale=1.5)\n",
    "\n",
    "        adata = self.adata\n",
    "        gene2idx = self.node_map\n",
    "        cond2name = dict(adata.obs[['condition', 'condition_name']].values)\n",
    "        gene_raw2id = dict(zip(adata.var.index.values, adata.var.gene_name.values))\n",
    "\n",
    "        de_idx = [gene2idx[gene_raw2id[i]] for i in\n",
    "                  adata.uns['top_non_dropout_de_20'][cond2name[query]]]\n",
    "        genes = [gene_raw2id[i] for i in\n",
    "                 adata.uns['top_non_dropout_de_20'][cond2name[query]]]\n",
    "        truth = adata[adata.obs.condition == query].X.toarray()[:, de_idx]\n",
    "        \n",
    "        query_ = [q for q in query.split('+') if q != 'ctrl']\n",
    "        pred = self.predict([query_])['_'.join(query_)][de_idx]\n",
    "        ctrl_means = adata[adata.obs['condition'] == 'ctrl'].to_df().mean()[\n",
    "            de_idx].values\n",
    "\n",
    "        pred = pred - ctrl_means\n",
    "        truth = truth - ctrl_means\n",
    "        \n",
    "        plt.figure(figsize=[16.5,4.5])\n",
    "        plt.title(query)\n",
    "        plt.boxplot(truth, showfliers=False,\n",
    "                    medianprops = dict(linewidth=0))    \n",
    "\n",
    "        for i in range(pred.shape[0]):\n",
    "            _ = plt.scatter(i+1, pred[i], color='red')\n",
    "\n",
    "        plt.axhline(0, linestyle=\"dashed\", color = 'green')\n",
    "\n",
    "        ax = plt.gca()\n",
    "        ax.xaxis.set_ticklabels(genes, rotation = 90)\n",
    "\n",
    "        plt.ylabel(\"Change in Gene Expression over Control\",labelpad=10)\n",
    "        plt.tick_params(axis='x', which='major', pad=5)\n",
    "        plt.tick_params(axis='y', which='major', pad=5)\n",
    "        sns.despine()\n",
    "        \n",
    "        if save_file:\n",
    "            plt.savefig(save_file, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    def train(self, epochs = 20, \n",
    "              lr = 1e-3,\n",
    "              weight_decay = 5e-4\n",
    "             ):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epochs: int\n",
    "            number of epochs to train\n",
    "        lr: float\n",
    "            learning rate\n",
    "        weight_decay: float\n",
    "            weight decay\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        train_loader = self.dataloader['train_loader']\n",
    "        val_loader = self.dataloader['val_loader']\n",
    "            \n",
    "        self.model = self.model.to(self.device)\n",
    "        best_model = deepcopy(self.model)\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "        scheduler = StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "\n",
    "        min_val = np.inf\n",
    "        print_sys('Start Training...')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "\n",
    "            for step, batch in enumerate(train_loader):\n",
    "                batch.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                y = batch.y\n",
    "                if self.config['uncertainty']:\n",
    "                    pred, logvar = self.model(batch)\n",
    "                    loss = uncertainty_loss_fct(pred, logvar, y, batch.pert,\n",
    "                                      reg = self.config['uncertainty_reg'],\n",
    "                                      ctrl = self.ctrl_expression, \n",
    "                                      dict_filter = self.dict_filter,\n",
    "                                      direction_lambda = self.config['direction_lambda'])\n",
    "                else:\n",
    "                    pred = self.model(batch)\n",
    "                    loss = loss_fct(pred, y, batch.pert,\n",
    "                                  ctrl = self.ctrl_expression, \n",
    "                                  dict_filter = self.dict_filter,\n",
    "                                  direction_lambda = self.config['direction_lambda'])\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_value_(self.model.parameters(), clip_value=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                if self.wandb:\n",
    "                    self.wandb.log({'training_loss': loss.item()})\n",
    "\n",
    "                if step % 50 == 0:\n",
    "                    log = \"Epoch {} Step {} Train Loss: {:.4f}\" \n",
    "                    print_sys(log.format(epoch + 1, step + 1, loss.item()))\n",
    "\n",
    "            scheduler.step()\n",
    "            # Evaluate model performance on train and val set\n",
    "            train_res = evaluate(train_loader, self.model,\n",
    "                                 self.config['uncertainty'], self.device)\n",
    "            val_res = evaluate(val_loader, self.model,\n",
    "                                 self.config['uncertainty'], self.device)\n",
    "            \n",
    "            train_metrics, _ = compute_metrics(train_res)\n",
    "            val_metrics, _ = compute_metrics(val_res)\n",
    "\n",
    "            # Print epoch performance\n",
    "            log = \"Epoch {}: Train Overall MSE: {:.4f} \" \\\n",
    "                  \"Validation Overall MSE: {:.4f}. \"\n",
    "            print_sys(log.format(epoch + 1, train_metrics['mse'], \n",
    "                             val_metrics['mse']))\n",
    "            \n",
    "            # Print epoch performance for DE genes\n",
    "            log = \"Train Top 20 DE MSE: {:.4f} \" \\\n",
    "                  \"Validation Top 20 DE MSE: {:.4f}. \"\n",
    "            print_sys(log.format(train_metrics['mse_de'],\n",
    "                             val_metrics['mse_de']))\n",
    "            \n",
    "            if self.wandb:\n",
    "                metrics = ['mse', 'pearson']\n",
    "                for m in metrics:\n",
    "                    self.wandb.log({'train_' + m: train_metrics[m],\n",
    "                               'val_'+m: val_metrics[m],\n",
    "                               'train_de_' + m: train_metrics[m + '_de'],\n",
    "                               'val_de_'+m: val_metrics[m + '_de']})\n",
    "               \n",
    "            if val_metrics['mse_de'] < min_val:\n",
    "                min_val = val_metrics['mse_de']\n",
    "                best_model = deepcopy(self.model)\n",
    "                \n",
    "        print_sys(\"Done!\")\n",
    "        self.best_model = best_model\n",
    "\n",
    "        if 'test_loader' not in self.dataloader:\n",
    "            print_sys('Done! No test dataloader detected.')\n",
    "            return\n",
    "            \n",
    "        # Model testing\n",
    "        test_loader = self.dataloader['test_loader']\n",
    "        print_sys(\"Start Testing...\")\n",
    "        test_res = evaluate(test_loader, self.best_model,\n",
    "                            self.config['uncertainty'], self.device)\n",
    "        test_metrics, test_pert_res = compute_metrics(test_res)    \n",
    "        log = \"Best performing model: Test Top 20 DE MSE: {:.4f}\"\n",
    "        print_sys(log.format(test_metrics['mse_de']))\n",
    "        \n",
    "        if self.wandb:\n",
    "            metrics = ['mse', 'pearson']\n",
    "            for m in metrics:\n",
    "                self.wandb.log({'test_' + m: test_metrics[m],\n",
    "                           'test_de_'+m: test_metrics[m + '_de']                     \n",
    "                          })\n",
    "                \n",
    "        out = deeper_analysis(self.adata, test_res)\n",
    "        out_non_dropout = non_dropout_analysis(self.adata, test_res)\n",
    "        \n",
    "        metrics = ['pearson_delta']\n",
    "        metrics_non_dropout = ['frac_opposite_direction_top20_non_dropout',\n",
    "                               'frac_sigma_below_1_non_dropout',\n",
    "                               'mse_top20_de_non_dropout']\n",
    "        \n",
    "        if self.wandb:\n",
    "            for m in metrics:\n",
    "                self.wandb.log({'test_' + m: np.mean([j[m] for i,j in out.items() if m in j])})\n",
    "\n",
    "            for m in metrics_non_dropout:\n",
    "                self.wandb.log({'test_' + m: np.mean([j[m] for i,j in out_non_dropout.items() if m in j])})        \n",
    "\n",
    "        if self.split == 'simulation':\n",
    "            print_sys(\"Start doing subgroup analysis for simulation split...\")\n",
    "            subgroup = self.subgroup\n",
    "            subgroup_analysis = {}\n",
    "            for name in subgroup['test_subgroup'].keys():\n",
    "                subgroup_analysis[name] = {}\n",
    "                for m in list(list(test_pert_res.values())[0].keys()):\n",
    "                    subgroup_analysis[name][m] = []\n",
    "\n",
    "            for name, pert_list in subgroup['test_subgroup'].items():\n",
    "                for pert in pert_list:\n",
    "                    for m, res in test_pert_res[pert].items():\n",
    "                        subgroup_analysis[name][m].append(res)\n",
    "\n",
    "            for name, result in subgroup_analysis.items():\n",
    "                for m in result.keys():\n",
    "                    subgroup_analysis[name][m] = np.mean(subgroup_analysis[name][m])\n",
    "                    if self.wandb:\n",
    "                        self.wandb.log({'test_' + name + '_' + m: subgroup_analysis[name][m]})\n",
    "\n",
    "                    print_sys('test_' + name + '_' + m + ': ' + str(subgroup_analysis[name][m]))\n",
    "\n",
    "            ## deeper analysis\n",
    "            subgroup_analysis = {}\n",
    "            for name in subgroup['test_subgroup'].keys():\n",
    "                subgroup_analysis[name] = {}\n",
    "                for m in metrics:\n",
    "                    subgroup_analysis[name][m] = []\n",
    "\n",
    "                for m in metrics_non_dropout:\n",
    "                    subgroup_analysis[name][m] = []\n",
    "\n",
    "            for name, pert_list in subgroup['test_subgroup'].items():\n",
    "                for pert in pert_list:\n",
    "                    for m in metrics:\n",
    "                        subgroup_analysis[name][m].append(out[pert][m])\n",
    "\n",
    "                    for m in metrics_non_dropout:\n",
    "                        subgroup_analysis[name][m].append(out_non_dropout[pert][m])\n",
    "\n",
    "            for name, result in subgroup_analysis.items():\n",
    "                for m in result.keys():\n",
    "                    subgroup_analysis[name][m] = np.mean(subgroup_analysis[name][m])\n",
    "                    if self.wandb:\n",
    "                        self.wandb.log({'test_' + name + '_' + m: subgroup_analysis[name][m]})\n",
    "\n",
    "                    print_sys('test_' + name + '_' + m + ': ' + str(subgroup_analysis[name][m]))\n",
    "        print_sys('Done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DataLoading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Found local copy...\n",
      "Found local copy...\n",
      "These perturbations are not in the GO graph and their perturbation can thus not be predicted\n",
      "['RHOXF2BB+ctrl' 'LYL1+IER5L' 'ctrl+IER5L' 'KIAA1804+ctrl' 'IER5L+ctrl'\n",
      " 'RHOXF2BB+ZBTB25' 'RHOXF2BB+SET']\n",
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Creating new splits....\n",
      "Saving new splits at ./data/norman/splits/norman_single_1_0.75.pkl\n",
      "Done!\n",
      "Creating dataloaders....\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "pert_data = PertData('./data')\n",
    "pert_data.load(data_name = 'norman')\n",
    "pert_data.prepare_split(split = 'single', seed = 1,combo_single_split_test_set_fraction=0.3,val_size=0.3)\n",
    "pert_data.get_dataloader(batch_size = 32, test_batch_size = 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    }
   ],
   "source": [
    "gears_model_original = GEARS(pert_data, device = 'cuda', \n",
    "                        weight_bias_track = False, \n",
    "                        proj_name = 'pertnet', \n",
    "                        exp_name = 'pertnet')\n",
    "gears_model_original.model_initialize(hidden_size = 64,gears_model=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    }
   ],
   "source": [
    "gears_model_exprembedding = GEARS(pert_data, device = 'cuda', \n",
    "                        weight_bias_track = False, \n",
    "                        proj_name = 'pertnet', \n",
    "                        exp_name = 'pertnet')\n",
    "gears_model_exprembedding.model_initialize(hidden_size = 64,gears_model=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    }
   ],
   "source": [
    "gears_model_gat = GEARS(pert_data, device = 'cuda', \n",
    "                        weight_bias_track = False, \n",
    "                        proj_name = 'pertnet', \n",
    "                        exp_name = 'pertnet')\n",
    "gears_model_gat.model_initialize(hidden_size = 64,gears_model=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    }
   ],
   "source": [
    "gears_model_transformer = GEARS(pert_data, device = 'cuda', \n",
    "                        weight_bias_track = False, \n",
    "                        proj_name = 'pertnet', \n",
    "                        exp_name = 'pertnet')\n",
    "gears_model_transformer.model_initialize(hidden_size = 64,gears_model=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    }
   ],
   "source": [
    "gears_model_no_coexpress = GEARS(pert_data, device = 'cuda', \n",
    "                        weight_bias_track = False, \n",
    "                        proj_name = 'pertnet', \n",
    "                        exp_name = 'pertnet')\n",
    "gears_model_no_coexpress.model_initialize(hidden_size = 64,gears_model=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    }
   ],
   "source": [
    "gears_model_no_perturb = GEARS(pert_data, device = 'cuda', \n",
    "                        weight_bias_track = False, \n",
    "                        proj_name = 'pertnet', \n",
    "                        exp_name = 'pertnet')\n",
    "gears_model_no_perturb.model_initialize(hidden_size = 64,gears_model=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    }
   ],
   "source": [
    "gears_model_selfattn = GEARS(pert_data, device = 'cuda', \n",
    "                        weight_bias_track = False, \n",
    "                        proj_name = 'pertnet', \n",
    "                        exp_name = 'pertnet')\n",
    "gears_model_selfattn.model_initialize(hidden_size = 64,gears_model=6,num_heads=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 1 Train Loss: 0.4411\n",
      "Epoch 1 Step 51 Train Loss: 0.4418\n",
      "Epoch 1 Step 101 Train Loss: 0.4522\n",
      "Epoch 1 Step 151 Train Loss: 0.4335\n",
      "Epoch 1 Step 201 Train Loss: 0.4504\n",
      "Epoch 1 Step 251 Train Loss: 0.5684\n",
      "Epoch 1 Step 301 Train Loss: 0.5218\n",
      "Epoch 1 Step 351 Train Loss: 0.4561\n",
      "Epoch 1 Step 401 Train Loss: 0.5256\n",
      "Epoch 1 Step 451 Train Loss: 0.3913\n",
      "Epoch 1 Step 501 Train Loss: 0.5284\n",
      "Epoch 1 Step 551 Train Loss: 0.4517\n",
      "Epoch 1 Step 601 Train Loss: 0.4085\n",
      "Epoch 1 Step 651 Train Loss: 0.4714\n",
      "Epoch 1 Step 701 Train Loss: 0.4411\n",
      "Epoch 1 Step 751 Train Loss: 0.4634\n",
      "Epoch 1 Step 801 Train Loss: 0.4281\n",
      "Epoch 1 Step 851 Train Loss: 0.4448\n",
      "Epoch 1 Step 901 Train Loss: 0.4474\n",
      "Epoch 1 Step 951 Train Loss: 0.5067\n",
      "Epoch 1 Step 1001 Train Loss: 0.4006\n",
      "Epoch 1 Step 1051 Train Loss: 0.4608\n",
      "Epoch 1 Step 1101 Train Loss: 0.4402\n",
      "Epoch 1 Step 1151 Train Loss: 0.4424\n",
      "Epoch 1: Train Overall MSE: 0.0037 Validation Overall MSE: 0.0051. \n",
      "Train Top 20 DE MSE: 0.1350 Validation Top 20 DE MSE: 0.2877. \n",
      "Epoch 2 Step 1 Train Loss: 0.4131\n",
      "Epoch 2 Step 51 Train Loss: 0.4416\n",
      "Epoch 2 Step 101 Train Loss: 0.4909\n",
      "Epoch 2 Step 151 Train Loss: 0.4920\n",
      "Epoch 2 Step 201 Train Loss: 0.3776\n",
      "Epoch 2 Step 251 Train Loss: 0.4754\n",
      "Epoch 2 Step 301 Train Loss: 0.3954\n",
      "Epoch 2 Step 351 Train Loss: 0.4344\n",
      "Epoch 2 Step 401 Train Loss: 0.4599\n",
      "Epoch 2 Step 451 Train Loss: 0.4305\n",
      "Epoch 2 Step 501 Train Loss: 0.4008\n",
      "Epoch 2 Step 551 Train Loss: 0.5179\n",
      "Epoch 2 Step 601 Train Loss: 0.4619\n",
      "Epoch 2 Step 651 Train Loss: 0.4249\n",
      "Epoch 2 Step 701 Train Loss: 0.4111\n",
      "Epoch 2 Step 751 Train Loss: 0.4580\n",
      "Epoch 2 Step 801 Train Loss: 0.4513\n",
      "Epoch 2 Step 851 Train Loss: 0.4758\n",
      "Epoch 2 Step 901 Train Loss: 0.4312\n",
      "Epoch 2 Step 951 Train Loss: 0.4494\n",
      "Epoch 2 Step 1001 Train Loss: 0.4248\n",
      "Epoch 2 Step 1051 Train Loss: 0.4470\n",
      "Epoch 2 Step 1101 Train Loss: 0.4297\n",
      "Epoch 2 Step 1151 Train Loss: 0.4077\n",
      "Epoch 2: Train Overall MSE: 0.0032 Validation Overall MSE: 0.0055. \n",
      "Train Top 20 DE MSE: 0.1574 Validation Top 20 DE MSE: 0.3486. \n",
      "Epoch 3 Step 1 Train Loss: 0.4420\n",
      "Epoch 3 Step 51 Train Loss: 0.6443\n",
      "Epoch 3 Step 101 Train Loss: 0.5323\n",
      "Epoch 3 Step 151 Train Loss: 0.4650\n",
      "Epoch 3 Step 201 Train Loss: 0.4277\n",
      "Epoch 3 Step 251 Train Loss: 0.4105\n",
      "Epoch 3 Step 301 Train Loss: 0.4950\n",
      "Epoch 3 Step 351 Train Loss: 0.4324\n",
      "Epoch 3 Step 401 Train Loss: 0.5042\n",
      "Epoch 3 Step 451 Train Loss: 0.4642\n",
      "Epoch 3 Step 501 Train Loss: 0.4417\n",
      "Epoch 3 Step 551 Train Loss: 0.4818\n",
      "Epoch 3 Step 601 Train Loss: 0.4622\n",
      "Epoch 3 Step 651 Train Loss: 0.4434\n",
      "Epoch 3 Step 701 Train Loss: 0.4551\n",
      "Epoch 3 Step 751 Train Loss: 0.4470\n",
      "Epoch 3 Step 801 Train Loss: 0.4036\n",
      "Epoch 3 Step 851 Train Loss: 0.5664\n",
      "Epoch 3 Step 901 Train Loss: 0.3773\n",
      "Epoch 3 Step 951 Train Loss: 0.4401\n",
      "Epoch 3 Step 1001 Train Loss: 0.4917\n",
      "Epoch 3 Step 1051 Train Loss: 0.4324\n",
      "Epoch 3 Step 1101 Train Loss: 0.4273\n",
      "Epoch 3 Step 1151 Train Loss: 0.4390\n",
      "Epoch 3: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0043. \n",
      "Train Top 20 DE MSE: 0.0891 Validation Top 20 DE MSE: 0.2902. \n",
      "Epoch 4 Step 1 Train Loss: 0.4092\n",
      "Epoch 4 Step 51 Train Loss: 0.4131\n",
      "Epoch 4 Step 101 Train Loss: 0.4705\n",
      "Epoch 4 Step 151 Train Loss: 0.4809\n",
      "Epoch 4 Step 201 Train Loss: 0.3991\n",
      "Epoch 4 Step 251 Train Loss: 0.4370\n",
      "Epoch 4 Step 301 Train Loss: 0.5239\n",
      "Epoch 4 Step 351 Train Loss: 0.4832\n",
      "Epoch 4 Step 401 Train Loss: 0.4969\n",
      "Epoch 4 Step 451 Train Loss: 0.5316\n",
      "Epoch 4 Step 501 Train Loss: 0.4729\n",
      "Epoch 4 Step 551 Train Loss: 0.5180\n",
      "Epoch 4 Step 601 Train Loss: 0.5011\n",
      "Epoch 4 Step 651 Train Loss: 0.4184\n",
      "Epoch 4 Step 701 Train Loss: 0.4646\n",
      "Epoch 4 Step 751 Train Loss: 0.4413\n",
      "Epoch 4 Step 801 Train Loss: 0.4417\n",
      "Epoch 4 Step 851 Train Loss: 0.4400\n",
      "Epoch 4 Step 901 Train Loss: 0.5161\n",
      "Epoch 4 Step 951 Train Loss: 0.4419\n",
      "Epoch 4 Step 1001 Train Loss: 0.5059\n",
      "Epoch 4 Step 1051 Train Loss: 0.3997\n",
      "Epoch 4 Step 1101 Train Loss: 0.4107\n",
      "Epoch 4 Step 1151 Train Loss: 0.4074\n",
      "Epoch 4: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0041. \n",
      "Train Top 20 DE MSE: 0.0718 Validation Top 20 DE MSE: 0.2606. \n",
      "Done!\n",
      "Start Testing...\n",
      "Best performing model: Test Top 20 DE MSE: 0.2164\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "gears_model_original.train(epochs=4,lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Expression Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 1 Train Loss: 0.5108\n",
      "Epoch 1 Step 51 Train Loss: 0.3805\n",
      "Epoch 1 Step 101 Train Loss: 0.3655\n",
      "Epoch 1 Step 151 Train Loss: 0.2835\n",
      "Epoch 1 Step 201 Train Loss: 0.3007\n",
      "Epoch 1 Step 251 Train Loss: 0.3619\n",
      "Epoch 1 Step 301 Train Loss: 0.3355\n",
      "Epoch 1 Step 351 Train Loss: 0.3000\n",
      "Epoch 1 Step 401 Train Loss: 0.3493\n",
      "Epoch 1 Step 451 Train Loss: 0.3174\n",
      "Epoch 1 Step 501 Train Loss: 0.3228\n",
      "Epoch 1 Step 551 Train Loss: 0.2948\n",
      "Epoch 1 Step 601 Train Loss: 0.3454\n",
      "Epoch 1 Step 651 Train Loss: 0.3223\n",
      "Epoch 1 Step 701 Train Loss: 0.3367\n",
      "Epoch 1 Step 751 Train Loss: 0.3140\n",
      "Epoch 1 Step 801 Train Loss: 0.3196\n",
      "Epoch 1 Step 851 Train Loss: 0.2850\n",
      "Epoch 1 Step 901 Train Loss: 0.3061\n",
      "Epoch 1 Step 951 Train Loss: 0.3696\n",
      "Epoch 1 Step 1001 Train Loss: 0.3797\n",
      "Epoch 1 Step 1051 Train Loss: 0.3778\n",
      "Epoch 1 Step 1101 Train Loss: 0.3465\n",
      "Epoch 1 Step 1151 Train Loss: 0.3376\n",
      "Epoch 1: Train Overall MSE: 0.0059 Validation Overall MSE: 0.0072. \n",
      "Train Top 20 DE MSE: 0.1824 Validation Top 20 DE MSE: 0.2770. \n",
      "Epoch 2 Step 1 Train Loss: 0.3284\n",
      "Epoch 2 Step 51 Train Loss: 0.3019\n",
      "Epoch 2 Step 101 Train Loss: 0.3323\n",
      "Epoch 2 Step 151 Train Loss: 0.3454\n",
      "Epoch 2 Step 201 Train Loss: 0.3142\n",
      "Epoch 2 Step 251 Train Loss: 0.3375\n",
      "Epoch 2 Step 301 Train Loss: 0.3574\n",
      "Epoch 2 Step 351 Train Loss: 0.3097\n",
      "Epoch 2 Step 401 Train Loss: 0.3395\n",
      "Epoch 2 Step 451 Train Loss: 0.3616\n",
      "Epoch 2 Step 501 Train Loss: 0.3536\n",
      "Epoch 2 Step 551 Train Loss: 0.3983\n",
      "Epoch 2 Step 601 Train Loss: 0.3362\n",
      "Epoch 2 Step 651 Train Loss: 0.3245\n",
      "Epoch 2 Step 701 Train Loss: 0.3457\n",
      "Epoch 2 Step 751 Train Loss: 0.3748\n",
      "Epoch 2 Step 801 Train Loss: 0.3577\n",
      "Epoch 2 Step 851 Train Loss: 0.3581\n",
      "Epoch 2 Step 901 Train Loss: 0.3645\n",
      "Epoch 2 Step 951 Train Loss: 0.3422\n",
      "Epoch 2 Step 1001 Train Loss: 0.3584\n",
      "Epoch 2 Step 1051 Train Loss: 0.3502\n",
      "Epoch 2 Step 1101 Train Loss: 0.3356\n",
      "Epoch 2 Step 1151 Train Loss: 0.3281\n",
      "Epoch 2: Train Overall MSE: 0.0061 Validation Overall MSE: 0.0074. \n",
      "Train Top 20 DE MSE: 0.1102 Validation Top 20 DE MSE: 0.2513. \n",
      "Epoch 3 Step 1 Train Loss: 0.3357\n",
      "Epoch 3 Step 51 Train Loss: 0.3450\n",
      "Epoch 3 Step 101 Train Loss: 0.3302\n",
      "Epoch 3 Step 151 Train Loss: 0.4003\n",
      "Epoch 3 Step 201 Train Loss: 0.3334\n",
      "Epoch 3 Step 251 Train Loss: 0.3357\n",
      "Epoch 3 Step 301 Train Loss: 0.3688\n",
      "Epoch 3 Step 351 Train Loss: 0.3556\n",
      "Epoch 3 Step 401 Train Loss: 0.3926\n",
      "Epoch 3 Step 451 Train Loss: 0.3659\n",
      "Epoch 3 Step 501 Train Loss: 0.3316\n",
      "Epoch 3 Step 551 Train Loss: 0.3871\n",
      "Epoch 3 Step 601 Train Loss: 0.3279\n",
      "Epoch 3 Step 651 Train Loss: 0.3433\n",
      "Epoch 3 Step 701 Train Loss: 0.3379\n",
      "Epoch 3 Step 751 Train Loss: 0.3544\n",
      "Epoch 3 Step 801 Train Loss: 0.3431\n",
      "Epoch 3 Step 851 Train Loss: 0.3709\n",
      "Epoch 3 Step 901 Train Loss: 0.3526\n",
      "Epoch 3 Step 951 Train Loss: 0.3383\n",
      "Epoch 3 Step 1001 Train Loss: 0.3585\n",
      "Epoch 3 Step 1051 Train Loss: 0.3393\n",
      "Epoch 3 Step 1101 Train Loss: 0.3554\n",
      "Epoch 3 Step 1151 Train Loss: 0.3629\n",
      "Epoch 3: Train Overall MSE: 0.0063 Validation Overall MSE: 0.0073. \n",
      "Train Top 20 DE MSE: 0.1113 Validation Top 20 DE MSE: 0.2605. \n",
      "Epoch 4 Step 1 Train Loss: 0.3476\n",
      "Epoch 4 Step 51 Train Loss: 0.3442\n",
      "Epoch 4 Step 101 Train Loss: 0.3746\n",
      "Epoch 4 Step 151 Train Loss: 0.3703\n",
      "Epoch 4 Step 201 Train Loss: 0.4014\n",
      "Epoch 4 Step 251 Train Loss: 0.3319\n",
      "Epoch 4 Step 301 Train Loss: 0.3448\n",
      "Epoch 4 Step 351 Train Loss: 0.3703\n",
      "Epoch 4 Step 401 Train Loss: 0.3603\n",
      "Epoch 4 Step 451 Train Loss: 0.3690\n",
      "Epoch 4 Step 501 Train Loss: 0.3844\n",
      "Epoch 4 Step 551 Train Loss: 0.3781\n",
      "Epoch 4 Step 601 Train Loss: 0.3675\n",
      "Epoch 4 Step 651 Train Loss: 0.3871\n",
      "Epoch 4 Step 701 Train Loss: 0.3336\n",
      "Epoch 4 Step 751 Train Loss: 0.3748\n",
      "Epoch 4 Step 801 Train Loss: 0.4175\n",
      "Epoch 4 Step 851 Train Loss: 0.3639\n",
      "Epoch 4 Step 901 Train Loss: 0.3801\n",
      "Epoch 4 Step 951 Train Loss: 0.3546\n",
      "Epoch 4 Step 1001 Train Loss: 0.4054\n",
      "Epoch 4 Step 1051 Train Loss: 0.3583\n",
      "Epoch 4 Step 1101 Train Loss: 0.3635\n",
      "Epoch 4 Step 1151 Train Loss: 0.3714\n",
      "Epoch 4: Train Overall MSE: 0.0061 Validation Overall MSE: 0.0065. \n",
      "Train Top 20 DE MSE: 0.1003 Validation Top 20 DE MSE: 0.2650. \n",
      "Done!\n",
      "Start Testing...\n",
      "Best performing model: Test Top 20 DE MSE: 0.1958\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "gears_model_exprembedding.train(epochs=4,lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 1 Train Loss: 0.4610\n",
      "Epoch 1 Step 51 Train Loss: 0.4692\n",
      "Epoch 1 Step 101 Train Loss: 0.5143\n",
      "Epoch 1 Step 151 Train Loss: 0.4216\n",
      "Epoch 1 Step 201 Train Loss: 0.5278\n",
      "Epoch 1 Step 251 Train Loss: 0.5040\n",
      "Epoch 1 Step 301 Train Loss: 0.4528\n",
      "Epoch 1 Step 351 Train Loss: 0.4533\n",
      "Epoch 1 Step 401 Train Loss: 0.3846\n",
      "Epoch 1 Step 451 Train Loss: 0.4289\n",
      "Epoch 1 Step 501 Train Loss: 0.4295\n",
      "Epoch 1 Step 551 Train Loss: 0.3633\n",
      "Epoch 1 Step 601 Train Loss: 0.4315\n",
      "Epoch 1 Step 651 Train Loss: 0.4092\n",
      "Epoch 1 Step 701 Train Loss: 0.4397\n",
      "Epoch 1 Step 751 Train Loss: 0.4432\n",
      "Epoch 1 Step 801 Train Loss: 0.3824\n",
      "Epoch 1 Step 851 Train Loss: 0.4844\n",
      "Epoch 1 Step 901 Train Loss: 0.4462\n",
      "Epoch 1 Step 951 Train Loss: 0.4579\n",
      "Epoch 1 Step 1001 Train Loss: 0.4149\n",
      "Epoch 1 Step 1051 Train Loss: 0.4442\n",
      "Epoch 1 Step 1101 Train Loss: 0.4533\n",
      "Epoch 1 Step 1151 Train Loss: 0.4139\n",
      "Epoch 1: Train Overall MSE: 0.0124 Validation Overall MSE: 0.0125. \n",
      "Train Top 20 DE MSE: 0.2136 Validation Top 20 DE MSE: 0.3015. \n",
      "Epoch 2 Step 1 Train Loss: 0.4496\n",
      "Epoch 2 Step 51 Train Loss: 0.4248\n",
      "Epoch 2 Step 101 Train Loss: 0.4678\n",
      "Epoch 2 Step 151 Train Loss: 0.4675\n",
      "Epoch 2 Step 201 Train Loss: 0.4391\n",
      "Epoch 2 Step 251 Train Loss: 0.4294\n",
      "Epoch 2 Step 301 Train Loss: 0.4370\n",
      "Epoch 2 Step 351 Train Loss: 0.4901\n",
      "Epoch 2 Step 401 Train Loss: 0.5394\n",
      "Epoch 2 Step 451 Train Loss: 0.4379\n",
      "Epoch 2 Step 501 Train Loss: 0.4967\n",
      "Epoch 2 Step 551 Train Loss: 0.4594\n",
      "Epoch 2 Step 601 Train Loss: 0.4237\n",
      "Epoch 2 Step 651 Train Loss: 0.4772\n",
      "Epoch 2 Step 701 Train Loss: 0.4385\n",
      "Epoch 2 Step 751 Train Loss: 0.4812\n",
      "Epoch 2 Step 801 Train Loss: 0.4020\n",
      "Epoch 2 Step 851 Train Loss: 0.4700\n",
      "Epoch 2 Step 901 Train Loss: 0.4470\n",
      "Epoch 2 Step 951 Train Loss: 0.4984\n",
      "Epoch 2 Step 1001 Train Loss: 0.4243\n",
      "Epoch 2 Step 1051 Train Loss: 0.3428\n",
      "Epoch 2 Step 1101 Train Loss: 0.4755\n",
      "Epoch 2 Step 1151 Train Loss: 0.4476\n",
      "Epoch 2: Train Overall MSE: 0.0038 Validation Overall MSE: 0.0050. \n",
      "Train Top 20 DE MSE: 0.2001 Validation Top 20 DE MSE: 0.3118. \n",
      "Epoch 3 Step 1 Train Loss: 0.4353\n",
      "Epoch 3 Step 51 Train Loss: 0.4194\n",
      "Epoch 3 Step 101 Train Loss: 0.4498\n",
      "Epoch 3 Step 151 Train Loss: 0.5514\n",
      "Epoch 3 Step 201 Train Loss: 0.4074\n",
      "Epoch 3 Step 251 Train Loss: 0.4626\n",
      "Epoch 3 Step 301 Train Loss: 0.4321\n",
      "Epoch 3 Step 351 Train Loss: 0.4505\n",
      "Epoch 3 Step 401 Train Loss: 0.4225\n",
      "Epoch 3 Step 451 Train Loss: 0.4669\n",
      "Epoch 3 Step 501 Train Loss: 0.5676\n",
      "Epoch 3 Step 551 Train Loss: 0.4430\n",
      "Epoch 3 Step 601 Train Loss: 0.4906\n",
      "Epoch 3 Step 651 Train Loss: 0.4655\n",
      "Epoch 3 Step 701 Train Loss: 0.4276\n",
      "Epoch 3 Step 751 Train Loss: 0.4091\n",
      "Epoch 3 Step 801 Train Loss: 0.4015\n",
      "Epoch 3 Step 851 Train Loss: 0.4386\n",
      "Epoch 3 Step 901 Train Loss: 0.4173\n",
      "Epoch 3 Step 951 Train Loss: 0.4549\n",
      "Epoch 3 Step 1001 Train Loss: 0.4473\n",
      "Epoch 3 Step 1051 Train Loss: 0.4771\n",
      "Epoch 3 Step 1101 Train Loss: 0.4053\n",
      "Epoch 3 Step 1151 Train Loss: 0.4756\n",
      "Epoch 3: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0044. \n",
      "Train Top 20 DE MSE: 0.1021 Validation Top 20 DE MSE: 0.2935. \n",
      "Epoch 4 Step 1 Train Loss: 0.5730\n",
      "Epoch 4 Step 51 Train Loss: 0.4469\n",
      "Epoch 4 Step 101 Train Loss: 0.4641\n",
      "Epoch 4 Step 151 Train Loss: 0.4620\n",
      "Epoch 4 Step 201 Train Loss: 0.4636\n",
      "Epoch 4 Step 251 Train Loss: 0.4289\n",
      "Epoch 4 Step 301 Train Loss: 0.5303\n",
      "Epoch 4 Step 351 Train Loss: 0.4139\n",
      "Epoch 4 Step 401 Train Loss: 0.4317\n",
      "Epoch 4 Step 451 Train Loss: 0.4632\n",
      "Epoch 4 Step 501 Train Loss: 0.5298\n",
      "Epoch 4 Step 551 Train Loss: 0.4105\n",
      "Epoch 4 Step 601 Train Loss: 0.4804\n",
      "Epoch 4 Step 651 Train Loss: 0.4779\n",
      "Epoch 4 Step 701 Train Loss: 0.5109\n",
      "Epoch 4 Step 751 Train Loss: 0.4227\n",
      "Epoch 4 Step 801 Train Loss: 0.4291\n",
      "Epoch 4 Step 851 Train Loss: 0.4555\n",
      "Epoch 4 Step 901 Train Loss: 0.4426\n",
      "Epoch 4 Step 951 Train Loss: 0.4238\n",
      "Epoch 4 Step 1001 Train Loss: 0.4588\n",
      "Epoch 4 Step 1051 Train Loss: 0.4404\n",
      "Epoch 4 Step 1101 Train Loss: 0.4685\n",
      "Epoch 4 Step 1151 Train Loss: 0.3958\n",
      "Epoch 4: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0044. \n",
      "Train Top 20 DE MSE: 0.0789 Validation Top 20 DE MSE: 0.2859. \n",
      "Done!\n",
      "Start Testing...\n",
      "Best performing model: Test Top 20 DE MSE: 0.2225\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "gears_model_gat.train(epochs=4,lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 1 Train Loss: 0.4627\n",
      "Epoch 1 Step 51 Train Loss: 0.5066\n",
      "Epoch 1 Step 101 Train Loss: 0.5343\n",
      "Epoch 1 Step 151 Train Loss: 0.4130\n",
      "Epoch 1 Step 201 Train Loss: 0.4797\n",
      "Epoch 1 Step 251 Train Loss: 0.4367\n",
      "Epoch 1 Step 301 Train Loss: 0.4367\n",
      "Epoch 1 Step 351 Train Loss: 0.3971\n",
      "Epoch 1 Step 401 Train Loss: 0.4761\n",
      "Epoch 1 Step 451 Train Loss: 0.4701\n",
      "Epoch 1 Step 501 Train Loss: 0.4243\n",
      "Epoch 1 Step 551 Train Loss: 0.3872\n",
      "Epoch 1 Step 601 Train Loss: 0.4274\n",
      "Epoch 1 Step 651 Train Loss: 0.4545\n",
      "Epoch 1 Step 701 Train Loss: 0.5219\n",
      "Epoch 1 Step 751 Train Loss: 0.4333\n",
      "Epoch 1 Step 801 Train Loss: 0.4157\n",
      "Epoch 1 Step 851 Train Loss: 0.4597\n",
      "Epoch 1 Step 901 Train Loss: 0.4447\n",
      "Epoch 1 Step 951 Train Loss: 0.4322\n",
      "Epoch 1 Step 1001 Train Loss: 0.4503\n",
      "Epoch 1 Step 1051 Train Loss: 0.4134\n",
      "Epoch 1 Step 1101 Train Loss: 0.4539\n",
      "Epoch 1 Step 1151 Train Loss: 0.4331\n",
      "Epoch 1: Train Overall MSE: 0.0050 Validation Overall MSE: 0.0061. \n",
      "Train Top 20 DE MSE: 0.1219 Validation Top 20 DE MSE: 0.2725. \n",
      "Epoch 2 Step 1 Train Loss: 0.4362\n",
      "Epoch 2 Step 51 Train Loss: 0.4621\n",
      "Epoch 2 Step 101 Train Loss: 0.4456\n",
      "Epoch 2 Step 151 Train Loss: 0.4388\n",
      "Epoch 2 Step 201 Train Loss: 0.4900\n",
      "Epoch 2 Step 251 Train Loss: 0.3980\n",
      "Epoch 2 Step 301 Train Loss: 0.4172\n",
      "Epoch 2 Step 351 Train Loss: 0.5054\n",
      "Epoch 2 Step 401 Train Loss: 0.4510\n",
      "Epoch 2 Step 451 Train Loss: 0.4530\n",
      "Epoch 2 Step 501 Train Loss: 0.4246\n",
      "Epoch 2 Step 551 Train Loss: 0.5204\n",
      "Epoch 2 Step 601 Train Loss: 0.4453\n",
      "Epoch 2 Step 651 Train Loss: 0.4757\n",
      "Epoch 2 Step 701 Train Loss: 0.4439\n",
      "Epoch 2 Step 751 Train Loss: 0.4282\n",
      "Epoch 2 Step 801 Train Loss: 0.3913\n",
      "Epoch 2 Step 851 Train Loss: 0.4919\n",
      "Epoch 2 Step 901 Train Loss: 0.5187\n",
      "Epoch 2 Step 951 Train Loss: 0.4428\n",
      "Epoch 2 Step 1001 Train Loss: 0.4944\n",
      "Epoch 2 Step 1051 Train Loss: 0.4192\n",
      "Epoch 2 Step 1101 Train Loss: 0.4743\n",
      "Epoch 2 Step 1151 Train Loss: 0.4378\n",
      "Epoch 2: Train Overall MSE: 0.0050 Validation Overall MSE: 0.0065. \n",
      "Train Top 20 DE MSE: 0.1066 Validation Top 20 DE MSE: 0.2779. \n",
      "Epoch 3 Step 1 Train Loss: 0.4585\n",
      "Epoch 3 Step 51 Train Loss: 0.4276\n",
      "Epoch 3 Step 101 Train Loss: 0.4461\n",
      "Epoch 3 Step 151 Train Loss: 0.3924\n",
      "Epoch 3 Step 201 Train Loss: 0.4735\n",
      "Epoch 3 Step 251 Train Loss: 0.5082\n",
      "Epoch 3 Step 301 Train Loss: 0.4232\n",
      "Epoch 3 Step 351 Train Loss: 0.4860\n",
      "Epoch 3 Step 401 Train Loss: 0.5307\n",
      "Epoch 3 Step 451 Train Loss: 0.4159\n",
      "Epoch 3 Step 501 Train Loss: 0.4302\n",
      "Epoch 3 Step 551 Train Loss: 0.5504\n",
      "Epoch 3 Step 601 Train Loss: 0.4209\n",
      "Epoch 3 Step 651 Train Loss: 0.4206\n",
      "Epoch 3 Step 701 Train Loss: 0.4553\n",
      "Epoch 3 Step 751 Train Loss: 0.4553\n",
      "Epoch 3 Step 801 Train Loss: 0.4722\n",
      "Epoch 3 Step 851 Train Loss: 0.4751\n",
      "Epoch 3 Step 901 Train Loss: 0.4357\n",
      "Epoch 3 Step 951 Train Loss: 0.4202\n",
      "Epoch 3 Step 1001 Train Loss: 0.4045\n",
      "Epoch 3 Step 1051 Train Loss: 0.4494\n",
      "Epoch 3 Step 1101 Train Loss: 0.4655\n",
      "Epoch 3 Step 1151 Train Loss: 0.5265\n",
      "Epoch 3: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0046. \n",
      "Train Top 20 DE MSE: 0.0722 Validation Top 20 DE MSE: 0.2948. \n",
      "Epoch 4 Step 1 Train Loss: 0.4418\n",
      "Epoch 4 Step 51 Train Loss: 0.4782\n",
      "Epoch 4 Step 101 Train Loss: 0.6603\n",
      "Epoch 4 Step 151 Train Loss: 0.3955\n",
      "Epoch 4 Step 201 Train Loss: 0.4711\n",
      "Epoch 4 Step 251 Train Loss: 0.5533\n",
      "Epoch 4 Step 301 Train Loss: 0.4595\n",
      "Epoch 4 Step 351 Train Loss: 0.3956\n",
      "Epoch 4 Step 401 Train Loss: 0.4082\n",
      "Epoch 4 Step 451 Train Loss: 0.4165\n",
      "Epoch 4 Step 501 Train Loss: 0.4374\n",
      "Epoch 4 Step 551 Train Loss: 0.4745\n",
      "Epoch 4 Step 601 Train Loss: 0.4531\n",
      "Epoch 4 Step 651 Train Loss: 0.4410\n",
      "Epoch 4 Step 701 Train Loss: 0.4433\n",
      "Epoch 4 Step 751 Train Loss: 0.4419\n",
      "Epoch 4 Step 801 Train Loss: 0.5038\n",
      "Epoch 4 Step 851 Train Loss: 0.4531\n",
      "Epoch 4 Step 901 Train Loss: 0.4686\n",
      "Epoch 4 Step 951 Train Loss: 0.4091\n",
      "Epoch 4 Step 1001 Train Loss: 0.4895\n",
      "Epoch 4 Step 1051 Train Loss: 0.4814\n",
      "Epoch 4 Step 1101 Train Loss: 0.4862\n",
      "Epoch 4 Step 1151 Train Loss: 0.4506\n",
      "Epoch 4: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0044. \n",
      "Train Top 20 DE MSE: 0.0659 Validation Top 20 DE MSE: 0.2969. \n",
      "Done!\n",
      "Start Testing...\n",
      "Best performing model: Test Top 20 DE MSE: 0.2116\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "gears_model_transformer.train(epochs=4,lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. No Coexpression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "Epoch 1 Step 1 Train Loss: 0.4776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 51 Train Loss: 0.4662\n",
      "Epoch 1 Step 101 Train Loss: 0.4490\n",
      "Epoch 1 Step 151 Train Loss: 0.4724\n",
      "Epoch 1 Step 201 Train Loss: 0.5042\n",
      "Epoch 1 Step 251 Train Loss: 0.4535\n",
      "Epoch 1 Step 301 Train Loss: 0.4327\n",
      "Epoch 1 Step 351 Train Loss: 0.4124\n",
      "Epoch 1 Step 401 Train Loss: 0.4738\n",
      "Epoch 1 Step 451 Train Loss: 0.4339\n",
      "Epoch 1 Step 501 Train Loss: 0.4857\n",
      "Epoch 1 Step 551 Train Loss: 0.4317\n",
      "Epoch 1 Step 601 Train Loss: 0.4184\n",
      "Epoch 1 Step 651 Train Loss: 0.4794\n",
      "Epoch 1 Step 701 Train Loss: 0.4502\n",
      "Epoch 1 Step 751 Train Loss: 0.5521\n",
      "Epoch 1 Step 801 Train Loss: 0.3946\n",
      "Epoch 1 Step 851 Train Loss: 0.4744\n",
      "Epoch 1 Step 901 Train Loss: 0.4498\n",
      "Epoch 1 Step 951 Train Loss: 0.4808\n",
      "Epoch 1 Step 1001 Train Loss: 0.4243\n",
      "Epoch 1 Step 1051 Train Loss: 0.3817\n",
      "Epoch 1 Step 1101 Train Loss: 0.3950\n",
      "Epoch 1 Step 1151 Train Loss: 0.4014\n",
      "Epoch 1: Train Overall MSE: 0.0064 Validation Overall MSE: 0.0065. \n",
      "Train Top 20 DE MSE: 0.1874 Validation Top 20 DE MSE: 0.2695. \n",
      "Epoch 2 Step 1 Train Loss: 0.4369\n",
      "Epoch 2 Step 51 Train Loss: 0.5250\n",
      "Epoch 2 Step 101 Train Loss: 0.4778\n",
      "Epoch 2 Step 151 Train Loss: 0.5477\n",
      "Epoch 2 Step 201 Train Loss: 0.4391\n",
      "Epoch 2 Step 251 Train Loss: 0.4502\n",
      "Epoch 2 Step 301 Train Loss: 0.5271\n",
      "Epoch 2 Step 351 Train Loss: 0.4815\n",
      "Epoch 2 Step 401 Train Loss: 0.4615\n",
      "Epoch 2 Step 451 Train Loss: 0.5543\n",
      "Epoch 2 Step 501 Train Loss: 0.4931\n",
      "Epoch 2 Step 551 Train Loss: 0.4649\n",
      "Epoch 2 Step 601 Train Loss: 0.4751\n",
      "Epoch 2 Step 651 Train Loss: 0.4032\n",
      "Epoch 2 Step 701 Train Loss: 0.4103\n",
      "Epoch 2 Step 751 Train Loss: 0.4702\n",
      "Epoch 2 Step 801 Train Loss: 0.4626\n",
      "Epoch 2 Step 851 Train Loss: 0.3907\n",
      "Epoch 2 Step 901 Train Loss: 0.4199\n",
      "Epoch 2 Step 951 Train Loss: 0.5304\n",
      "Epoch 2 Step 1001 Train Loss: 0.4249\n",
      "Epoch 2 Step 1051 Train Loss: 0.4215\n",
      "Epoch 2 Step 1101 Train Loss: 0.4309\n",
      "Epoch 2 Step 1151 Train Loss: 0.6055\n",
      "Epoch 2: Train Overall MSE: 0.0027 Validation Overall MSE: 0.0051. \n",
      "Train Top 20 DE MSE: 0.1212 Validation Top 20 DE MSE: 0.3003. \n",
      "Epoch 3 Step 1 Train Loss: 0.4386\n",
      "Epoch 3 Step 51 Train Loss: 0.4169\n",
      "Epoch 3 Step 101 Train Loss: 0.4239\n",
      "Epoch 3 Step 151 Train Loss: 0.4555\n",
      "Epoch 3 Step 201 Train Loss: 0.4403\n",
      "Epoch 3 Step 251 Train Loss: 0.4356\n",
      "Epoch 3 Step 301 Train Loss: 0.4331\n",
      "Epoch 3 Step 351 Train Loss: 0.5378\n",
      "Epoch 3 Step 401 Train Loss: 0.4576\n",
      "Epoch 3 Step 451 Train Loss: 0.4123\n",
      "Epoch 3 Step 501 Train Loss: 0.4409\n",
      "Epoch 3 Step 551 Train Loss: 0.4561\n",
      "Epoch 3 Step 601 Train Loss: 0.3970\n",
      "Epoch 3 Step 651 Train Loss: 0.4659\n",
      "Epoch 3 Step 701 Train Loss: 0.3822\n",
      "Epoch 3 Step 751 Train Loss: 0.3872\n",
      "Epoch 3 Step 801 Train Loss: 0.4964\n",
      "Epoch 3 Step 851 Train Loss: 0.4991\n",
      "Epoch 3 Step 901 Train Loss: 0.4073\n",
      "Epoch 3 Step 951 Train Loss: 0.4043\n",
      "Epoch 3 Step 1001 Train Loss: 0.4632\n",
      "Epoch 3 Step 1051 Train Loss: 0.4256\n",
      "Epoch 3 Step 1101 Train Loss: 0.4322\n",
      "Epoch 3 Step 1151 Train Loss: 0.4445\n",
      "Epoch 3: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0042. \n",
      "Train Top 20 DE MSE: 0.1161 Validation Top 20 DE MSE: 0.2885. \n",
      "Epoch 4 Step 1 Train Loss: 0.4118\n",
      "Epoch 4 Step 51 Train Loss: 0.4421\n",
      "Epoch 4 Step 101 Train Loss: 0.4610\n",
      "Epoch 4 Step 151 Train Loss: 0.4839\n",
      "Epoch 4 Step 201 Train Loss: 0.4406\n",
      "Epoch 4 Step 251 Train Loss: 0.4553\n",
      "Epoch 4 Step 301 Train Loss: 0.4250\n",
      "Epoch 4 Step 351 Train Loss: 0.3862\n",
      "Epoch 4 Step 401 Train Loss: 0.4229\n",
      "Epoch 4 Step 451 Train Loss: 0.4736\n",
      "Epoch 4 Step 501 Train Loss: 0.4419\n",
      "Epoch 4 Step 551 Train Loss: 0.4332\n",
      "Epoch 4 Step 601 Train Loss: 0.5089\n",
      "Epoch 4 Step 651 Train Loss: 0.4919\n",
      "Epoch 4 Step 701 Train Loss: 0.4674\n",
      "Epoch 4 Step 751 Train Loss: 0.4992\n",
      "Epoch 4 Step 801 Train Loss: 0.5025\n",
      "Epoch 4 Step 851 Train Loss: 0.4009\n",
      "Epoch 4 Step 901 Train Loss: 0.4256\n",
      "Epoch 4 Step 951 Train Loss: 0.4312\n",
      "Epoch 4 Step 1001 Train Loss: 0.5214\n",
      "Epoch 4 Step 1051 Train Loss: 0.5097\n",
      "Epoch 4 Step 1101 Train Loss: 0.4460\n",
      "Epoch 4 Step 1151 Train Loss: 0.4704\n",
      "Epoch 4: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0043. \n",
      "Train Top 20 DE MSE: 0.0801 Validation Top 20 DE MSE: 0.2731. \n",
      "Done!\n",
      "Start Testing...\n",
      "Best performing model: Test Top 20 DE MSE: 0.2339\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "gears_model_no_coexpress.train(epochs=4,lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. No Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 1 Train Loss: 0.4856\n",
      "Epoch 1 Step 51 Train Loss: 0.4436\n",
      "Epoch 1 Step 101 Train Loss: 0.4474\n",
      "Epoch 1 Step 151 Train Loss: 0.4374\n",
      "Epoch 1 Step 201 Train Loss: 0.4466\n",
      "Epoch 1 Step 251 Train Loss: 0.4339\n",
      "Epoch 1 Step 301 Train Loss: 0.4007\n",
      "Epoch 1 Step 351 Train Loss: 0.4587\n",
      "Epoch 1 Step 401 Train Loss: 0.4201\n",
      "Epoch 1 Step 451 Train Loss: 0.4091\n",
      "Epoch 1 Step 501 Train Loss: 0.4770\n",
      "Epoch 1 Step 551 Train Loss: 0.3892\n",
      "Epoch 1 Step 601 Train Loss: 0.3814\n",
      "Epoch 1 Step 651 Train Loss: 0.3765\n",
      "Epoch 1 Step 701 Train Loss: 0.3833\n",
      "Epoch 1 Step 751 Train Loss: 0.4161\n",
      "Epoch 1 Step 801 Train Loss: 0.4525\n",
      "Epoch 1 Step 851 Train Loss: 0.4557\n",
      "Epoch 1 Step 901 Train Loss: 0.4196\n",
      "Epoch 1 Step 951 Train Loss: 0.4109\n",
      "Epoch 1 Step 1001 Train Loss: 0.4758\n",
      "Epoch 1 Step 1051 Train Loss: 0.3981\n",
      "Epoch 1 Step 1101 Train Loss: 0.4389\n",
      "Epoch 1 Step 1151 Train Loss: 0.4217\n",
      "Epoch 1: Train Overall MSE: 0.0030 Validation Overall MSE: 0.0049. \n",
      "Train Top 20 DE MSE: 0.0992 Validation Top 20 DE MSE: 0.2926. \n",
      "Epoch 2 Step 1 Train Loss: 0.3885\n",
      "Epoch 2 Step 51 Train Loss: 0.4176\n",
      "Epoch 2 Step 101 Train Loss: 0.3969\n",
      "Epoch 2 Step 151 Train Loss: 0.4321\n",
      "Epoch 2 Step 201 Train Loss: 0.4550\n",
      "Epoch 2 Step 251 Train Loss: 0.4315\n",
      "Epoch 2 Step 301 Train Loss: 0.3804\n",
      "Epoch 2 Step 351 Train Loss: 0.4691\n",
      "Epoch 2 Step 401 Train Loss: 0.4745\n",
      "Epoch 2 Step 451 Train Loss: 0.4161\n",
      "Epoch 2 Step 501 Train Loss: 0.4306\n",
      "Epoch 2 Step 551 Train Loss: 0.4632\n",
      "Epoch 2 Step 601 Train Loss: 0.4776\n",
      "Epoch 2 Step 651 Train Loss: 0.3906\n",
      "Epoch 2 Step 701 Train Loss: 0.4013\n",
      "Epoch 2 Step 751 Train Loss: 0.4569\n",
      "Epoch 2 Step 801 Train Loss: 0.4781\n",
      "Epoch 2 Step 851 Train Loss: 0.4742\n",
      "Epoch 2 Step 901 Train Loss: 0.4341\n",
      "Epoch 2 Step 951 Train Loss: 0.4258\n",
      "Epoch 2 Step 1001 Train Loss: 0.4700\n",
      "Epoch 2 Step 1051 Train Loss: 0.4554\n",
      "Epoch 2 Step 1101 Train Loss: 0.4571\n",
      "Epoch 2 Step 1151 Train Loss: 0.4313\n",
      "Epoch 2: Train Overall MSE: 0.0037 Validation Overall MSE: 0.0054. \n",
      "Train Top 20 DE MSE: 0.1191 Validation Top 20 DE MSE: 0.3399. \n",
      "Epoch 3 Step 1 Train Loss: 0.5154\n",
      "Epoch 3 Step 51 Train Loss: 0.4965\n",
      "Epoch 3 Step 101 Train Loss: 0.4063\n",
      "Epoch 3 Step 151 Train Loss: 0.4731\n",
      "Epoch 3 Step 201 Train Loss: 0.4331\n",
      "Epoch 3 Step 251 Train Loss: 0.4286\n",
      "Epoch 3 Step 301 Train Loss: 0.4122\n",
      "Epoch 3 Step 351 Train Loss: 0.3885\n",
      "Epoch 3 Step 401 Train Loss: 0.4250\n",
      "Epoch 3 Step 451 Train Loss: 0.5468\n",
      "Epoch 3 Step 501 Train Loss: 0.4754\n",
      "Epoch 3 Step 551 Train Loss: 0.4251\n",
      "Epoch 3 Step 601 Train Loss: 0.4377\n",
      "Epoch 3 Step 651 Train Loss: 0.4381\n",
      "Epoch 3 Step 701 Train Loss: 0.4678\n",
      "Epoch 3 Step 751 Train Loss: 0.4997\n",
      "Epoch 3 Step 801 Train Loss: 0.4175\n",
      "Epoch 3 Step 851 Train Loss: 0.5608\n",
      "Epoch 3 Step 901 Train Loss: 0.4453\n",
      "Epoch 3 Step 951 Train Loss: 0.5481\n",
      "Epoch 3 Step 1001 Train Loss: 0.4019\n",
      "Epoch 3 Step 1051 Train Loss: 0.4293\n",
      "Epoch 3 Step 1101 Train Loss: 0.6114\n",
      "Epoch 3 Step 1151 Train Loss: 0.4822\n",
      "Epoch 3: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0048. \n",
      "Train Top 20 DE MSE: 0.0749 Validation Top 20 DE MSE: 0.3346. \n",
      "Epoch 4 Step 1 Train Loss: 0.4232\n",
      "Epoch 4 Step 51 Train Loss: 0.4562\n",
      "Epoch 4 Step 101 Train Loss: 0.5080\n",
      "Epoch 4 Step 151 Train Loss: 0.3845\n",
      "Epoch 4 Step 201 Train Loss: 0.4835\n",
      "Epoch 4 Step 251 Train Loss: 0.4536\n",
      "Epoch 4 Step 301 Train Loss: 0.4117\n",
      "Epoch 4 Step 351 Train Loss: 0.5040\n",
      "Epoch 4 Step 401 Train Loss: 0.4933\n",
      "Epoch 4 Step 451 Train Loss: 0.5354\n",
      "Epoch 4 Step 501 Train Loss: 0.4399\n",
      "Epoch 4 Step 551 Train Loss: 0.4674\n",
      "Epoch 4 Step 601 Train Loss: 0.5159\n",
      "Epoch 4 Step 651 Train Loss: 0.5415\n",
      "Epoch 4 Step 701 Train Loss: 0.4296\n",
      "Epoch 4 Step 751 Train Loss: 0.4654\n",
      "Epoch 4 Step 801 Train Loss: 0.4727\n",
      "Epoch 4 Step 851 Train Loss: 0.3871\n",
      "Epoch 4 Step 901 Train Loss: 0.5764\n",
      "Epoch 4 Step 951 Train Loss: 0.3826\n",
      "Epoch 4 Step 1001 Train Loss: 0.4555\n",
      "Epoch 4 Step 1051 Train Loss: 0.5003\n",
      "Epoch 4 Step 1101 Train Loss: 0.4523\n",
      "Epoch 4 Step 1151 Train Loss: 0.3874\n",
      "Epoch 4: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0048. \n",
      "Train Top 20 DE MSE: 0.0764 Validation Top 20 DE MSE: 0.3438. \n",
      "Done!\n",
      "Start Testing...\n",
      "Best performing model: Test Top 20 DE MSE: 0.2377\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "gears_model_no_perturb.train(epochs=4,lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 1 Train Loss: 0.4122\n",
      "Epoch 1 Step 51 Train Loss: 0.4788\n",
      "Epoch 1 Step 101 Train Loss: 0.4838\n",
      "Epoch 1 Step 151 Train Loss: 0.4221\n",
      "Epoch 1 Step 201 Train Loss: 0.4664\n",
      "Epoch 1 Step 251 Train Loss: 0.4285\n",
      "Epoch 1 Step 301 Train Loss: 0.4294\n",
      "Epoch 1 Step 351 Train Loss: 0.4496\n",
      "Epoch 1 Step 401 Train Loss: 0.4592\n",
      "Epoch 1 Step 451 Train Loss: 0.4063\n",
      "Epoch 1 Step 501 Train Loss: 0.5250\n",
      "Epoch 1 Step 551 Train Loss: 0.4552\n",
      "Epoch 1 Step 601 Train Loss: 0.4108\n",
      "Epoch 1 Step 651 Train Loss: 0.4561\n",
      "Epoch 1 Step 701 Train Loss: 0.5632\n",
      "Epoch 1 Step 751 Train Loss: 0.4393\n",
      "Epoch 1 Step 801 Train Loss: 0.4792\n",
      "Epoch 1 Step 851 Train Loss: 0.3856\n",
      "Epoch 1 Step 901 Train Loss: 0.4604\n",
      "Epoch 1 Step 951 Train Loss: 0.4472\n",
      "Epoch 1 Step 1001 Train Loss: 0.4842\n",
      "Epoch 1 Step 1051 Train Loss: 0.4794\n",
      "Epoch 1 Step 1101 Train Loss: 0.5084\n",
      "Epoch 1 Step 1151 Train Loss: 0.4088\n",
      "Epoch 1: Train Overall MSE: 0.0042 Validation Overall MSE: 0.0046. \n",
      "Train Top 20 DE MSE: 0.1678 Validation Top 20 DE MSE: 0.2952. \n",
      "Epoch 2 Step 1 Train Loss: 0.4394\n",
      "Epoch 2 Step 51 Train Loss: 0.5259\n",
      "Epoch 2 Step 101 Train Loss: 0.5266\n",
      "Epoch 2 Step 151 Train Loss: 0.4329\n",
      "Epoch 2 Step 201 Train Loss: 0.4530\n",
      "Epoch 2 Step 251 Train Loss: 0.4805\n",
      "Epoch 2 Step 301 Train Loss: 0.4382\n",
      "Epoch 2 Step 351 Train Loss: 0.4775\n",
      "Epoch 2 Step 401 Train Loss: 0.4446\n",
      "Epoch 2 Step 451 Train Loss: 0.4854\n",
      "Epoch 2 Step 501 Train Loss: 0.4445\n",
      "Epoch 2 Step 551 Train Loss: 0.3937\n",
      "Epoch 2 Step 601 Train Loss: 0.4417\n",
      "Epoch 2 Step 651 Train Loss: 0.4195\n",
      "Epoch 2 Step 701 Train Loss: 0.4617\n",
      "Epoch 2 Step 751 Train Loss: 0.5607\n",
      "Epoch 2 Step 801 Train Loss: 0.4795\n",
      "Epoch 2 Step 851 Train Loss: 0.4729\n",
      "Epoch 2 Step 901 Train Loss: 0.4877\n",
      "Epoch 2 Step 951 Train Loss: 0.5279\n",
      "Epoch 2 Step 1001 Train Loss: 0.4553\n",
      "Epoch 2 Step 1051 Train Loss: 0.6147\n",
      "Epoch 2 Step 1101 Train Loss: 0.5087\n",
      "Epoch 2 Step 1151 Train Loss: 0.5081\n",
      "Epoch 2: Train Overall MSE: 0.0091 Validation Overall MSE: 0.0239. \n",
      "Train Top 20 DE MSE: 0.2392 Validation Top 20 DE MSE: 0.3391. \n",
      "Epoch 3 Step 1 Train Loss: 0.4406\n",
      "Epoch 3 Step 51 Train Loss: 0.4923\n",
      "Epoch 3 Step 101 Train Loss: 0.5296\n",
      "Epoch 3 Step 151 Train Loss: 0.5627\n",
      "Epoch 3 Step 201 Train Loss: 0.4266\n",
      "Epoch 3 Step 251 Train Loss: 0.3895\n",
      "Epoch 3 Step 301 Train Loss: 0.4693\n",
      "Epoch 3 Step 351 Train Loss: 0.4753\n",
      "Epoch 3 Step 401 Train Loss: 0.4565\n",
      "Epoch 3 Step 451 Train Loss: 0.5177\n",
      "Epoch 3 Step 501 Train Loss: 0.4318\n",
      "Epoch 3 Step 551 Train Loss: 0.4947\n",
      "Epoch 3 Step 601 Train Loss: 0.4993\n",
      "Epoch 3 Step 651 Train Loss: 0.4592\n",
      "Epoch 3 Step 701 Train Loss: 0.4740\n",
      "Epoch 3 Step 751 Train Loss: 0.4309\n",
      "Epoch 3 Step 801 Train Loss: 0.4603\n",
      "Epoch 3 Step 851 Train Loss: 0.4573\n",
      "Epoch 3 Step 901 Train Loss: 0.4662\n",
      "Epoch 3 Step 951 Train Loss: 0.5849\n",
      "Epoch 3 Step 1001 Train Loss: 0.4535\n",
      "Epoch 3 Step 1051 Train Loss: 0.5296\n",
      "Epoch 3 Step 1101 Train Loss: 0.4834\n",
      "Epoch 3 Step 1151 Train Loss: 0.5506\n",
      "Epoch 3: Train Overall MSE: 0.0032 Validation Overall MSE: 0.0049. \n",
      "Train Top 20 DE MSE: 0.1358 Validation Top 20 DE MSE: 0.3091. \n",
      "Epoch 4 Step 1 Train Loss: 0.4397\n",
      "Epoch 4 Step 51 Train Loss: 0.4455\n",
      "Epoch 4 Step 101 Train Loss: 0.4068\n",
      "Epoch 4 Step 151 Train Loss: 0.5044\n",
      "Epoch 4 Step 201 Train Loss: 0.4645\n",
      "Epoch 4 Step 251 Train Loss: 0.5155\n",
      "Epoch 4 Step 301 Train Loss: 0.5411\n",
      "Epoch 4 Step 351 Train Loss: 0.4936\n",
      "Epoch 4 Step 401 Train Loss: 0.5212\n",
      "Epoch 4 Step 451 Train Loss: 0.5083\n",
      "Epoch 4 Step 501 Train Loss: 0.4560\n",
      "Epoch 4 Step 551 Train Loss: 0.5067\n",
      "Epoch 4 Step 601 Train Loss: 0.5097\n",
      "Epoch 4 Step 651 Train Loss: 0.4502\n",
      "Epoch 4 Step 701 Train Loss: 0.5680\n",
      "Epoch 4 Step 751 Train Loss: 0.4799\n",
      "Epoch 4 Step 801 Train Loss: 0.4672\n",
      "Epoch 4 Step 851 Train Loss: 0.5086\n",
      "Epoch 4 Step 901 Train Loss: 0.4928\n",
      "Epoch 4 Step 951 Train Loss: 0.4468\n",
      "Epoch 4 Step 1001 Train Loss: 0.4944\n",
      "Epoch 4 Step 1051 Train Loss: 0.4722\n",
      "Epoch 4 Step 1101 Train Loss: 0.4423\n",
      "Epoch 4 Step 1151 Train Loss: 0.4905\n",
      "Epoch 4: Train Overall MSE: 0.0037 Validation Overall MSE: 0.0114. \n",
      "Train Top 20 DE MSE: 0.1420 Validation Top 20 DE MSE: 0.3307. \n",
      "Done!\n",
      "Start Testing...\n",
      "Best performing model: Test Top 20 DE MSE: 0.2195\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "gears_model_selfattn.train(epochs=4,lr=1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "perturb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
