{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Original Model Utils:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gears import PertData\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import pickle\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## utils\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import sys, os\n",
    "import requests\n",
    "from torch_geometric.data import Data\n",
    "from zipfile import ZipFile\n",
    "import tarfile\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "from dcor import distance_correlation\n",
    "from multiprocessing import Pool\n",
    "import scanpy as sc\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch_geometric.nn import SGConv\n",
    "\n",
    "def parse_single_pert(i):\n",
    "    a = i.split('+')[0]\n",
    "    b = i.split('+')[1]\n",
    "    if a == 'ctrl':\n",
    "        pert = b\n",
    "    else:\n",
    "        pert = a\n",
    "    return pert\n",
    "\n",
    "def parse_combo_pert(i):\n",
    "    return i.split('+')[0], i.split('+')[1]\n",
    "\n",
    "def combine_res(res_1, res_2):\n",
    "    res_out = {}\n",
    "    for key in res_1:\n",
    "        res_out[key] = np.concatenate([res_1[key], res_2[key]])\n",
    "    return res_out\n",
    "\n",
    "def parse_any_pert(p):\n",
    "    if ('ctrl' in p) and (p != 'ctrl'):\n",
    "        return [parse_single_pert(p)]\n",
    "    elif 'ctrl' not in p:\n",
    "        out = parse_combo_pert(p)\n",
    "        return [out[0], out[1]]\n",
    "\n",
    "def np_pearson_cor(x, y):\n",
    "    xv = x - x.mean(axis=0)\n",
    "    yv = y - y.mean(axis=0)\n",
    "    xvss = (xv * xv).sum(axis=0)\n",
    "    yvss = (yv * yv).sum(axis=0)\n",
    "    result = np.matmul(xv.transpose(), yv) / np.sqrt(np.outer(xvss, yvss))\n",
    "    # bound the values to -1 to 1 in the event of precision issues\n",
    "    return np.maximum(np.minimum(result, 1.0), -1.0)\n",
    "\n",
    "def dataverse_download(url, save_path):\n",
    "    \"\"\"\n",
    "    Dataverse download helper with progress bar\n",
    "\n",
    "    Args:\n",
    "        url (str): the url of the dataset\n",
    "        path (str): the path to save the dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    if os.path.exists(save_path):\n",
    "        print_sys('Found local copy...')\n",
    "    else:\n",
    "        print_sys(\"Downloading...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        total_size_in_bytes= int(response.headers.get('content-length', 0))\n",
    "        block_size = 1024\n",
    "        progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "        with open(save_path, 'wb') as file:\n",
    "            for data in response.iter_content(block_size):\n",
    "                progress_bar.update(len(data))\n",
    "                file.write(data)\n",
    "        progress_bar.close()\n",
    "        \n",
    "def zip_data_download_wrapper(url, save_path, data_path):\n",
    "    \"\"\"\n",
    "    Wrapper for zip file download\n",
    "\n",
    "    Args:\n",
    "        url (str): the url of the dataset\n",
    "        save_path (str): the path where the file is donwloaded\n",
    "        data_path (str): the path to save the extracted dataset\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        print_sys('Found local copy...')\n",
    "    else:\n",
    "        dataverse_download(url, save_path + '.zip')\n",
    "        print_sys('Extracting zip file...')\n",
    "        with ZipFile((save_path + '.zip'), 'r') as zip:\n",
    "            zip.extractall(path = data_path)\n",
    "        print_sys(\"Done!\")  \n",
    "        \n",
    "def tar_data_download_wrapper(url, save_path, data_path):\n",
    "    \"\"\"\n",
    "    Wrapper for tar file download\n",
    "\n",
    "    Args:\n",
    "        url (str): the url of the dataset\n",
    "        save_path (str): the path where the file is donwloaded\n",
    "        data_path (str): the path to save the extracted dataset\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        print_sys('Found local copy...')\n",
    "    else:\n",
    "        dataverse_download(url, save_path + '.tar.gz')\n",
    "        print_sys('Extracting tar file...')\n",
    "        with tarfile.open(save_path  + '.tar.gz') as tar:\n",
    "            tar.extractall(path= data_path)\n",
    "        print_sys(\"Done!\")  \n",
    "        \n",
    "def get_go_auto(gene_list, data_path, data_name):\n",
    "    \"\"\"\n",
    "    Get gene ontology data\n",
    "\n",
    "    Args:\n",
    "        gene_list (list): list of gene names\n",
    "        data_path (str): the path to save the extracted dataset\n",
    "        data_name (str): the name of the dataset\n",
    "\n",
    "    Returns:\n",
    "        df_edge_list (pd.DataFrame): gene ontology edge list\n",
    "    \"\"\"\n",
    "    go_path = os.path.join(data_path, data_name, 'go.csv')\n",
    "    \n",
    "    if os.path.exists(go_path):\n",
    "        return pd.read_csv(go_path)\n",
    "    else:\n",
    "        ## download gene2go.pkl\n",
    "        if not os.path.exists(os.path.join(data_path, 'gene2go.pkl')):\n",
    "            server_path = 'https://dataverse.harvard.edu/api/access/datafile/6153417'\n",
    "            dataverse_download(server_path, os.path.join(data_path, 'gene2go.pkl'))\n",
    "        with open(os.path.join(data_path, 'gene2go.pkl'), 'rb') as f:\n",
    "            gene2go = pickle.load(f)\n",
    "\n",
    "        gene2go = {i: list(gene2go[i]) for i in gene_list if i in gene2go}\n",
    "        edge_list = []\n",
    "        for g1 in tqdm(gene2go.keys()):\n",
    "            for g2 in gene2go.keys():\n",
    "                edge_list.append((g1, g2, len(np.intersect1d(gene2go[g1],\n",
    "                   gene2go[g2]))/len(np.union1d(gene2go[g1], gene2go[g2]))))\n",
    "\n",
    "        edge_list_filter = [i for i in edge_list if i[2] > 0]\n",
    "        further_filter = [i for i in edge_list if i[2] > 0.1]\n",
    "        df_edge_list = pd.DataFrame(further_filter).rename(columns = {0: 'gene1',\n",
    "                                                                      1: 'gene2',\n",
    "                                                                      2: 'score'})\n",
    "\n",
    "        df_edge_list = df_edge_list.rename(columns = {'gene1': 'source',\n",
    "                                                      'gene2': 'target',\n",
    "                                                      'score': 'importance'})\n",
    "        df_edge_list.to_csv(go_path, index = False)        \n",
    "        return df_edge_list\n",
    "\n",
    "class GeneSimNetwork():\n",
    "    \"\"\"\n",
    "    GeneSimNetwork class\n",
    "\n",
    "    Args:\n",
    "        edge_list (pd.DataFrame): edge list of the network\n",
    "        gene_list (list): list of gene names\n",
    "        node_map (dict): dictionary mapping gene names to node indices\n",
    "\n",
    "    Attributes:\n",
    "        edge_index (torch.Tensor): edge index of the network\n",
    "        edge_weight (torch.Tensor): edge weight of the network\n",
    "        G (nx.DiGraph): networkx graph object\n",
    "    \"\"\"\n",
    "    def __init__(self, edge_list, gene_list, node_map):\n",
    "        \"\"\"\n",
    "        Initialize GeneSimNetwork class\n",
    "        \"\"\"\n",
    "\n",
    "        self.edge_list = edge_list\n",
    "        self.G = nx.from_pandas_edgelist(self.edge_list, source='source',\n",
    "                        target='target', edge_attr=['importance'],\n",
    "                        create_using=nx.DiGraph())    \n",
    "        self.gene_list = gene_list\n",
    "        for n in self.gene_list:\n",
    "            if n not in self.G.nodes():\n",
    "                self.G.add_node(n)\n",
    "        \n",
    "        edge_index_ = [(node_map[e[0]], node_map[e[1]]) for e in\n",
    "                      self.G.edges]\n",
    "        self.edge_index = torch.tensor(edge_index_, dtype=torch.long).T\n",
    "        #self.edge_weight = torch.Tensor(self.edge_list['importance'].values)\n",
    "        \n",
    "        edge_attr = nx.get_edge_attributes(self.G, 'importance') \n",
    "        importance = np.array([edge_attr[e] for e in self.G.edges])\n",
    "        self.edge_weight = torch.Tensor(importance)\n",
    "\n",
    "def get_GO_edge_list(args):\n",
    "    \"\"\"\n",
    "    Get gene ontology edge list\n",
    "    \"\"\"\n",
    "    g1, gene2go = args\n",
    "    edge_list = []\n",
    "    for g2 in gene2go.keys():\n",
    "        score = len(gene2go[g1].intersection(gene2go[g2])) / len(\n",
    "            gene2go[g1].union(gene2go[g2]))\n",
    "        if score > 0.1:\n",
    "            edge_list.append((g1, g2, score))\n",
    "    return edge_list\n",
    "        \n",
    "def make_GO(data_path, pert_list, data_name, num_workers=25, save=True):\n",
    "    \"\"\"\n",
    "    Creates Gene Ontology graph from a custom set of genes\n",
    "    \"\"\"\n",
    "\n",
    "    fname = './data/go_essential_' + data_name + '.csv'\n",
    "    if os.path.exists(fname):\n",
    "        return pd.read_csv(fname)\n",
    "\n",
    "    with open(os.path.join(data_path, 'gene2go_all.pkl'), 'rb') as f:\n",
    "        gene2go = pickle.load(f)\n",
    "    #perturbation_list\n",
    "    gene2go = {i: gene2go[i] for i in pert_list}\n",
    "\n",
    "    print('Creating custom GO graph, this can take a few minutes')\n",
    "    with Pool(num_workers) as p:\n",
    "        all_edge_list = list(\n",
    "            tqdm(p.imap(get_GO_edge_list, ((g, gene2go) for g in gene2go.keys())),\n",
    "                      total=len(gene2go.keys())))\n",
    "    edge_list = []\n",
    "    for i in all_edge_list:\n",
    "        edge_list = edge_list + i\n",
    "\n",
    "    df_edge_list = pd.DataFrame(edge_list).rename(\n",
    "        columns={0: 'source', 1: 'target', 2: 'importance'})\n",
    "    \n",
    "    if save:\n",
    "        if(data_path is not None):\n",
    "            fname = os.path.join(data_path,f\"go_essential{data_name}.csv\")\n",
    "        print(f'Saving edge_list to file {fname}')\n",
    "        df_edge_list.to_csv(fname, index=False)\n",
    "\n",
    "    return df_edge_list\n",
    "\n",
    "def get_similarity_network(network_type, adata, threshold, k,\n",
    "                           data_path, data_name, split, seed, train_gene_set_size,\n",
    "                           set2conditions, default_pert_graph=True, pert_list=None):\n",
    "    \n",
    "    if network_type == 'co-express':\n",
    "        df_out = get_coexpression_network_from_train(adata, threshold, k,\n",
    "                                                     data_path, data_name, split,\n",
    "                                                     seed, train_gene_set_size,\n",
    "                                                     set2conditions)\n",
    "    elif network_type == 'go':\n",
    "        if default_pert_graph:\n",
    "            server_path = 'https://dataverse.harvard.edu/api/access/datafile/6934319'\n",
    "            tar_data_download_wrapper(server_path, \n",
    "                                     os.path.join(data_path, 'go_essential_all'),\n",
    "                                     data_path)\n",
    "            df_jaccard = pd.read_csv(os.path.join(data_path, \n",
    "                                     'go_essential_all/go_essential_all.csv'))\n",
    "\n",
    "        else:\n",
    "            df_jaccard = make_GO(data_path, pert_list, data_name)\n",
    "\n",
    "        df_out = df_jaccard.groupby('target').apply(lambda x: x.nlargest(k + 1,\n",
    "                                    ['importance'])).reset_index(drop = True)\n",
    "\n",
    "    return df_out\n",
    "\n",
    "def get_coexpression_network_from_train(adata, threshold, k, data_path,\n",
    "                                        data_name, split, seed, train_gene_set_size,\n",
    "                                        set2conditions):\n",
    "    \"\"\"\n",
    "    Infer co-expression network from training data\n",
    "\n",
    "    Args:\n",
    "        adata (anndata.AnnData): anndata object\n",
    "        threshold (float): threshold for co-expression\n",
    "        k (int): number of edges to keep\n",
    "        data_path (str): path to data\n",
    "        data_name (str): name of dataset\n",
    "        split (str): split of dataset\n",
    "        seed (int): seed for random number generator\n",
    "        train_gene_set_size (int): size of training gene set\n",
    "        set2conditions (dict): dictionary of perturbations to conditions\n",
    "    \"\"\"\n",
    "    \n",
    "    fname = os.path.join(os.path.join(data_path, data_name), split + '_'  +\n",
    "                         str(seed) + '_' + str(train_gene_set_size) + '_' +\n",
    "                         str(threshold) + '_' + str(k) +\n",
    "                         '_co_expression_network.csv')\n",
    "    \n",
    "    if os.path.exists(fname):\n",
    "        return pd.read_csv(fname)\n",
    "    else:\n",
    "        gene_list = [f for f in adata.var.gene_name.values]\n",
    "        idx2gene = dict(zip(range(len(gene_list)), gene_list)) \n",
    "        X = adata.X\n",
    "        train_perts = set2conditions['train']\n",
    "        X_tr = X[np.isin(adata.obs.condition, [i for i in train_perts if 'ctrl' in i])]\n",
    "        gene_list = adata.var['gene_name'].values\n",
    "\n",
    "        X_tr = X_tr.toarray()\n",
    "        out = np_pearson_cor(X_tr, X_tr)\n",
    "        out[np.isnan(out)] = 0\n",
    "        out = np.abs(out)\n",
    "\n",
    "        out_sort_idx = np.argsort(out)[:, -(k + 1):]\n",
    "        out_sort_val = np.sort(out)[:, -(k + 1):]\n",
    "\n",
    "        df_g = []\n",
    "        for i in range(out_sort_idx.shape[0]):\n",
    "            target = idx2gene[i]\n",
    "            for j in range(out_sort_idx.shape[1]):\n",
    "                df_g.append((idx2gene[out_sort_idx[i, j]], target, out_sort_val[i, j]))\n",
    "\n",
    "        df_g = [i for i in df_g if i[2] > threshold]\n",
    "        df_co_expression = pd.DataFrame(df_g).rename(columns = {0: 'source',\n",
    "                                                                1: 'target',\n",
    "                                                                2: 'importance'})\n",
    "        df_co_expression.to_csv(fname, index = False)\n",
    "        return df_co_expression\n",
    "    \n",
    "def filter_pert_in_go(condition, pert_names):\n",
    "    \"\"\"\n",
    "    Filter perturbations in GO graph\n",
    "\n",
    "    Args:\n",
    "        condition (str): whether condition is 'ctrl' or not\n",
    "        pert_names (list): list of perturbations\n",
    "    \"\"\"\n",
    "\n",
    "    if condition == 'ctrl':\n",
    "        return True\n",
    "    else:\n",
    "        cond1 = condition.split('+')[0]\n",
    "        cond2 = condition.split('+')[1]\n",
    "        num_ctrl = (cond1 == 'ctrl') + (cond2 == 'ctrl')\n",
    "        num_in_perts = (cond1 in pert_names) + (cond2 in pert_names)\n",
    "        if num_ctrl + num_in_perts == 2:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "def uncertainty_loss_fct(pred, logvar, y, perts, reg = 0.1, ctrl = None,\n",
    "                         direction_lambda = 1e-3, dict_filter = None):\n",
    "    \"\"\"\n",
    "    Uncertainty loss function\n",
    "\n",
    "    Args:\n",
    "        pred (torch.tensor): predicted values\n",
    "        logvar (torch.tensor): log variance\n",
    "        y (torch.tensor): true values\n",
    "        perts (list): list of perturbations\n",
    "        reg (float): regularization parameter\n",
    "        ctrl (str): control perturbation\n",
    "        direction_lambda (float): direction loss weight hyperparameter\n",
    "        dict_filter (dict): dictionary of perturbations to conditions\n",
    "\n",
    "    \"\"\"\n",
    "    gamma = 2                     \n",
    "    perts = np.array(perts)\n",
    "    losses = torch.tensor(0.0, requires_grad=True).to(pred.device)\n",
    "    for p in set(perts):\n",
    "        if p!= 'ctrl':\n",
    "            retain_idx = dict_filter[p]\n",
    "            pred_p = pred[np.where(perts==p)[0]][:, retain_idx]\n",
    "            y_p = y[np.where(perts==p)[0]][:, retain_idx]\n",
    "            logvar_p = logvar[np.where(perts==p)[0]][:, retain_idx]\n",
    "        else:\n",
    "            pred_p = pred[np.where(perts==p)[0]]\n",
    "            y_p = y[np.where(perts==p)[0]]\n",
    "            logvar_p = logvar[np.where(perts==p)[0]]\n",
    "                         \n",
    "        # uncertainty based loss\n",
    "        losses += torch.sum((pred_p - y_p)**(2 + gamma) + reg * torch.exp(\n",
    "            -logvar_p)  * (pred_p - y_p)**(2 + gamma))/pred_p.shape[0]/pred_p.shape[1]\n",
    "                         \n",
    "        # direction loss                 \n",
    "        if p!= 'ctrl':\n",
    "            losses += torch.sum(direction_lambda *\n",
    "                                (torch.sign(y_p - ctrl[retain_idx]) -\n",
    "                                 torch.sign(pred_p - ctrl[retain_idx]))**2)/\\\n",
    "                                 pred_p.shape[0]/pred_p.shape[1]\n",
    "        else:\n",
    "            losses += torch.sum(direction_lambda *\n",
    "                                (torch.sign(y_p - ctrl) -\n",
    "                                 torch.sign(pred_p - ctrl))**2)/\\\n",
    "                                 pred_p.shape[0]/pred_p.shape[1]\n",
    "            \n",
    "    return losses/(len(set(perts)))\n",
    "\n",
    "def loss_fct(pred, y, perts, ctrl = None, direction_lambda = 1e-3, dict_filter = None):\n",
    "    \"\"\"\n",
    "    Main MSE Loss function, includes direction loss\n",
    "\n",
    "    Args:\n",
    "        pred (torch.tensor): predicted values\n",
    "        y (torch.tensor): true values\n",
    "        perts (list): list of perturbations\n",
    "        ctrl (str): control perturbation\n",
    "        direction_lambda (float): direction loss weight hyperparameter\n",
    "        dict_filter (dict): dictionary of perturbations to conditions\n",
    "\n",
    "    \"\"\"\n",
    "    gamma = 2\n",
    "    mse_p = torch.nn.MSELoss()\n",
    "    perts = np.array(perts)\n",
    "    losses = torch.tensor(0.0, requires_grad=True).to(pred.device)\n",
    "\n",
    "    for p in set(perts):\n",
    "        pert_idx = np.where(perts == p)[0]\n",
    "        \n",
    "        # during training, we remove the all zero genes into calculation of loss.\n",
    "        # this gives a cleaner direction loss. empirically, the performance stays the same.\n",
    "        if p!= 'ctrl':\n",
    "            # print(dict_filter)\n",
    "            retain_idx = dict_filter[p]\n",
    "            pred_p = pred[pert_idx][:, retain_idx]\n",
    "            y_p = y[pert_idx][:, retain_idx]\n",
    "        else:\n",
    "            pred_p = pred[pert_idx]\n",
    "            y_p = y[pert_idx]\n",
    "        losses = losses + torch.sum((pred_p - y_p)**(2 + gamma))/pred_p.shape[0]/pred_p.shape[1]\n",
    "                         \n",
    "        ## direction loss\n",
    "        if (p!= 'ctrl'):\n",
    "            losses = losses + torch.sum(direction_lambda *\n",
    "                                (torch.sign(y_p - ctrl[retain_idx]) -\n",
    "                                 torch.sign(pred_p - ctrl[retain_idx]))**2)/\\\n",
    "                                 pred_p.shape[0]/pred_p.shape[1]\n",
    "        else:\n",
    "            losses = losses + torch.sum(direction_lambda * (torch.sign(y_p - ctrl) -\n",
    "                                                torch.sign(pred_p - ctrl))**2)/\\\n",
    "                                                pred_p.shape[0]/pred_p.shape[1]\n",
    "    return losses/(len(set(perts)))\n",
    "\n",
    "def print_sys(s):\n",
    "    \"\"\"system print\n",
    "\n",
    "    Args:\n",
    "        s (str): the string to print\n",
    "    \"\"\"\n",
    "    print(s, flush = True, file = sys.stderr)\n",
    "    \n",
    "def create_cell_graph_for_prediction(X, pert_idx, pert_gene):\n",
    "    \"\"\"\n",
    "    Create a perturbation specific cell graph for inference\n",
    "\n",
    "    Args:\n",
    "        X (np.array): gene expression matrix\n",
    "        pert_idx (list): list of perturbation indices\n",
    "        pert_gene (list): list of perturbations\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if pert_idx is None:\n",
    "        pert_idx = [-1]\n",
    "    return Data(x=torch.Tensor(X).T, pert_idx = pert_idx, pert=pert_gene)\n",
    "    \n",
    "def create_cell_graph_dataset_for_prediction(pert_gene, ctrl_adata, gene_names,\n",
    "                                             device, num_samples = 300):\n",
    "    \"\"\"\n",
    "    Create a perturbation specific cell graph dataset for inference\n",
    "\n",
    "    Args:\n",
    "        pert_gene (list): list of perturbations\n",
    "        ctrl_adata (anndata): control anndata\n",
    "        gene_names (list): list of gene names\n",
    "        device (torch.device): device to use\n",
    "        num_samples (int): number of samples to use for inference (default: 300)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the indices (and signs) of applied perturbation\n",
    "    pert_idx = [np.where(p == np.array(gene_names))[0][0] for p in pert_gene]\n",
    "\n",
    "    Xs = ctrl_adata[np.random.randint(0, len(ctrl_adata), num_samples), :].X.toarray()\n",
    "    # Create cell graphs\n",
    "    cell_graphs = [create_cell_graph_for_prediction(X, pert_idx, pert_gene).to(device) for X in Xs]\n",
    "    return cell_graphs\n",
    "\n",
    "def get_coeffs(singles_expr, first_expr, second_expr, double_expr):\n",
    "    \"\"\"\n",
    "    Get coefficients for GI calculation\n",
    "\n",
    "    Args:\n",
    "        singles_expr (np.array): single perturbation expression\n",
    "        first_expr (np.array): first perturbation expression\n",
    "        second_expr (np.array): second perturbation expression\n",
    "        double_expr (np.array): double perturbation expression\n",
    "\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    results['ts'] = TheilSenRegressor(fit_intercept=False,\n",
    "                          max_subpopulation=1e5,\n",
    "                          max_iter=1000,\n",
    "                          random_state=1000)   \n",
    "    X = singles_expr\n",
    "    y = double_expr\n",
    "    results['ts'].fit(X, y.ravel())\n",
    "    Zts = results['ts'].predict(X)\n",
    "    results['c1'] = results['ts'].coef_[0]\n",
    "    results['c2'] = results['ts'].coef_[1]\n",
    "    results['mag'] = np.sqrt((results['c1']**2 + results['c2']**2))\n",
    "    \n",
    "    results['dcor'] = distance_correlation(singles_expr, double_expr)\n",
    "    results['dcor_singles'] = distance_correlation(first_expr, second_expr)\n",
    "    results['dcor_first'] = distance_correlation(first_expr, double_expr)\n",
    "    results['dcor_second'] = distance_correlation(second_expr, double_expr)\n",
    "    results['corr_fit'] = np.corrcoef(Zts.flatten(), double_expr.flatten())[0,1]\n",
    "    results['dominance'] = np.abs(np.log10(results['c1']/results['c2']))\n",
    "    results['eq_contr'] = np.min([results['dcor_first'], results['dcor_second']])/\\\n",
    "                        np.max([results['dcor_first'], results['dcor_second']])\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_GI_params(preds, combo):\n",
    "    \"\"\"\n",
    "    Get GI parameters\n",
    "\n",
    "    Args:\n",
    "        preds (dict): dictionary of predictions\n",
    "        combo (list): list of perturbations\n",
    "\n",
    "    \"\"\"\n",
    "    singles_expr = np.array([preds[combo[0]], preds[combo[1]]]).T\n",
    "    first_expr = np.array(preds[combo[0]]).T\n",
    "    second_expr = np.array(preds[combo[1]]).T\n",
    "    double_expr = np.array(preds[combo[0]+'_'+combo[1]]).T\n",
    "    \n",
    "    return get_coeffs(singles_expr, first_expr, second_expr, double_expr)\n",
    "\n",
    "def get_GI_genes_idx(adata, GI_gene_file):\n",
    "    \"\"\"\n",
    "    Optional: Reads a file containing a list of GI genes (usually those\n",
    "    with high mean expression)\n",
    "\n",
    "    Args:\n",
    "        adata (anndata): anndata object\n",
    "        GI_gene_file (str): file containing GI genes (generally corresponds\n",
    "        to genes with high mean expression)\n",
    "    \"\"\"\n",
    "    # Genes used for linear model fitting\n",
    "    GI_genes = np.load(GI_gene_file, allow_pickle=True)\n",
    "    GI_genes_idx = np.where([g in GI_genes for g in adata.var.gene_name.values])[0]\n",
    "    \n",
    "    return GI_genes_idx\n",
    "\n",
    "def get_mean_control(adata):\n",
    "    \"\"\"\n",
    "    Get mean control expression\n",
    "    \"\"\"\n",
    "    mean_ctrl_exp = adata[adata.obs['condition'] == 'ctrl'].to_df().mean()\n",
    "    return mean_ctrl_exp\n",
    "\n",
    "def get_genes_from_perts(perts):\n",
    "    \"\"\"\n",
    "    Returns list of genes involved in a given perturbation list\n",
    "    \"\"\"\n",
    "\n",
    "    if type(perts) is str:\n",
    "        perts = [perts]\n",
    "    gene_list = [p.split('+') for p in np.unique(perts)]\n",
    "    gene_list = [item for sublist in gene_list for item in sublist]\n",
    "    gene_list = [g for g in gene_list if g != 'ctrl']\n",
    "    return list(np.unique(gene_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inference\n",
    "def evaluate(loader, model, uncertainty, device):\n",
    "    \"\"\"\n",
    "    Run model in inference mode using a given data loader\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    pert_cat = []\n",
    "    pred = []\n",
    "    truth = []\n",
    "    pred_de = []\n",
    "    truth_de = []\n",
    "    results = {}\n",
    "    logvar = []\n",
    "    \n",
    "    for itr, batch in enumerate(loader):\n",
    "\n",
    "        batch.to(device)\n",
    "        pert_cat.extend(batch.pert)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if uncertainty:\n",
    "                p, unc = model(batch)\n",
    "                logvar.extend(unc.cpu())\n",
    "            else:\n",
    "                p = model(batch)\n",
    "            t = batch.y\n",
    "            pred.extend(p.cpu())\n",
    "            truth.extend(t.cpu())\n",
    "            \n",
    "            # Differentially expressed genes\n",
    "            for itr, de_idx in enumerate(batch.de_idx):\n",
    "                pred_de.append(p[itr, de_idx])\n",
    "                truth_de.append(t[itr, de_idx])\n",
    "\n",
    "    # all genes\n",
    "    results['pert_cat'] = np.array(pert_cat)\n",
    "    pred = torch.stack(pred)\n",
    "    truth = torch.stack(truth)\n",
    "    results['pred']= pred.detach().cpu().numpy()\n",
    "    results['truth']= truth.detach().cpu().numpy()\n",
    "\n",
    "    pred_de = torch.stack(pred_de)\n",
    "    truth_de = torch.stack(truth_de)\n",
    "    results['pred_de']= pred_de.detach().cpu().numpy()\n",
    "    results['truth_de']= truth_de.detach().cpu().numpy()\n",
    "    \n",
    "    if uncertainty:\n",
    "        results['logvar'] = torch.stack(logvar).detach().cpu().numpy()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compute_metrics(results):\n",
    "    \"\"\"\n",
    "    Given results from a model run and the ground truth, compute metrics\n",
    "\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    metrics_pert = {}\n",
    "\n",
    "    metric2fct = {\n",
    "           'mse': mse,\n",
    "           'pearson': pearsonr,\n",
    "    }\n",
    "    \n",
    "    for m in metric2fct.keys():\n",
    "        metrics[m] = []\n",
    "        metrics[m + '_de'] = []\n",
    "\n",
    "    for pert in np.unique(results['pert_cat']):\n",
    "\n",
    "        metrics_pert[pert] = {}\n",
    "        p_idx = np.where(results['pert_cat'] == pert)[0]\n",
    "            \n",
    "        for m, fct in metric2fct.items():\n",
    "            if m == 'pearson':\n",
    "                #results \n",
    "                #results['pred'] is every single possible perturbation's prediction.    \n",
    "                pred_subset_expr = results['pred_de'][p_idx]\n",
    "                truth_subset_expr = results['truth_de'][p_idx]\n",
    "                try:\n",
    "                    val = fct(results['pred_de'][p_idx].mean(0), results['truth_de'][p_idx].mean(0))[0]\n",
    "                except:\n",
    "                    # print(f\" pred{results['pred_de'][p_idx].shape}\", f\" truth :{results['truth_de'][p_idx].shape}\")\n",
    "                    val = fct(results['pred_de'][p_idx], results['truth_de'][p_idx])[0]\n",
    "                \n",
    "                \n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "            else:\n",
    "                val = fct(results['pred'][p_idx].mean(0), results['truth'][p_idx].mean(0))\n",
    "\n",
    "            metrics_pert[pert][m] = val\n",
    "            metrics[m].append(metrics_pert[pert][m])\n",
    "\n",
    "       \n",
    "        if pert != 'ctrl':\n",
    "            \n",
    "            for m, fct in metric2fct.items():\n",
    "                if m == 'pearson':\n",
    "                    try:\n",
    "                        val = fct(results['pred_de'][p_idx].mean(0), results['truth_de'][p_idx].mean(0))[0]\n",
    "                    except:\n",
    "                        # print(f\" pred{results['pred_de'][p_idx].shape}\", f\" truth :{results['truth_de'][p_idx].shape}\")\n",
    "                        val = fct(results['pred_de'][p_idx], results['truth_de'][p_idx])[0]\n",
    "                    # val = fct(results['pred_de'][p_idx].mean(0), results['truth_de'][p_idx].mean(0))[0]\n",
    "                    if np.isnan(val):\n",
    "                        val = 0\n",
    "                else:\n",
    "                    val = fct(results['pred_de'][p_idx].mean(0), results['truth_de'][p_idx].mean(0))\n",
    "                    \n",
    "                metrics_pert[pert][m + '_de'] = val\n",
    "                metrics[m + '_de'].append(metrics_pert[pert][m + '_de'])\n",
    "\n",
    "        else:\n",
    "            for m, fct in metric2fct.items():\n",
    "                metrics_pert[pert][m + '_de'] = 0\n",
    "    \n",
    "    for m in metric2fct.keys():\n",
    "        \n",
    "        metrics[m] = np.mean(metrics[m])\n",
    "        metrics[m + '_de'] = np.mean(metrics[m + '_de'])\n",
    "    \n",
    "    return metrics, metrics_pert\n",
    "\n",
    "def non_zero_analysis(adata, test_res):\n",
    "    metric2fct = {\n",
    "           'pearson': pearsonr,\n",
    "           'mse': mse\n",
    "    }\n",
    "\n",
    "    pert_metric = {}\n",
    "    \n",
    "    ## in silico modeling and upperbounding\n",
    "    pert2pert_full_id = dict(adata.obs[['condition', 'condition_name']].values)\n",
    "    geneid2name = dict(zip(adata.var.index.values, adata.var['gene_name']))\n",
    "    geneid2idx = dict(zip(adata.var.index.values, range(len(adata.var.index.values))))\n",
    "\n",
    "    # calculate mean expression for each condition\n",
    "    unique_conditions = adata.obs.condition.unique()\n",
    "    conditions2index = {}\n",
    "    for i in unique_conditions:\n",
    "        conditions2index[i] = np.where(adata.obs.condition == i)[0]\n",
    "\n",
    "    condition2mean_expression = {}\n",
    "    for i, j in conditions2index.items():\n",
    "        condition2mean_expression[i] = np.mean(adata.X[j], axis = 0)\n",
    "    pert_list = np.array(list(condition2mean_expression.keys()))\n",
    "    mean_expression = np.array(list(condition2mean_expression.values())).reshape(len(adata.obs.condition.unique()), adata.X.toarray().shape[1])\n",
    "    ctrl = mean_expression[np.where(pert_list == 'ctrl')[0]]\n",
    "    \n",
    "    gene_list = adata.var['gene_name'].values\n",
    "\n",
    "    for pert in np.unique(test_res['pert_cat']):\n",
    "        pert_metric[pert] = {}\n",
    "        \n",
    "        pert_idx = np.where(test_res['pert_cat'] == pert)[0]    \n",
    "        de_idx = [geneid2idx[i] for i in adata.uns['top_non_zero_de_20'][pert2pert_full_id[pert]]]\n",
    "\n",
    "        direc_change = np.abs(np.sign(test_res['pred'][pert_idx].mean(0)[de_idx] - ctrl[0][de_idx]) - np.sign(test_res['truth'][pert_idx].mean(0)[de_idx] - ctrl[0][de_idx]))            \n",
    "        frac_correct_direction = len(np.where(direc_change == 0)[0])/len(de_idx)\n",
    "        pert_metric[pert]['frac_correct_direction_top20_non_zero'] = frac_correct_direction\n",
    "        \n",
    "        frac_direction_opposite = len(np.where(direc_change == 2)[0])/len(de_idx)\n",
    "        pert_metric[pert]['frac_opposite_direction_top20_non_zero'] = frac_direction_opposite\n",
    "        \n",
    "        frac_direction_opposite = len(np.where(direc_change == 1)[0])/len(de_idx)\n",
    "        pert_metric[pert]['frac_0/1_direction_top20_non_zero'] = frac_direction_opposite\n",
    "        \n",
    "        mean = np.mean(test_res['truth'][pert_idx][:, de_idx], axis = 0)\n",
    "        std = np.std(test_res['truth'][pert_idx][:, de_idx], axis = 0)\n",
    "        min_ = np.min(test_res['truth'][pert_idx][:, de_idx], axis = 0)\n",
    "        max_ = np.max(test_res['truth'][pert_idx][:, de_idx], axis = 0)\n",
    "        q25 = np.quantile(test_res['truth'][pert_idx][:, de_idx], 0.25, axis = 0)\n",
    "        q75 = np.quantile(test_res['truth'][pert_idx][:, de_idx], 0.75, axis = 0)\n",
    "        q55 = np.quantile(test_res['truth'][pert_idx][:, de_idx], 0.55, axis = 0)\n",
    "        q45 = np.quantile(test_res['truth'][pert_idx][:, de_idx], 0.45, axis = 0)\n",
    "        q40 = np.quantile(test_res['truth'][pert_idx][:, de_idx], 0.4, axis = 0)\n",
    "        q60 = np.quantile(test_res['truth'][pert_idx][:, de_idx], 0.6, axis = 0)\n",
    "        \n",
    "        zero_des = np.intersect1d(np.where(min_ == 0)[0], np.where(max_ == 0)[0])\n",
    "        nonzero_des = np.setdiff1d(list(range(20)), zero_des)\n",
    "        \n",
    "        if len(nonzero_des) == 0:\n",
    "            pass\n",
    "            # pert that all de genes are 0...\n",
    "        else:            \n",
    "            pred_mean = np.mean(test_res['pred'][pert_idx][:, de_idx], axis = 0).reshape(-1,)\n",
    "            true_mean = np.mean(test_res['truth'][pert_idx][:, de_idx], axis = 0).reshape(-1,)\n",
    "           \n",
    "            in_range = (pred_mean[nonzero_des] >= min_[nonzero_des]) & (pred_mean[nonzero_des] <= max_[nonzero_des])\n",
    "            frac_in_range = sum(in_range)/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_in_range_non_zero'] = frac_in_range\n",
    "\n",
    "            in_range_5 = (pred_mean[nonzero_des] >= q45[nonzero_des]) & (pred_mean[nonzero_des] <= q55[nonzero_des])\n",
    "            frac_in_range_45_55 = sum(in_range_5)/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_in_range_45_55_non_zero'] = frac_in_range_45_55\n",
    "\n",
    "            in_range_10 = (pred_mean[nonzero_des] >= q40[nonzero_des]) & (pred_mean[nonzero_des] <= q60[nonzero_des])\n",
    "            frac_in_range_40_60 = sum(in_range_10)/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_in_range_40_60_non_zero'] = frac_in_range_40_60\n",
    "\n",
    "            in_range_25 = (pred_mean[nonzero_des] >= q25[nonzero_des]) & (pred_mean[nonzero_des] <= q75[nonzero_des])\n",
    "            frac_in_range_25_75 = sum(in_range_25)/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_in_range_25_75_non_zero'] = frac_in_range_25_75\n",
    "\n",
    "            zero_idx = np.where(std > 0)[0]\n",
    "            sigma = (np.abs(pred_mean[zero_idx] - mean[zero_idx]))/(std[zero_idx])\n",
    "            pert_metric[pert]['mean_sigma_non_zero'] = np.mean(sigma)\n",
    "            pert_metric[pert]['std_sigma_non_zero'] = np.std(sigma)\n",
    "            pert_metric[pert]['frac_sigma_below_1_non_zero'] = 1 - len(np.where(sigma > 1)[0])/len(zero_idx)\n",
    "            pert_metric[pert]['frac_sigma_below_2_non_zero'] = 1 - len(np.where(sigma > 2)[0])/len(zero_idx)\n",
    "        \n",
    "        p_idx = np.where(test_res['pert_cat'] == pert)[0]\n",
    "        for m, fct in metric2fct.items():\n",
    "            if m != 'mse':\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx] - ctrl[0][de_idx], test_res['truth'][p_idx].mean(0)[de_idx]-ctrl[0][de_idx])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_delta_top20_de_non_zero'] = val\n",
    "\n",
    "\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx], test_res['truth'][p_idx].mean(0)[de_idx])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_top20_de_non_zero'] = val\n",
    "            else:\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx] - ctrl[0][de_idx], test_res['truth'][p_idx].mean(0)[de_idx]-ctrl[0][de_idx])\n",
    "                pert_metric[pert][m + '_top20_de_non_zero'] = val\n",
    "                \n",
    "    return pert_metric\n",
    "\n",
    "def non_dropout_analysis(adata, test_res):\n",
    "    metric2fct = {\n",
    "           'pearson': pearsonr,\n",
    "           'mse': mse\n",
    "    }\n",
    "\n",
    "    pert_metric = {}\n",
    "    \n",
    "    ## in silico modeling and upperbounding\n",
    "    pert2pert_full_id = dict(adata.obs[['condition', 'condition_name']].values)\n",
    "    geneid2name = dict(zip(adata.var.index.values, adata.var['gene_name']))\n",
    "    geneid2idx = dict(zip(adata.var.index.values, range(len(adata.var.index.values))))\n",
    "\n",
    "    # calculate mean expression for each condition\n",
    "    unique_conditions = adata.obs.condition.unique()\n",
    "    conditions2index = {}\n",
    "    for i in unique_conditions:\n",
    "        conditions2index[i] = np.where(adata.obs.condition == i)[0]\n",
    "\n",
    "    condition2mean_expression = {}\n",
    "    for i, j in conditions2index.items():\n",
    "        condition2mean_expression[i] = np.mean(adata.X[j], axis = 0)\n",
    "    pert_list = np.array(list(condition2mean_expression.keys()))\n",
    "    mean_expression = np.array(list(condition2mean_expression.values())).reshape(len(adata.obs.condition.unique()), adata.X.toarray().shape[1])\n",
    "    ctrl = mean_expression[np.where(pert_list == 'ctrl')[0]]\n",
    "    \n",
    "    gene_list = adata.var['gene_name'].values\n",
    "\n",
    "    for pert in np.unique(test_res['pert_cat']):\n",
    "        pert_metric[pert] = {}\n",
    "        \n",
    "        pert_idx = np.where(test_res['pert_cat'] == pert)[0]    \n",
    "        de_idx = [geneid2idx[i] for i in adata.uns['top_non_dropout_de_20'][pert2pert_full_id[pert]]]\n",
    "        non_zero_idx = adata.uns['non_zeros_gene_idx'][pert2pert_full_id[pert]]\n",
    "        non_dropout_gene_idx = adata.uns['non_dropout_gene_idx'][pert2pert_full_id[pert]]\n",
    "             \n",
    "        direc_change = np.abs(np.sign(test_res['pred'][pert_idx].mean(0)[de_idx] - ctrl[0][de_idx]) - np.sign(test_res['truth'][pert_idx].mean(0)[de_idx] - ctrl[0][de_idx]))            \n",
    "        frac_correct_direction = len(np.where(direc_change == 0)[0])/len(de_idx)\n",
    "        pert_metric[pert]['frac_correct_direction_top20_non_dropout'] = frac_correct_direction\n",
    "        \n",
    "        frac_direction_opposite = len(np.where(direc_change == 2)[0])/len(de_idx)\n",
    "        pert_metric[pert]['frac_opposite_direction_top20_non_dropout'] = frac_direction_opposite\n",
    "        \n",
    "        frac_direction_opposite = len(np.where(direc_change == 1)[0])/len(de_idx)\n",
    "        pert_metric[pert]['frac_0/1_direction_top20_non_dropout'] = frac_direction_opposite\n",
    "        \n",
    "        direc_change = np.abs(np.sign(test_res['pred'][pert_idx].mean(0)[non_zero_idx] - ctrl[0][non_zero_idx]) - np.sign(test_res['truth'][pert_idx].mean(0)[non_zero_idx] - ctrl[0][non_zero_idx]))            \n",
    "        frac_correct_direction = len(np.where(direc_change == 0)[0])/len(non_zero_idx)\n",
    "        pert_metric[pert]['frac_correct_direction_non_zero'] = frac_correct_direction\n",
    "\n",
    "        frac_direction_opposite = len(np.where(direc_change == 2)[0])/len(non_zero_idx)\n",
    "        pert_metric[pert]['frac_opposite_direction_non_zero'] = frac_direction_opposite\n",
    "        \n",
    "        frac_direction_opposite = len(np.where(direc_change == 1)[0])/len(non_zero_idx)\n",
    "        pert_metric[pert]['frac_0/1_direction_non_zero'] = frac_direction_opposite\n",
    "        \n",
    "        direc_change = np.abs(np.sign(test_res['pred'][pert_idx].mean(0)[non_dropout_gene_idx] - ctrl[0][non_dropout_gene_idx]) - np.sign(test_res['truth'][pert_idx].mean(0)[non_dropout_gene_idx] - ctrl[0][non_dropout_gene_idx]))            \n",
    "        frac_correct_direction = len(np.where(direc_change == 0)[0])/len(non_dropout_gene_idx)\n",
    "        pert_metric[pert]['frac_correct_direction_non_dropout'] = frac_correct_direction\n",
    "        \n",
    "        frac_direction_opposite = len(np.where(direc_change == 2)[0])/len(non_dropout_gene_idx)\n",
    "        pert_metric[pert]['frac_opposite_direction_non_dropout'] = frac_direction_opposite\n",
    "        \n",
    "        frac_direction_opposite = len(np.where(direc_change == 1)[0])/len(non_dropout_gene_idx)\n",
    "        pert_metric[pert]['frac_0/1_direction_non_dropout'] = frac_direction_opposite\n",
    "        \n",
    "        mean = np.mean(test_res['truth'][pert_idx][:, de_idx], axis = 0)\n",
    "        std = np.std(test_res['truth'][pert_idx][:, de_idx], axis = 0)\n",
    "        min_ = np.min(test_res['truth'][pert_idx][:, de_idx], axis = 0)\n",
    "        max_ = np.max(test_res['truth'][pert_idx][:, de_idx], axis = 0)\n",
    "        q25 = np.quantile(test_res['truth'][pert_idx][:, de_idx], 0.25, axis = 0)\n",
    "        q75 = np.quantile(test_res['truth'][pert_idx][:, de_idx], 0.75, axis = 0)\n",
    "        q55 = np.quantile(test_res['truth'][pert_idx][:, de_idx], 0.55, axis = 0)\n",
    "        q45 = np.quantile(test_res['truth'][pert_idx][:, de_idx], 0.45, axis = 0)\n",
    "        q40 = np.quantile(test_res['truth'][pert_idx][:, de_idx], 0.4, axis = 0)\n",
    "        q60 = np.quantile(test_res['truth'][pert_idx][:, de_idx], 0.6, axis = 0)\n",
    "        \n",
    "        zero_des = np.intersect1d(np.where(min_ == 0)[0], np.where(max_ == 0)[0])\n",
    "        nonzero_des = np.setdiff1d(list(range(20)), zero_des)\n",
    "        \n",
    "        if len(nonzero_des) == 0:\n",
    "            pass\n",
    "            # pert that all de genes are 0...\n",
    "        else:            \n",
    "            pred_mean = np.mean(test_res['pred'][pert_idx][:, de_idx], axis = 0).reshape(-1,)\n",
    "            true_mean = np.mean(test_res['truth'][pert_idx][:, de_idx], axis = 0).reshape(-1,)\n",
    "           \n",
    "            in_range = (pred_mean[nonzero_des] >= min_[nonzero_des]) & (pred_mean[nonzero_des] <= max_[nonzero_des])\n",
    "            frac_in_range = sum(in_range)/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_in_range_non_dropout'] = frac_in_range\n",
    "\n",
    "            in_range_5 = (pred_mean[nonzero_des] >= q45[nonzero_des]) & (pred_mean[nonzero_des] <= q55[nonzero_des])\n",
    "            frac_in_range_45_55 = sum(in_range_5)/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_in_range_45_55_non_dropout'] = frac_in_range_45_55\n",
    "\n",
    "            in_range_10 = (pred_mean[nonzero_des] >= q40[nonzero_des]) & (pred_mean[nonzero_des] <= q60[nonzero_des])\n",
    "            frac_in_range_40_60 = sum(in_range_10)/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_in_range_40_60_non_dropout'] = frac_in_range_40_60\n",
    "\n",
    "            in_range_25 = (pred_mean[nonzero_des] >= q25[nonzero_des]) & (pred_mean[nonzero_des] <= q75[nonzero_des])\n",
    "            frac_in_range_25_75 = sum(in_range_25)/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_in_range_25_75_non_dropout'] = frac_in_range_25_75\n",
    "\n",
    "            zero_idx = np.where(std > 0)[0]\n",
    "            sigma = (np.abs(pred_mean[zero_idx] - mean[zero_idx]))/(std[zero_idx])\n",
    "            pert_metric[pert]['mean_sigma_non_dropout'] = np.mean(sigma)\n",
    "            pert_metric[pert]['std_sigma_non_dropout'] = np.std(sigma)\n",
    "            pert_metric[pert]['frac_sigma_below_1_non_dropout'] = 1 - len(np.where(sigma > 1)[0])/len(zero_idx)\n",
    "            pert_metric[pert]['frac_sigma_below_2_non_dropout'] = 1 - len(np.where(sigma > 2)[0])/len(zero_idx)\n",
    "        \n",
    "        p_idx = np.where(test_res['pert_cat'] == pert)[0]\n",
    "        for m, fct in metric2fct.items():\n",
    "            if m != 'mse':\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx] - ctrl[0][de_idx], test_res['truth'][p_idx].mean(0)[de_idx]-ctrl[0][de_idx])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_delta_top20_de_non_dropout'] = val\n",
    "\n",
    "\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx], test_res['truth'][p_idx].mean(0)[de_idx])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_top20_de_non_dropout'] = val\n",
    "            else:\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx] - ctrl[0][de_idx], test_res['truth'][p_idx].mean(0)[de_idx]-ctrl[0][de_idx])\n",
    "                pert_metric[pert][m + '_top20_de_non_dropout'] = val\n",
    "                \n",
    "    return pert_metric\n",
    "    \n",
    "def deeper_analysis(adata, test_res, de_column_prefix = 'rank_genes_groups_cov', most_variable_genes = None):\n",
    "    \n",
    "    metric2fct = {\n",
    "           'pearson': pearsonr,\n",
    "           'mse': mse\n",
    "    }\n",
    "\n",
    "    pert_metric = {}\n",
    "\n",
    "    ## in silico modeling and upperbounding\n",
    "    pert2pert_full_id = dict(adata.obs[['condition', 'condition_name']].values)\n",
    "    geneid2name = dict(zip(adata.var.index.values, adata.var['gene_name']))\n",
    "    geneid2idx = dict(zip(adata.var.index.values, range(len(adata.var.index.values))))\n",
    "\n",
    "    # calculate mean expression for each condition\n",
    "    unique_conditions = adata.obs.condition.unique()\n",
    "    conditions2index = {}\n",
    "    for i in unique_conditions:\n",
    "        conditions2index[i] = np.where(adata.obs.condition == i)[0]\n",
    "\n",
    "    condition2mean_expression = {}\n",
    "    for i, j in conditions2index.items():\n",
    "        condition2mean_expression[i] = np.mean(adata.X[j], axis = 0)\n",
    "    pert_list = np.array(list(condition2mean_expression.keys()))\n",
    "    mean_expression = np.array(list(condition2mean_expression.values())).reshape(len(adata.obs.condition.unique()), adata.X.toarray().shape[1])\n",
    "    ctrl = mean_expression[np.where(pert_list == 'ctrl')[0]]\n",
    "    \n",
    "    if most_variable_genes is None:\n",
    "        most_variable_genes = np.argsort(np.std(mean_expression, axis = 0))[-200:]\n",
    "        \n",
    "    gene_list = adata.var['gene_name'].values\n",
    "\n",
    "    for pert in np.unique(test_res['pert_cat']):\n",
    "        pert_metric[pert] = {}\n",
    "        de_idx = [geneid2idx[i] for i in adata.uns['rank_genes_groups_cov_all'][pert2pert_full_id[pert]][:20]]\n",
    "        de_idx_200 = [geneid2idx[i] for i in adata.uns['rank_genes_groups_cov_all'][pert2pert_full_id[pert]][:200]]\n",
    "        de_idx_100 = [geneid2idx[i] for i in adata.uns['rank_genes_groups_cov_all'][pert2pert_full_id[pert]][:100]]\n",
    "        de_idx_50 = [geneid2idx[i] for i in adata.uns['rank_genes_groups_cov_all'][pert2pert_full_id[pert]][:50]]\n",
    "\n",
    "        pert_idx = np.where(test_res['pert_cat'] == pert)[0]    \n",
    "        pred_mean = np.mean(test_res['pred_de'][pert_idx], axis = 0).reshape(-1,)\n",
    "        true_mean = np.mean(test_res['truth_de'][pert_idx], axis = 0).reshape(-1,)\n",
    "        \n",
    "        direc_change = np.abs(np.sign(test_res['pred'][pert_idx].mean(0) - ctrl[0]) - np.sign(test_res['truth'][pert_idx].mean(0) - ctrl[0]))            \n",
    "        frac_correct_direction = len(np.where(direc_change == 0)[0])/len(geneid2name)\n",
    "        pert_metric[pert]['frac_correct_direction_all'] = frac_correct_direction\n",
    "\n",
    "        de_idx_map = {20: de_idx,\n",
    "                      50: de_idx_50,\n",
    "                      100: de_idx_100,\n",
    "                      200: de_idx_200\n",
    "                     }\n",
    "        \n",
    "        for val in [20, 50, 100, 200]:\n",
    "            \n",
    "            direc_change = np.abs(np.sign(test_res['pred'][pert_idx].mean(0)[de_idx_map[val]] - ctrl[0][de_idx_map[val]]) - np.sign(test_res['truth'][pert_idx].mean(0)[de_idx_map[val]] - ctrl[0][de_idx_map[val]]))            \n",
    "            frac_correct_direction = len(np.where(direc_change == 0)[0])/val\n",
    "            pert_metric[pert]['frac_correct_direction_' + str(val)] = frac_correct_direction\n",
    "\n",
    "        mean = np.mean(test_res['truth_de'][pert_idx], axis = 0)\n",
    "        std = np.std(test_res['truth_de'][pert_idx], axis = 0)\n",
    "        min_ = np.min(test_res['truth_de'][pert_idx], axis = 0)\n",
    "        max_ = np.max(test_res['truth_de'][pert_idx], axis = 0)\n",
    "        q25 = np.quantile(test_res['truth_de'][pert_idx], 0.25, axis = 0)\n",
    "        q75 = np.quantile(test_res['truth_de'][pert_idx], 0.75, axis = 0)\n",
    "        q55 = np.quantile(test_res['truth_de'][pert_idx], 0.55, axis = 0)\n",
    "        q45 = np.quantile(test_res['truth_de'][pert_idx], 0.45, axis = 0)\n",
    "        q40 = np.quantile(test_res['truth_de'][pert_idx], 0.4, axis = 0)\n",
    "        q60 = np.quantile(test_res['truth_de'][pert_idx], 0.6, axis = 0)\n",
    "\n",
    "        zero_des = np.intersect1d(np.where(min_ == 0)[0], np.where(max_ == 0)[0])\n",
    "        nonzero_des = np.setdiff1d(list(range(20)), zero_des)\n",
    "        if len(nonzero_des) == 0:\n",
    "            pass\n",
    "            # pert that all de genes are 0...\n",
    "        else:            \n",
    "            \n",
    "            direc_change = np.abs(np.sign(pred_mean[nonzero_des] - ctrl[0][de_idx][nonzero_des]) - np.sign(true_mean[nonzero_des] - ctrl[0][de_idx][nonzero_des]))            \n",
    "            frac_correct_direction = len(np.where(direc_change == 0)[0])/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_correct_direction_20_nonzero'] = frac_correct_direction\n",
    "            \n",
    "            in_range = (pred_mean[nonzero_des] >= min_[nonzero_des]) & (pred_mean[nonzero_des] <= max_[nonzero_des])\n",
    "            frac_in_range = sum(in_range)/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_in_range'] = frac_in_range\n",
    "\n",
    "            in_range_5 = (pred_mean[nonzero_des] >= q45[nonzero_des]) & (pred_mean[nonzero_des] <= q55[nonzero_des])\n",
    "            frac_in_range_45_55 = sum(in_range_5)/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_in_range_45_55'] = frac_in_range_45_55\n",
    "\n",
    "            in_range_10 = (pred_mean[nonzero_des] >= q40[nonzero_des]) & (pred_mean[nonzero_des] <= q60[nonzero_des])\n",
    "            frac_in_range_40_60 = sum(in_range_10)/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_in_range_40_60'] = frac_in_range_40_60\n",
    "\n",
    "            in_range_25 = (pred_mean[nonzero_des] >= q25[nonzero_des]) & (pred_mean[nonzero_des] <= q75[nonzero_des])\n",
    "            frac_in_range_25_75 = sum(in_range_25)/len(nonzero_des)\n",
    "            pert_metric[pert]['frac_in_range_25_75'] = frac_in_range_25_75\n",
    "\n",
    "            zero_idx = np.where(std > 0)[0]\n",
    "            sigma = (np.abs(pred_mean[zero_idx] - mean[zero_idx]))/(std[zero_idx])\n",
    "            pert_metric[pert]['mean_sigma'] = np.mean(sigma)\n",
    "            pert_metric[pert]['std_sigma'] = np.std(sigma)\n",
    "            pert_metric[pert]['frac_sigma_below_1'] = 1 - len(np.where(sigma > 1)[0])/len(zero_idx)\n",
    "            pert_metric[pert]['frac_sigma_below_2'] = 1 - len(np.where(sigma > 2)[0])/len(zero_idx)\n",
    "\n",
    "        ## correlation on delta\n",
    "        p_idx = np.where(test_res['pert_cat'] == pert)[0]\n",
    "\n",
    "        for m, fct in metric2fct.items():\n",
    "            if m != 'mse':\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)- ctrl[0], test_res['truth'][p_idx].mean(0)-ctrl[0])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "\n",
    "                pert_metric[pert][m + '_delta'] = val\n",
    "                \n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx] - ctrl[0][de_idx], test_res['truth'][p_idx].mean(0)[de_idx]-ctrl[0][de_idx])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "\n",
    "                pert_metric[pert][m + '_delta_de'] = val\n",
    "\n",
    "        ## up fold changes > 10?\n",
    "        pert_mean = np.mean(test_res['truth'][p_idx], axis = 0).reshape(-1,)\n",
    "\n",
    "        fold_change = pert_mean/ctrl\n",
    "        fold_change[np.isnan(fold_change)] = 0\n",
    "        fold_change[np.isinf(fold_change)] = 0\n",
    "        ## this is to remove the ones that are super low and the fold change becomes unmeaningful\n",
    "        fold_change[0][np.where(pert_mean < 0.5)[0]] = 0\n",
    "\n",
    "        o =  np.where(fold_change[0] > 0)[0]\n",
    "\n",
    "        pred_fc = test_res['pred'][p_idx].mean(0)[o]\n",
    "        true_fc = test_res['truth'][p_idx].mean(0)[o]\n",
    "        ctrl_fc = ctrl[0][o]\n",
    "\n",
    "        if len(o) > 0:\n",
    "            pert_metric[pert]['fold_change_gap_all'] = np.mean(np.abs(pred_fc/ctrl_fc - true_fc/ctrl_fc))\n",
    "\n",
    "\n",
    "        o = np.intersect1d(np.where(fold_change[0] <0.333)[0], np.where(fold_change[0] > 0)[0])\n",
    "\n",
    "        pred_fc = test_res['pred'][p_idx].mean(0)[o]\n",
    "        true_fc = test_res['truth'][p_idx].mean(0)[o]\n",
    "        ctrl_fc = ctrl[0][o]\n",
    "\n",
    "        if len(o) > 0:\n",
    "            pert_metric[pert]['fold_change_gap_downreg_0.33'] = np.mean(np.abs(pred_fc/ctrl_fc - true_fc/ctrl_fc))\n",
    "\n",
    "\n",
    "        o = np.intersect1d(np.where(fold_change[0] <0.1)[0], np.where(fold_change[0] > 0)[0])\n",
    "\n",
    "        pred_fc = test_res['pred'][p_idx].mean(0)[o]\n",
    "        true_fc = test_res['truth'][p_idx].mean(0)[o]\n",
    "        ctrl_fc = ctrl[0][o]\n",
    "\n",
    "        if len(o) > 0:\n",
    "            pert_metric[pert]['fold_change_gap_downreg_0.1'] = np.mean(np.abs(pred_fc/ctrl_fc - true_fc/ctrl_fc))\n",
    "\n",
    "        o = np.where(fold_change[0] > 3)[0]\n",
    "\n",
    "        pred_fc = test_res['pred'][p_idx].mean(0)[o]\n",
    "        true_fc = test_res['truth'][p_idx].mean(0)[o]\n",
    "        ctrl_fc = ctrl[0][o]\n",
    "\n",
    "        if len(o) > 0:\n",
    "            pert_metric[pert]['fold_change_gap_upreg_3'] = np.mean(np.abs(pred_fc/ctrl_fc - true_fc/ctrl_fc))\n",
    "\n",
    "        o = np.where(fold_change[0] > 10)[0]\n",
    "\n",
    "        pred_fc = test_res['pred'][p_idx].mean(0)[o]\n",
    "        true_fc = test_res['truth'][p_idx].mean(0)[o]\n",
    "        ctrl_fc = ctrl[0][o]\n",
    "\n",
    "        if len(o) > 0:\n",
    "            pert_metric[pert]['fold_change_gap_upreg_10'] = np.mean(np.abs(pred_fc/ctrl_fc - true_fc/ctrl_fc))\n",
    "\n",
    "        ## most variable genes\n",
    "        for m, fct in metric2fct.items():\n",
    "            if m != 'mse':\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[most_variable_genes] - ctrl[0][most_variable_genes], test_res['truth'][p_idx].mean(0)[most_variable_genes]-ctrl[0][most_variable_genes])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_delta_top200_hvg'] = val\n",
    "\n",
    "\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[most_variable_genes], test_res['truth'][p_idx].mean(0)[most_variable_genes])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_top200_hvg'] = val\n",
    "            else:\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[most_variable_genes], test_res['truth'][p_idx].mean(0)[most_variable_genes])\n",
    "                pert_metric[pert][m + '_top200_hvg'] = val\n",
    "\n",
    "\n",
    "        ## top 20/50/100/200 DEs\n",
    "        for m, fct in metric2fct.items():\n",
    "            if m != 'mse':\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx] - ctrl[0][de_idx], test_res['truth'][p_idx].mean(0)[de_idx]-ctrl[0][de_idx])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_delta_top20_de'] = val\n",
    "\n",
    "\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx], test_res['truth'][p_idx].mean(0)[de_idx])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_top20_de'] = val\n",
    "            else:\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx] - ctrl[0][de_idx], test_res['truth'][p_idx].mean(0)[de_idx]-ctrl[0][de_idx])\n",
    "                pert_metric[pert][m + '_top20_de'] = val\n",
    "\n",
    "        \n",
    "        for m, fct in metric2fct.items():\n",
    "            if m != 'mse':\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx_200] - ctrl[0][de_idx_200], test_res['truth'][p_idx].mean(0)[de_idx_200]-ctrl[0][de_idx_200])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_delta_top200_de'] = val\n",
    "\n",
    "\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx_200], test_res['truth'][p_idx].mean(0)[de_idx_200])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_top200_de'] = val\n",
    "            else:\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx_200] - ctrl[0][de_idx_200], test_res['truth'][p_idx].mean(0)[de_idx_200]-ctrl[0][de_idx_200])\n",
    "                pert_metric[pert][m + '_top200_de'] = val\n",
    "\n",
    "        for m, fct in metric2fct.items():\n",
    "            if m != 'mse':\n",
    "\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx_100] - ctrl[0][de_idx_100], test_res['truth'][p_idx].mean(0)[de_idx_100]-ctrl[0][de_idx_100])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_delta_top100_de'] = val\n",
    "\n",
    "\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx_100], test_res['truth'][p_idx].mean(0)[de_idx_100])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_top100_de'] = val\n",
    "            else:\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx_100] - ctrl[0][de_idx_100], test_res['truth'][p_idx].mean(0)[de_idx_100]-ctrl[0][de_idx_100])\n",
    "                pert_metric[pert][m + '_top100_de'] = val\n",
    "\n",
    "        for m, fct in metric2fct.items():\n",
    "            if m != 'mse':\n",
    "\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx_50] - ctrl[0][de_idx_50], test_res['truth'][p_idx].mean(0)[de_idx_50]-ctrl[0][de_idx_50])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_delta_top50_de'] = val\n",
    "\n",
    "\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx_50], test_res['truth'][p_idx].mean(0)[de_idx_50])[0]\n",
    "                if np.isnan(val):\n",
    "                    val = 0\n",
    "                pert_metric[pert][m + '_top50_de'] = val\n",
    "            else:\n",
    "                val = fct(test_res['pred'][p_idx].mean(0)[de_idx_50] - ctrl[0][de_idx_50], test_res['truth'][p_idx].mean(0)[de_idx_50]-ctrl[0][de_idx_50])\n",
    "                pert_metric[pert][m + '_top50_de'] = val\n",
    "\n",
    "\n",
    "\n",
    "    return pert_metric\n",
    "\n",
    "def GI_subgroup(pert_metric):\n",
    "    GI_type2Score = {}\n",
    "    test_pert_list = list(pert_metric.keys())\n",
    "    for GI_type, gi_list in GIs.items():\n",
    "        intersect = np.intersect1d(gi_list, test_pert_list)\n",
    "        if len(intersect) != 0:\n",
    "            GI_type2Score[GI_type] = {}\n",
    "\n",
    "            for m in list(list(pert_metric.values())[0].keys()):\n",
    "                GI_type2Score[GI_type][m] = np.mean([pert_metric[i][m] for i in intersect if m in pert_metric[i]])\n",
    "                \n",
    "    return GI_type2Score\n",
    "\n",
    "def node_specific_batch_out(models, batch):\n",
    "    # Returns output for all node specific models as a matrix of dimension batch_size x nodes\n",
    "    outs = []\n",
    "    for idx in range(len(models)):\n",
    "        outs.append(models[idx](batch).detach().cpu().numpy()[:,idx])\n",
    "    return np.vstack(outs).T\n",
    "\n",
    "def batch_predict(loader, loaded_models, args):\n",
    "    # Prediction for node specific GNNs\n",
    "    preds = []\n",
    "    print(\"Loader size: \", len(loader))\n",
    "    for itr, batch in enumerate(loader):\n",
    "        print(itr)\n",
    "        batch = batch.to(args['device'])\n",
    "        preds.append(node_specific_batch_out(loaded_models, batch))\n",
    "\n",
    "    preds = np.vstack(preds)\n",
    "    return preds\n",
    "\n",
    "def get_high_umi_idx(gene_list):\n",
    "    # Genes used for linear model fitting\n",
    "    try:\n",
    "        high_umi = np.load('../genes_with_hi_mean.npy', allow_pickle=True)\n",
    "    except:\n",
    "        high_umi = np.load('./genes_with_hi_mean.npy', allow_pickle=True)\n",
    "    high_umi_idx = np.where([g in high_umi for g in gene_list])[0]\n",
    "    return high_umi_idx\n",
    "\n",
    "def get_mean_ctrl(adata):\n",
    "    return adata[adata.obs['condition'] == 'ctrl'].to_df().mean().reset_index(\n",
    "        drop=True)\n",
    "\n",
    "def get_single_name(g, all_perts):\n",
    "    name = g+'+ctrl'\n",
    "    if name in all_perts:\n",
    "        return name\n",
    "    else:\n",
    "        return 'ctrl+'+g\n",
    "\n",
    "def get_test_set_results_seen2(res, sel_GI_type):\n",
    "    # Get relevant test set results\n",
    "    test_pert_cats = [p for p in np.unique(res['pert_cat']) if\n",
    "                      p in GIs[sel_GI_type] or 'ctrl' in p]\n",
    "    pred_idx = np.where([t in test_pert_cats for t in res['pert_cat']])\n",
    "    out = {}\n",
    "    for key in res:\n",
    "        out[key] = res[key][pred_idx]\n",
    "    return out\n",
    "\n",
    "def get_all_vectors(all_res, mean_control, double,\n",
    "                    single1, single2, high_umi_idx):\n",
    "    # Pred\n",
    "    pred_df = pd.DataFrame(all_res['pred'])\n",
    "    pred_df['condition'] = all_res['pert_cat']\n",
    "    subset_df = pred_df[pred_df['condition'] == double].iloc[:, :-1]\n",
    "    delta_double_pred = subset_df.mean(0) - mean_control\n",
    "    single_df_1_pred = pred_df[pred_df['condition'] == single1].iloc[:, :-1]\n",
    "    single_df_2_pred = pred_df[pred_df['condition'] == single2].iloc[:, :-1]\n",
    "\n",
    "    # True\n",
    "    truth_df = pd.DataFrame(all_res['truth'])\n",
    "    truth_df['condition'] = all_res['pert_cat']\n",
    "    subset_df = truth_df[truth_df['condition'] == double].iloc[:, :-1]\n",
    "    delta_double_truth = subset_df.mean(0) - mean_control\n",
    "    single_df_1_truth = truth_df[truth_df['condition'] == single1].iloc[:, :-1]\n",
    "    single_df_2_truth = truth_df[truth_df['condition'] == single2].iloc[:, :-1]\n",
    "\n",
    "    delta_single_truth_1 = single_df_1_truth.mean(0) - mean_control\n",
    "    delta_single_truth_2 = single_df_2_truth.mean(0) - mean_control\n",
    "    delta_single_pred_1 = single_df_1_pred.mean(0) - mean_control\n",
    "    delta_single_pred_2 = single_df_2_pred.mean(0) - mean_control\n",
    "\n",
    "    return {'single_pred_1': delta_single_pred_1.values[high_umi_idx],\n",
    "            'single_pred_2': delta_single_pred_2.values[high_umi_idx],\n",
    "            'double_pred': delta_double_pred.values[high_umi_idx],\n",
    "            'single_truth_1': delta_single_truth_1.values[high_umi_idx],\n",
    "            'single_truth_2': delta_single_truth_2.values[high_umi_idx],\n",
    "            'double_truth': delta_double_truth.values[high_umi_idx]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, sizes, batch_norm=True, last_layer_act=\"linear\"):\n",
    "        \"\"\"\n",
    "        Multi-layer perceptron\n",
    "        :param sizes: list of sizes of the layers\n",
    "        :param batch_norm: whether to use batch normalization\n",
    "        :param last_layer_act: activation function of the last layer\n",
    "\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for s in range(len(sizes) - 1):\n",
    "            layers = layers + [\n",
    "                torch.nn.Linear(sizes[s], sizes[s + 1]),\n",
    "                torch.nn.BatchNorm1d(sizes[s + 1])\n",
    "                if batch_norm and s < len(sizes) - 1 else None,\n",
    "                torch.nn.ReLU()\n",
    "            ]\n",
    "\n",
    "        layers = [l for l in layers if l is not None][:-1]\n",
    "        self.activation = last_layer_act\n",
    "        self.network = torch.nn.Sequential(*layers)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class GEARS_Model(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    GEARS model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        :param args: arguments dictionary\n",
    "        \"\"\"\n",
    "\n",
    "        super(GEARS_Model, self).__init__()\n",
    "        self.args = args       \n",
    "        self.num_genes = args['num_genes']\n",
    "        self.num_perts = args['num_perts']\n",
    "        hidden_size = args['hidden_size']\n",
    "        self.uncertainty = args['uncertainty']\n",
    "        self.num_layers = args['num_go_gnn_layers']\n",
    "        self.indv_out_hidden_size = args['decoder_hidden_size']\n",
    "        self.num_layers_gene_pos = args['num_gene_gnn_layers']\n",
    "        self.no_perturb = args['no_perturb']\n",
    "        self.pert_emb_lambda = 0.2\n",
    "        \n",
    "        # perturbation positional embedding added only to the perturbed genes\n",
    "        self.pert_w = nn.Linear(1, hidden_size)\n",
    "           \n",
    "        # gene/globel perturbation embedding dictionary lookup\n",
    "        ## each gene has its own embedding .            \n",
    "        self.gene_emb = nn.Embedding(self.num_genes, hidden_size, max_norm=True)\n",
    "        ## each perturbation has its own embedding  \n",
    "        self.pert_emb = nn.Embedding(self.num_perts, hidden_size, max_norm=True)\n",
    "        \n",
    "        # transformation layer\n",
    "        self.emb_trans = nn.ReLU()\n",
    "        self.pert_base_trans = nn.ReLU()\n",
    "        self.transform = nn.ReLU()\n",
    "        \n",
    "        self.emb_trans_v2 = MLP([hidden_size, hidden_size, hidden_size], last_layer_act='ReLU')\n",
    "        self.pert_fuse = MLP([hidden_size, hidden_size, hidden_size], last_layer_act='ReLU')\n",
    "        \n",
    "        # gene co-expression GNN\n",
    "        self.G_coexpress = args['G_coexpress'].to(args['device'])\n",
    "        self.G_coexpress_weight = args['G_coexpress_weight'].to(args['device'])\n",
    "\n",
    "        self.emb_pos = nn.Embedding(self.num_genes, hidden_size, max_norm=True)\n",
    "        self.layers_emb_pos = torch.nn.ModuleList()\n",
    "        for i in range(1, self.num_layers_gene_pos + 1):\n",
    "            ## graph convolutional layers.\n",
    "            self.layers_emb_pos.append(SGConv(hidden_size, hidden_size, 1))\n",
    "        \n",
    "        ### perturbation gene ontology GNN\n",
    "        self.G_sim = args['G_go'].to(args['device'])\n",
    "        self.G_sim_weight = args['G_go_weight'].to(args['device'])\n",
    "\n",
    "        self.sim_layers = torch.nn.ModuleList()\n",
    "        for i in range(1, self.num_layers + 1):\n",
    "            self.sim_layers.append(SGConv(hidden_size, hidden_size, 1))\n",
    "        \n",
    "        # decoder shared MLP\n",
    "        self.recovery_w = MLP([hidden_size, hidden_size*2, hidden_size], last_layer_act='linear')\n",
    "        \n",
    "        # gene specific decoder\n",
    "        self.indv_w1 = nn.Parameter(torch.rand(self.num_genes,\n",
    "                                               hidden_size, 1))\n",
    "        self.indv_b1 = nn.Parameter(torch.rand(self.num_genes, 1))\n",
    "        self.act = nn.ReLU()\n",
    "        nn.init.xavier_normal_(self.indv_w1)\n",
    "        nn.init.xavier_normal_(self.indv_b1)\n",
    "        \n",
    "        # Cross gene MLP\n",
    "        self.cross_gene_state = MLP([self.num_genes, hidden_size,\n",
    "                                     hidden_size])\n",
    "        # final gene specific decoder\n",
    "        self.indv_w2 = nn.Parameter(torch.rand(1, self.num_genes,\n",
    "                                           hidden_size+1))\n",
    "        self.indv_b2 = nn.Parameter(torch.rand(1, self.num_genes))\n",
    "        nn.init.xavier_normal_(self.indv_w2)\n",
    "        nn.init.xavier_normal_(self.indv_b2)\n",
    "        \n",
    "        # batchnorms\n",
    "        self.bn_emb = nn.BatchNorm1d(hidden_size)\n",
    "        self.bn_pert_base = nn.BatchNorm1d(hidden_size)\n",
    "        self.bn_pert_base_trans = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # uncertainty mode\n",
    "        if self.uncertainty:\n",
    "            self.uncertainty_w = MLP([hidden_size, hidden_size*2, hidden_size, 1], last_layer_act='linear')\n",
    "        \n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        \"\"\"\n",
    "        x, pert_idx = data.x, data.pert_idx\n",
    "        if self.no_perturb:\n",
    "            out = x.reshape(-1,1)\n",
    "            out = torch.split(torch.flatten(out), self.num_genes)           \n",
    "            return torch.stack(out)\n",
    "        else:\n",
    "            num_graphs = len(data.batch.unique()) # each cell has its own graph\n",
    "            ## get base gene embeddings, num_batch of the same graph.\n",
    "            emb = self.gene_emb(torch.LongTensor(list(range(self.num_genes))).repeat(num_graphs, ).to(self.args['device'])) \n",
    "            emb = self.bn_emb(emb)\n",
    "            base_emb = self.emb_trans(emb)        \n",
    "\n",
    "            ## positional embeddings to differentiate each cell's embedding\n",
    "            pos_emb = self.emb_pos(torch.LongTensor(list(range(self.num_genes))).repeat(num_graphs, ).to(self.args['device']))\n",
    "            for idx, layer in enumerate(self.layers_emb_pos):\n",
    "                # pass in the positional embegginfs through a gcn with the co-expression graph.\n",
    "                pos_emb = layer(pos_emb, self.G_coexpress, self.G_coexpress_weight)\n",
    "                if idx < len(self.layers_emb_pos) - 1:\n",
    "                    # relu till the last layer.\n",
    "                    pos_emb = pos_emb.relu()\n",
    "\n",
    "            # combine base embeddings and positional embeddings\n",
    "            base_emb = base_emb + 0.2 * pos_emb\n",
    "            # pass embeddings for each cell through an mlp\n",
    "            base_emb = self.emb_trans_v2(base_emb)\n",
    "\n",
    "            ## get perturbation index and embeddings\n",
    "\n",
    "            pert_index = []\n",
    "            for idx, i in enumerate(pert_idx):\n",
    "                for j in i:\n",
    "                    if j != -1:\n",
    "                        ## idx indicates which cell, j corresponds to the perturbation number.\n",
    "                        pert_index.append([idx, j])\n",
    "            pert_index = torch.tensor(pert_index).T\n",
    "            ## perturbation embeddings for total number of perturbations to be considered.\n",
    "            pert_global_emb = self.pert_emb(torch.LongTensor(list(range(self.num_perts))).to(self.args['device']))        \n",
    "\n",
    "            ## augment global perturbation embedding with GNN\n",
    "            for idx, layer in enumerate(self.sim_layers):\n",
    "                # GCN with Perturbation graph constructed via Gene-Ontology Network\n",
    "                pert_global_emb = layer(pert_global_emb, self.G_sim, self.G_sim_weight)\n",
    "                if idx < self.num_layers - 1:\n",
    "                    pert_global_emb = pert_global_emb.relu()\n",
    "\n",
    "            ## add global perturbation embedding to each gene in each cell in the batch\n",
    "            base_emb = base_emb.reshape(num_graphs, self.num_genes, -1)\n",
    "\n",
    "            if pert_index.shape[0] != 0:\n",
    "                ### in case all samples in the batch are controls, then there is no indexing for pert_index.\n",
    "                pert_track = {}\n",
    "                for i, j in enumerate(pert_index[0]):\n",
    "                    if j.item() in pert_track:\n",
    "                        pert_track[j.item()] = pert_track[j.item()] + pert_global_emb[pert_index[1][i]]\n",
    "                    else:\n",
    "                        pert_track[j.item()] = pert_global_emb[pert_index[1][i]]\n",
    "\n",
    "                if len(list(pert_track.values())) > 0:\n",
    "                    if len(list(pert_track.values())) == 1:\n",
    "                        # circumvent when batch size = 1 with single perturbation and cannot feed into MLP\n",
    "                        emb_total = self.pert_fuse(torch.stack(list(pert_track.values()) * 2))\n",
    "                    else:\n",
    "                        emb_total = self.pert_fuse(torch.stack(list(pert_track.values())))\n",
    "\n",
    "                    for idx, j in enumerate(pert_track.keys()):\n",
    "                        base_emb[j] = base_emb[j] + emb_total[idx]\n",
    "\n",
    "            base_emb = base_emb.reshape(num_graphs * self.num_genes, -1)\n",
    "            base_emb = self.bn_pert_base(base_emb)\n",
    "\n",
    "            ## apply the first MLP\n",
    "            base_emb = self.transform(base_emb)        \n",
    "            out = self.recovery_w(base_emb)\n",
    "            out = out.reshape(num_graphs, self.num_genes, -1)\n",
    "            out = out.unsqueeze(-1) * self.indv_w1\n",
    "            w = torch.sum(out, axis = 2)\n",
    "            out = w + self.indv_b1\n",
    "\n",
    "            # Cross gene\n",
    "            cross_gene_embed = self.cross_gene_state(out.reshape(num_graphs, self.num_genes, -1).squeeze(2))\n",
    "            cross_gene_embed = cross_gene_embed.repeat(1, self.num_genes)\n",
    "\n",
    "            cross_gene_embed = cross_gene_embed.reshape([num_graphs,self.num_genes, -1])\n",
    "            cross_gene_out = torch.cat([out, cross_gene_embed], 2)\n",
    "\n",
    "            cross_gene_out = cross_gene_out * self.indv_w2\n",
    "            cross_gene_out = torch.sum(cross_gene_out, axis=2)\n",
    "            out = cross_gene_out + self.indv_b2        \n",
    "            out = out.reshape(num_graphs * self.num_genes, -1) + x.reshape(-1,1)\n",
    "            out = torch.split(torch.flatten(out), self.num_genes)\n",
    "\n",
    "            ## uncertainty head\n",
    "            if self.uncertainty:\n",
    "                out_logvar = self.uncertainty_w(base_emb)\n",
    "                out_logvar = torch.split(torch.flatten(out_logvar), self.num_genes)\n",
    "                return torch.stack(out), torch.stack(out_logvar)\n",
    "            \n",
    "            return torch.stack(out)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Have the expression as an embedding thats added to the base embedding of each cell as opposed to perturbation embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "class GEARS_EMBED(GEARS_Model):\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        self.expression_projection = nn.Linear(1,args[\"hidden_size\"])\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        \"\"\"\n",
    "        x, pert_idx = data.x, data.pert_idx\n",
    "        if self.no_perturb:\n",
    "            out = x.reshape(-1,1)\n",
    "            out = torch.split(torch.flatten(out), self.num_genes)           \n",
    "            return torch.stack(out)\n",
    "        else:\n",
    "            num_graphs = len(data.batch.unique()) # each cell has its own graph\n",
    "            ## get base gene embeddings, num_batch of the same graph.\n",
    "            emb = self.gene_emb(torch.LongTensor(list(range(self.num_genes))).repeat(num_graphs, ).to(self.args['device']))\n",
    "            emb = self.bn_emb(emb)\n",
    "            base_emb = self.emb_trans(emb)        \n",
    "            \n",
    "             # ────── 2) per‑gene mean‑centering + projection ────────────────\n",
    "            # reshape x → (B, G, 1)\n",
    "            x_cells = x.view(num_graphs, self.num_genes, 1)\n",
    "            means   = x_cells.mean(dim=0, keepdim=True)  # (1, G, 1)\n",
    "            centered= x_cells - means                    # (B, G, 1)\n",
    "            # back to (B*G, 1) for linear\n",
    "            centered = centered.view(-1, 1)\n",
    "            expr_emb = self.expression_projection(centered)  # (B*G, H)\n",
    "            # fuse expression offset into base\n",
    "            # print(f\"shapes of base embedding {base_emb.shape} \")\n",
    "            # print(f\"shapes of expression embedding {expr_emb}\")\n",
    "            base_emb = base_emb + expr_emb\n",
    "            # positional embeddings to differentiate each cell's embedding\n",
    "            pos_emb = self.emb_pos(torch.LongTensor(list(range(self.num_genes))).repeat(num_graphs, ).to(self.args['device']))\n",
    "            for idx, layer in enumerate(self.layers_emb_pos):\n",
    "                # pass in the positional embegginfs through a gcn with the co-expression graph.\n",
    "                pos_emb = layer(pos_emb, self.G_coexpress, self.G_coexpress_weight)\n",
    "                if idx < len(self.layers_emb_pos) - 1:\n",
    "                    # relu till the last layer.\n",
    "                    pos_emb = pos_emb.relu()\n",
    "                    \n",
    "            # combine base embeddings and positional embeddings\n",
    "            base_emb = base_emb  + 0.2 * pos_emb\n",
    "            # pass embeddings for each cell through an mlp\n",
    "            base_emb = self.emb_trans_v2(base_emb)\n",
    "\n",
    "            ## get perturbation index and embeddings\n",
    "\n",
    "            pert_index = []\n",
    "            for idx, i in enumerate(pert_idx):\n",
    "                for j in i:\n",
    "                    if j != -1:\n",
    "                        ## idx indicates which cell, j corresponds to the perturbation number.\n",
    "                        pert_index.append([idx, j])\n",
    "            pert_index = torch.tensor(pert_index).T\n",
    "            ## perturbation embeddings for total number of perturbations to be considered.\n",
    "            pert_global_emb = self.pert_emb(torch.LongTensor(list(range(self.num_perts))).to(self.args['device']))        \n",
    "\n",
    "            ## augment global perturbation embedding with GNN\n",
    "            for idx, layer in enumerate(self.sim_layers):\n",
    "                # GCN with Perturbation graph constructed via Gene-Ontology Network\n",
    "                pert_global_emb = layer(pert_global_emb, self.G_sim, self.G_sim_weight)\n",
    "                if idx < self.num_layers - 1:\n",
    "                    pert_global_emb = pert_global_emb.relu()\n",
    "\n",
    "            ## add global perturbation embedding to each gene in each cell in the batch\n",
    "            base_emb = base_emb.reshape(num_graphs, self.num_genes, -1)\n",
    "\n",
    "            if pert_index.shape[0] != 0:\n",
    "                ### in case all samples in the batch are controls, then there is no indexing for pert_index.\n",
    "                pert_track = {}\n",
    "                for i, j in enumerate(pert_index[0]):\n",
    "                    if j.item() in pert_track:\n",
    "                        pert_track[j.item()] = pert_track[j.item()] + pert_global_emb[pert_index[1][i]]\n",
    "                    else:\n",
    "                        pert_track[j.item()] = pert_global_emb[pert_index[1][i]]\n",
    "\n",
    "                if len(list(pert_track.values())) > 0:\n",
    "                    if len(list(pert_track.values())) == 1:\n",
    "                        # circumvent when batch size = 1 with single perturbation and cannot feed into MLP\n",
    "                        emb_total = self.pert_fuse(torch.stack(list(pert_track.values()) * 2))\n",
    "                    else:\n",
    "                        emb_total = self.pert_fuse(torch.stack(list(pert_track.values())))\n",
    "\n",
    "                    for idx, j in enumerate(pert_track.keys()):\n",
    "                        base_emb[j] = base_emb[j] + emb_total[idx]\n",
    "\n",
    "            base_emb = base_emb.reshape(num_graphs * self.num_genes, -1)\n",
    "            base_emb = self.bn_pert_base(base_emb)\n",
    "\n",
    "            ## apply the first MLP\n",
    "            base_emb = self.transform(base_emb)        \n",
    "            out = self.recovery_w(base_emb)\n",
    "            out = out.reshape(num_graphs, self.num_genes, -1)\n",
    "            out = out.unsqueeze(-1) * self.indv_w1\n",
    "            w = torch.sum(out, axis = 2)\n",
    "            out = w + self.indv_b1\n",
    "\n",
    "            # Cross gene\n",
    "            cross_gene_embed = self.cross_gene_state(out.reshape(num_graphs, self.num_genes, -1).squeeze(2))\n",
    "            cross_gene_embed = cross_gene_embed.repeat(1, self.num_genes)\n",
    "\n",
    "            cross_gene_embed = cross_gene_embed.reshape([num_graphs,self.num_genes, -1])\n",
    "            cross_gene_out = torch.cat([out, cross_gene_embed], 2)\n",
    "\n",
    "            cross_gene_out = cross_gene_out * self.indv_w2\n",
    "            cross_gene_out = torch.sum(cross_gene_out, axis=2)\n",
    "            out = cross_gene_out + self.indv_b2        \n",
    "            out = out.reshape(num_graphs * self.num_genes, -1) + x.reshape(-1,1)\n",
    "            out = torch.split(torch.flatten(out), self.num_genes)\n",
    "\n",
    "            ## uncertainty head\n",
    "            if self.uncertainty:\n",
    "                out_logvar = self.uncertainty_w(base_emb)\n",
    "                out_logvar = torch.split(torch.flatten(out_logvar), self.num_genes)\n",
    "                return torch.stack(out), torch.stack(out_logvar)\n",
    "            \n",
    "            return torch.stack(out)\n",
    "  \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Graph Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class GEARS_GAT(GEARS_Model):\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        \n",
    "        # GAT layers for co-expression GNN\n",
    "        self.layers_emb_pos = torch.nn.ModuleList()\n",
    "        for i in range(1, self.num_layers_gene_pos + 1):\n",
    "            self.layers_emb_pos.append(GATConv(args['hidden_size'], args['hidden_size'], heads=1))\n",
    "            \n",
    "        # GAT layers for perturbation similarity GNN\n",
    "        self.sim_layers = torch.nn.ModuleList()\n",
    "        for i in range(1, self.num_layers + 1):\n",
    "            self.sim_layers.append(GATConv(args['hidden_size'], args['hidden_size'], heads=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. TransformerConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import TransformerConv\n",
    "\n",
    "class GEARS_Transformer(GEARS_Model):\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        \n",
    "        # Transformer layers for co-expression GNN\n",
    "        self.layers_emb_pos = torch.nn.ModuleList()\n",
    "        for i in range(1, self.num_layers_gene_pos + 1):\n",
    "            self.layers_emb_pos.append(TransformerConv(args['hidden_size'], args['hidden_size'], heads=1))\n",
    "            \n",
    "        # Transformer layers for perturbation similarity GNN\n",
    "        self.sim_layers = torch.nn.ModuleList()\n",
    "        for i in range(1, self.num_layers + 1):\n",
    "            self.sim_layers.append(TransformerConv(args['hidden_size'], args['hidden_size'], heads=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. No gene-coexpression graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GEARS_No_Coexpress(GEARS_Model):\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        self.layers_emb_pos = torch.nn.ModuleList() # Empty module list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. No perturbation Coexpression Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GEARS_No_Perturb(GEARS_Model):\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        self.sim_layers = torch.nn.ModuleList()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gears api\n",
    "class GEARS:\n",
    "    \"\"\"\n",
    "    GEARS base model class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pert_data, \n",
    "                 device = 'cuda',\n",
    "                 weight_bias_track = False, \n",
    "                 proj_name = 'GEARS', \n",
    "                 exp_name = 'GEARS'):\n",
    "        \"\"\"\n",
    "        Initialize GEARS model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pert_data: PertData object\n",
    "            dataloader for perturbation data\n",
    "        device: str\n",
    "            Device to run the model on. Default: 'cuda'\n",
    "        weight_bias_track: bool\n",
    "            Whether to track performance on wandb. Default: False\n",
    "        proj_name: str\n",
    "            Project name for wandb. Default: 'GEARS'\n",
    "        exp_name: str\n",
    "            Experiment name for wandb. Default: 'GEARS'\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.weight_bias_track = weight_bias_track\n",
    "        \n",
    "        if self.weight_bias_track:\n",
    "            import wandb\n",
    "            wandb.init(project=proj_name, name=exp_name)  \n",
    "            self.wandb = wandb\n",
    "        else:\n",
    "            self.wandb = None\n",
    "        \n",
    "        self.device = device\n",
    "        self.config = None\n",
    "        \n",
    "        self.dataloader = pert_data.dataloader ## \n",
    "        self.adata = pert_data.adata\n",
    "        self.node_map = pert_data.node_map\n",
    "        self.node_map_pert = pert_data.node_map_pert\n",
    "        self.data_path = pert_data.data_path\n",
    "        self.dataset_name = pert_data.dataset_name\n",
    "        self.split = pert_data.split\n",
    "        self.seed = pert_data.seed\n",
    "        self.train_gene_set_size = pert_data.train_gene_set_size\n",
    "        self.set2conditions = pert_data.set2conditions\n",
    "        self.subgroup = pert_data.subgroup\n",
    "        self.gene_list = pert_data.gene_names.values.tolist()\n",
    "        self.pert_list = pert_data.pert_names.tolist()\n",
    "        self.num_genes = len(self.gene_list)\n",
    "        self.num_perts = len(self.pert_list)\n",
    "        self.default_pert_graph = pert_data.default_pert_graph\n",
    "        self.saved_pred = {}\n",
    "        self.saved_logvar_sum = {}\n",
    "        \n",
    "        self.ctrl_expression = torch.tensor(\n",
    "            np.mean(self.adata.X[self.adata.obs.condition.values == 'ctrl'],\n",
    "                    axis=0)).reshape(-1, ).to(self.device)\n",
    "        pert_full_id2pert = dict(self.adata.obs[['condition_name', 'condition']].values)\n",
    "        self.dict_filter = {pert_full_id2pert[i]: j for i, j in\n",
    "                            self.adata.uns['non_zeros_gene_idx'].items() if\n",
    "                            i in pert_full_id2pert}\n",
    "        self.ctrl_adata = self.adata[self.adata.obs['condition'] == 'ctrl']\n",
    "        \n",
    "        gene_dict = {g:i for i,g in enumerate(self.gene_list)}\n",
    "        self.pert2gene = {p: gene_dict[pert] for p, pert in\n",
    "                          enumerate(self.pert_list) if pert in self.gene_list}\n",
    "\n",
    "\n",
    "    def tunable_parameters(self):\n",
    "        \"\"\"\n",
    "        Return the tunable parameters of the model\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Tunable parameters of the model\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return {'hidden_size': 'hidden dimension, default 64',\n",
    "                'num_go_gnn_layers': 'number of GNN layers for GO graph, default 1',\n",
    "                'num_gene_gnn_layers': 'number of GNN layers for co-expression gene graph, default 1',\n",
    "                'decoder_hidden_size': 'hidden dimension for gene-specific decoder, default 16',\n",
    "                'num_similar_genes_go_graph': 'number of maximum similar K genes in the GO graph, default 20',\n",
    "                'num_similar_genes_co_express_graph': 'number of maximum similar K genes in the co expression graph, default 20',\n",
    "                'coexpress_threshold': 'pearson correlation threshold when constructing coexpression graph, default 0.4',\n",
    "                'uncertainty': 'whether or not to turn on uncertainty mode, default False',\n",
    "                'uncertainty_reg': 'regularization term to balance uncertainty loss and prediction loss, default 1',\n",
    "                'direction_lambda': 'regularization term to balance direction loss and prediction loss, default 1'\n",
    "               }\n",
    "    \n",
    "    def model_initialize(self, hidden_size = 64,\n",
    "                         num_go_gnn_layers = 1, \n",
    "                         num_gene_gnn_layers = 1,\n",
    "                         decoder_hidden_size = 16,\n",
    "                         num_similar_genes_go_graph = 20,\n",
    "                         num_similar_genes_co_express_graph = 20,                    \n",
    "                         coexpress_threshold = 0.4,\n",
    "                         uncertainty = False, \n",
    "                         uncertainty_reg = 1,\n",
    "                         direction_lambda = 1e-1,\n",
    "                         G_go = None,\n",
    "                         G_go_weight = None,\n",
    "                         G_coexpress = None,\n",
    "                         G_coexpress_weight = None,\n",
    "                         no_perturb = False,\n",
    "                         gears_model=0, \n",
    "                         **kwargs\n",
    "                        ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hidden_size: int\n",
    "            hidden dimension, default 64\n",
    "        num_go_gnn_layers: int\n",
    "            number of GNN layers for GO graph, default 1\n",
    "        num_gene_gnn_layers: int\n",
    "            number of GNN layers for co-expression gene graph, default 1\n",
    "        decoder_hidden_size: int\n",
    "            hidden dimension for gene-specific decoder, default 16\n",
    "        num_similar_genes_go_graph: int\n",
    "            number of maximum similar K genes in the GO graph, default 20\n",
    "        num_similar_genes_co_express_graph: int\n",
    "            number of maximum similar K genes in the co expression graph, default 20\n",
    "        coexpress_threshold: float\n",
    "            pearson correlation threshold when constructing coexpression graph, default 0.4\n",
    "        uncertainty: bool\n",
    "            whether or not to turn on uncertainty mode, default False\n",
    "        uncertainty_reg: float\n",
    "            regularization term to balance uncertainty loss and prediction loss, default 1\n",
    "        direction_lambda: float\n",
    "            regularization term to balance direction loss and prediction loss, default 1\n",
    "        G_go: scipy.sparse.csr_matrix\n",
    "            GO graph, default None\n",
    "        G_go_weight: scipy.sparse.csr_matrix\n",
    "            GO graph edge weights, default None\n",
    "        G_coexpress: scipy.sparse.csr_matrix\n",
    "            co-expression graph, default None\n",
    "        G_coexpress_weight: scipy.sparse.csr_matrix\n",
    "            co-expression graph edge weights, default None\n",
    "        no_perturb: bool\n",
    "            predict no perturbation condition, default False\n",
    "        gears_model: int \n",
    "            0- original model, 1- expression embedding, 2 - GAT, 3 - TransformerConv, 4- No Coexpression 5- No perturbation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        \n",
    "        self.config = {'hidden_size': hidden_size,\n",
    "                       'num_go_gnn_layers' : num_go_gnn_layers, \n",
    "                       'num_gene_gnn_layers' : num_gene_gnn_layers,\n",
    "                       'decoder_hidden_size' : decoder_hidden_size,\n",
    "                       'num_similar_genes_go_graph' : num_similar_genes_go_graph,\n",
    "                       'num_similar_genes_co_express_graph' : num_similar_genes_co_express_graph,\n",
    "                       'coexpress_threshold': coexpress_threshold,\n",
    "                       'uncertainty' : uncertainty, \n",
    "                       'uncertainty_reg' : uncertainty_reg,\n",
    "                       'direction_lambda' : direction_lambda,\n",
    "                       'G_go': G_go,\n",
    "                       'G_go_weight': G_go_weight,\n",
    "                       'G_coexpress': G_coexpress,\n",
    "                       'G_coexpress_weight': G_coexpress_weight,\n",
    "                       'device': self.device,\n",
    "                       'num_genes': self.num_genes,\n",
    "                       'num_perts': self.num_perts,\n",
    "                       'no_perturb': no_perturb,\n",
    "                       'gears_model': gears_model,\n",
    "                      }\n",
    "        \n",
    "        if self.wandb:\n",
    "            self.wandb.config.update(self.config)\n",
    "        \n",
    "        if self.config['G_coexpress'] is None:\n",
    "            ## calculating co expression similarity graph\n",
    "            edge_list = get_similarity_network(network_type='co-express',\n",
    "                                               adata=self.adata,\n",
    "                                               threshold=coexpress_threshold,\n",
    "                                               k=num_similar_genes_co_express_graph,\n",
    "                                               data_path=self.data_path,\n",
    "                                               data_name=self.dataset_name,\n",
    "                                               split=self.split, seed=self.seed,\n",
    "                                               train_gene_set_size=self.train_gene_set_size,\n",
    "                                               set2conditions=self.set2conditions)\n",
    "\n",
    "            sim_network = GeneSimNetwork(edge_list, self.gene_list, node_map = self.node_map)\n",
    "            self.config['G_coexpress'] = sim_network.edge_index\n",
    "            self.config['G_coexpress_weight'] = sim_network.edge_weight\n",
    "        \n",
    "        if self.config['G_go'] is None:\n",
    "            ## calculating gene ontology similarity graph\n",
    "            edge_list = get_similarity_network(network_type='go',\n",
    "                                               adata=self.adata,\n",
    "                                               threshold=coexpress_threshold,\n",
    "                                               k=num_similar_genes_go_graph,\n",
    "                                               pert_list=self.pert_list,\n",
    "                                               data_path=self.data_path,\n",
    "                                               data_name=self.dataset_name,\n",
    "                                               split=self.split, seed=self.seed,\n",
    "                                               train_gene_set_size=self.train_gene_set_size,\n",
    "                                               set2conditions=self.set2conditions,\n",
    "                                               default_pert_graph=self.default_pert_graph)\n",
    "\n",
    "            sim_network = GeneSimNetwork(edge_list, self.pert_list, node_map = self.node_map_pert)\n",
    "            self.config['G_go'] = sim_network.edge_index\n",
    "            self.config['G_go_weight'] = sim_network.edge_weight\n",
    "            \n",
    "        if self.config[\"gears_model\"] == 0 :\n",
    "            self.model = GEARS_Model(self.config).to(self.device)\n",
    "        elif self.config[\"gears_model\"] == 1:\n",
    "            self.model = GEARS_EMBED(self.config).to(self.device)\n",
    "        elif self.config[\"gears_model\"] == 2:\n",
    "            self.model = GEARS_GAT(self.config).to(self.device)\n",
    "        elif self.config[\"gears_model\"] == 3:\n",
    "            self.model = GEARS_Transformer(self.config).to(self.device)\n",
    "        elif self.config[\"gears_model\"] == 4:\n",
    "            self.model = GEARS_No_Coexpress(self.config).to(self.device)\n",
    "        elif self.config[\"gears_model\"] == 5:\n",
    "            self.model = GEARS_No_Perturb(self.config).to(self.device)\n",
    "            \n",
    "            \n",
    "        self.best_model = deepcopy(self.model)\n",
    "        \n",
    "    def load_pretrained(self, path):\n",
    "        \"\"\"\n",
    "        Load pretrained model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path: str\n",
    "            path to the pretrained model\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        with open(os.path.join(path, 'config.pkl'), 'rb') as f:\n",
    "            config = pickle.load(f)\n",
    "        \n",
    "        del config['device'], config['num_genes'], config['num_perts']\n",
    "        self.model_initialize(**config)\n",
    "        self.config = config\n",
    "        \n",
    "        state_dict = torch.load(os.path.join(path, 'model.pt'), map_location = torch.device('cpu'))\n",
    "        if next(iter(state_dict))[:7] == 'module.':\n",
    "            # the pretrained model is from data-parallel module\n",
    "            from collections import OrderedDict\n",
    "            new_state_dict = OrderedDict()\n",
    "            for k, v in state_dict.items():\n",
    "                name = k[7:] # remove `module.`\n",
    "                new_state_dict[name] = v\n",
    "            state_dict = new_state_dict\n",
    "        \n",
    "        self.model.load_state_dict(state_dict)\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.best_model = self.model\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        \"\"\"\n",
    "        Save the model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path: str\n",
    "            path to save the model\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        \n",
    "        if self.config is None:\n",
    "            raise ValueError('No model is initialized...')\n",
    "        \n",
    "        with open(os.path.join(path, 'config.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.config, f)\n",
    "       \n",
    "        torch.save(self.best_model.state_dict(), os.path.join(path, 'model.pt'))\n",
    "    \n",
    "    def predict(self, pert_list):\n",
    "        \"\"\"\n",
    "        Predict the transcriptome given a list of genes/gene combinations being\n",
    "        perturbed\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pert_list: list\n",
    "            list of genes/gene combiantions to be perturbed\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        results_pred: dict\n",
    "            dictionary of predicted transcriptome\n",
    "        results_logvar: dict\n",
    "            dictionary of uncertainty score\n",
    "\n",
    "        \"\"\"\n",
    "        ## given a list of single/combo genes, return the transcriptome\n",
    "        ## if uncertainty mode is on, also return uncertainty score.\n",
    "        \n",
    "        self.ctrl_adata = self.adata[self.adata.obs['condition'] == 'ctrl']\n",
    "        for pert in pert_list:\n",
    "            for i in pert:\n",
    "                if i not in self.pert_list:\n",
    "                    raise ValueError(i+ \" is not in the perturbation graph. \"\n",
    "                                        \"Please select from GEARS.pert_list!\")\n",
    "        \n",
    "        if self.config['uncertainty']:\n",
    "            results_logvar = {}\n",
    "            \n",
    "        self.best_model = self.best_model.to(self.device)\n",
    "        self.best_model.eval()\n",
    "        results_pred = {}\n",
    "        results_logvar_sum = {}\n",
    "        \n",
    "        from torch_geometric.data import DataLoader\n",
    "        for pert in pert_list:\n",
    "            try:\n",
    "                #If prediction is already saved, then skip inference\n",
    "                results_pred['_'.join(pert)] = self.saved_pred['_'.join(pert)]\n",
    "                if self.config['uncertainty']:\n",
    "                    results_logvar_sum['_'.join(pert)] = self.saved_logvar_sum['_'.join(pert)]\n",
    "                continue\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            cg = create_cell_graph_dataset_for_prediction(pert, self.ctrl_adata,\n",
    "                                                    self.pert_list, self.device)\n",
    "            loader = DataLoader(cg, 300, shuffle = False)\n",
    "            batch = next(iter(loader))\n",
    "            batch.to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if self.config['uncertainty']:\n",
    "                    p, unc = self.best_model(batch)\n",
    "                    results_logvar['_'.join(pert)] = np.mean(unc.detach().cpu().numpy(), axis = 0)\n",
    "                    results_logvar_sum['_'.join(pert)] = np.exp(-np.mean(results_logvar['_'.join(pert)]))\n",
    "                else:\n",
    "                    p = self.best_model(batch)\n",
    "                    \n",
    "            results_pred['_'.join(pert)] = np.mean(p.detach().cpu().numpy(), axis = 0)\n",
    "                \n",
    "        self.saved_pred.update(results_pred)\n",
    "        \n",
    "        if self.config['uncertainty']:\n",
    "            self.saved_logvar_sum.update(results_logvar_sum)\n",
    "            return results_pred, results_logvar_sum\n",
    "        else:\n",
    "            return results_pred\n",
    "        \n",
    "    def GI_predict(self, combo, GI_genes_file='./genes_with_hi_mean.npy'):\n",
    "        \"\"\"\n",
    "        Predict the GI scores following perturbation of a given gene combination\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        combo: list\n",
    "            list of genes to be perturbed\n",
    "        GI_genes_file: str\n",
    "            path to the file containing genes with high mean expression\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        GI scores for the given combinatorial perturbation based on GEARS\n",
    "        predictions\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        ## if uncertainty mode is on, also return uncertainty score.\n",
    "        try:\n",
    "            # If prediction is already saved, then skip inference\n",
    "            pred = {}\n",
    "            pred[combo[0]] = self.saved_pred[combo[0]]\n",
    "            pred[combo[1]] = self.saved_pred[combo[1]]\n",
    "            pred['_'.join(combo)] = self.saved_pred['_'.join(combo)]\n",
    "        except:\n",
    "            if self.config['uncertainty']:\n",
    "                pred = self.predict([[combo[0]], [combo[1]], combo])[0]\n",
    "            else:\n",
    "                pred = self.predict([[combo[0]], [combo[1]], combo])\n",
    "\n",
    "        mean_control = get_mean_control(self.adata).values  \n",
    "        pred = {p:pred[p]-mean_control for p in pred} \n",
    "\n",
    "        if GI_genes_file is not None:\n",
    "            # If focussing on a specific subset of genes for calculating metrics\n",
    "            GI_genes_idx = get_GI_genes_idx(self.adata, GI_genes_file)       \n",
    "        else:\n",
    "            GI_genes_idx = np.arange(len(self.adata.var.gene_name.values))\n",
    "            \n",
    "        pred = {p:pred[p][GI_genes_idx] for p in pred}\n",
    "        return get_GI_params(pred, combo)\n",
    "    \n",
    "    def plot_perturbation(self, query, save_file = None):\n",
    "        \"\"\"\n",
    "        Plot the perturbation graph\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query: str\n",
    "            condition to be queried\n",
    "        save_file: str\n",
    "            path to save the plot\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        import seaborn as sns\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        sns.set_theme(style=\"ticks\", rc={\"axes.facecolor\": (0, 0, 0, 0)}, font_scale=1.5)\n",
    "\n",
    "        adata = self.adata\n",
    "        gene2idx = self.node_map\n",
    "        cond2name = dict(adata.obs[['condition', 'condition_name']].values)\n",
    "        gene_raw2id = dict(zip(adata.var.index.values, adata.var.gene_name.values))\n",
    "\n",
    "        de_idx = [gene2idx[gene_raw2id[i]] for i in\n",
    "                  adata.uns['top_non_dropout_de_20'][cond2name[query]]]\n",
    "        genes = [gene_raw2id[i] for i in\n",
    "                 adata.uns['top_non_dropout_de_20'][cond2name[query]]]\n",
    "        truth = adata[adata.obs.condition == query].X.toarray()[:, de_idx]\n",
    "        \n",
    "        query_ = [q for q in query.split('+') if q != 'ctrl']\n",
    "        pred = self.predict([query_])['_'.join(query_)][de_idx]\n",
    "        ctrl_means = adata[adata.obs['condition'] == 'ctrl'].to_df().mean()[\n",
    "            de_idx].values\n",
    "\n",
    "        pred = pred - ctrl_means\n",
    "        truth = truth - ctrl_means\n",
    "        \n",
    "        plt.figure(figsize=[16.5,4.5])\n",
    "        plt.title(query)\n",
    "        plt.boxplot(truth, showfliers=False,\n",
    "                    medianprops = dict(linewidth=0))    \n",
    "\n",
    "        for i in range(pred.shape[0]):\n",
    "            _ = plt.scatter(i+1, pred[i], color='red')\n",
    "\n",
    "        plt.axhline(0, linestyle=\"dashed\", color = 'green')\n",
    "\n",
    "        ax = plt.gca()\n",
    "        ax.xaxis.set_ticklabels(genes, rotation = 90)\n",
    "\n",
    "        plt.ylabel(\"Change in Gene Expression over Control\",labelpad=10)\n",
    "        plt.tick_params(axis='x', which='major', pad=5)\n",
    "        plt.tick_params(axis='y', which='major', pad=5)\n",
    "        sns.despine()\n",
    "        \n",
    "        if save_file:\n",
    "            plt.savefig(save_file, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    def train(self, epochs = 20, \n",
    "              lr = 1e-3,\n",
    "              weight_decay = 5e-4\n",
    "             ):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epochs: int\n",
    "            number of epochs to train\n",
    "        lr: float\n",
    "            learning rate\n",
    "        weight_decay: float\n",
    "            weight decay\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        train_loader = self.dataloader['train_loader']\n",
    "        val_loader = self.dataloader['val_loader']\n",
    "            \n",
    "        self.model = self.model.to(self.device)\n",
    "        best_model = deepcopy(self.model)\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "        scheduler = StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "\n",
    "        min_val = np.inf\n",
    "        print_sys('Start Training...')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "\n",
    "            for step, batch in enumerate(train_loader):\n",
    "                batch.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                y = batch.y\n",
    "                if self.config['uncertainty']:\n",
    "                    pred, logvar = self.model(batch)\n",
    "                    loss = uncertainty_loss_fct(pred, logvar, y, batch.pert,\n",
    "                                      reg = self.config['uncertainty_reg'],\n",
    "                                      ctrl = self.ctrl_expression, \n",
    "                                      dict_filter = self.dict_filter,\n",
    "                                      direction_lambda = self.config['direction_lambda'])\n",
    "                else:\n",
    "                    pred = self.model(batch)\n",
    "                    loss = loss_fct(pred, y, batch.pert,\n",
    "                                  ctrl = self.ctrl_expression, \n",
    "                                  dict_filter = self.dict_filter,\n",
    "                                  direction_lambda = self.config['direction_lambda'])\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_value_(self.model.parameters(), clip_value=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                if self.wandb:\n",
    "                    self.wandb.log({'training_loss': loss.item()})\n",
    "\n",
    "                if step % 50 == 0:\n",
    "                    log = \"Epoch {} Step {} Train Loss: {:.4f}\" \n",
    "                    print_sys(log.format(epoch + 1, step + 1, loss.item()))\n",
    "\n",
    "            scheduler.step()\n",
    "            # Evaluate model performance on train and val set\n",
    "            train_res = evaluate(train_loader, self.model,\n",
    "                                 self.config['uncertainty'], self.device)\n",
    "            val_res = evaluate(val_loader, self.model,\n",
    "                                 self.config['uncertainty'], self.device)\n",
    "            \n",
    "            train_metrics, _ = compute_metrics(train_res)\n",
    "            val_metrics, _ = compute_metrics(val_res)\n",
    "\n",
    "            # Print epoch performance\n",
    "            log = \"Epoch {}: Train Overall MSE: {:.4f} \" \\\n",
    "                  \"Validation Overall MSE: {:.4f}. \"\n",
    "            print_sys(log.format(epoch + 1, train_metrics['mse'], \n",
    "                             val_metrics['mse']))\n",
    "            \n",
    "            # Print epoch performance for DE genes\n",
    "            log = \"Train Top 20 DE MSE: {:.4f} \" \\\n",
    "                  \"Validation Top 20 DE MSE: {:.4f}. \"\n",
    "            print_sys(log.format(train_metrics['mse_de'],\n",
    "                             val_metrics['mse_de']))\n",
    "            \n",
    "            if self.wandb:\n",
    "                metrics = ['mse', 'pearson']\n",
    "                for m in metrics:\n",
    "                    self.wandb.log({'train_' + m: train_metrics[m],\n",
    "                               'val_'+m: val_metrics[m],\n",
    "                               'train_de_' + m: train_metrics[m + '_de'],\n",
    "                               'val_de_'+m: val_metrics[m + '_de']})\n",
    "               \n",
    "            if val_metrics['mse_de'] < min_val:\n",
    "                min_val = val_metrics['mse_de']\n",
    "                best_model = deepcopy(self.model)\n",
    "                \n",
    "        print_sys(\"Done!\")\n",
    "        self.best_model = best_model\n",
    "\n",
    "        if 'test_loader' not in self.dataloader:\n",
    "            print_sys('Done! No test dataloader detected.')\n",
    "            return\n",
    "            \n",
    "        # Model testing\n",
    "        test_loader = self.dataloader['test_loader']\n",
    "        print_sys(\"Start Testing...\")\n",
    "        test_res = evaluate(test_loader, self.best_model,\n",
    "                            self.config['uncertainty'], self.device)\n",
    "        test_metrics, test_pert_res = compute_metrics(test_res)    \n",
    "        log = \"Best performing model: Test Top 20 DE MSE: {:.4f}\"\n",
    "        print_sys(log.format(test_metrics['mse_de']))\n",
    "        \n",
    "        if self.wandb:\n",
    "            metrics = ['mse', 'pearson']\n",
    "            for m in metrics:\n",
    "                self.wandb.log({'test_' + m: test_metrics[m],\n",
    "                           'test_de_'+m: test_metrics[m + '_de']                     \n",
    "                          })\n",
    "                \n",
    "        out = deeper_analysis(self.adata, test_res)\n",
    "        out_non_dropout = non_dropout_analysis(self.adata, test_res)\n",
    "        \n",
    "        metrics = ['pearson_delta']\n",
    "        metrics_non_dropout = ['frac_opposite_direction_top20_non_dropout',\n",
    "                               'frac_sigma_below_1_non_dropout',\n",
    "                               'mse_top20_de_non_dropout']\n",
    "        \n",
    "        if self.wandb:\n",
    "            for m in metrics:\n",
    "                self.wandb.log({'test_' + m: np.mean([j[m] for i,j in out.items() if m in j])})\n",
    "\n",
    "            for m in metrics_non_dropout:\n",
    "                self.wandb.log({'test_' + m: np.mean([j[m] for i,j in out_non_dropout.items() if m in j])})        \n",
    "\n",
    "        if self.split == 'simulation':\n",
    "            print_sys(\"Start doing subgroup analysis for simulation split...\")\n",
    "            subgroup = self.subgroup\n",
    "            subgroup_analysis = {}\n",
    "            for name in subgroup['test_subgroup'].keys():\n",
    "                subgroup_analysis[name] = {}\n",
    "                for m in list(list(test_pert_res.values())[0].keys()):\n",
    "                    subgroup_analysis[name][m] = []\n",
    "\n",
    "            for name, pert_list in subgroup['test_subgroup'].items():\n",
    "                for pert in pert_list:\n",
    "                    for m, res in test_pert_res[pert].items():\n",
    "                        subgroup_analysis[name][m].append(res)\n",
    "\n",
    "            for name, result in subgroup_analysis.items():\n",
    "                for m in result.keys():\n",
    "                    subgroup_analysis[name][m] = np.mean(subgroup_analysis[name][m])\n",
    "                    if self.wandb:\n",
    "                        self.wandb.log({'test_' + name + '_' + m: subgroup_analysis[name][m]})\n",
    "\n",
    "                    print_sys('test_' + name + '_' + m + ': ' + str(subgroup_analysis[name][m]))\n",
    "\n",
    "            ## deeper analysis\n",
    "            subgroup_analysis = {}\n",
    "            for name in subgroup['test_subgroup'].keys():\n",
    "                subgroup_analysis[name] = {}\n",
    "                for m in metrics:\n",
    "                    subgroup_analysis[name][m] = []\n",
    "\n",
    "                for m in metrics_non_dropout:\n",
    "                    subgroup_analysis[name][m] = []\n",
    "\n",
    "            for name, pert_list in subgroup['test_subgroup'].items():\n",
    "                for pert in pert_list:\n",
    "                    for m in metrics:\n",
    "                        subgroup_analysis[name][m].append(out[pert][m])\n",
    "\n",
    "                    for m in metrics_non_dropout:\n",
    "                        subgroup_analysis[name][m].append(out_non_dropout[pert][m])\n",
    "\n",
    "            for name, result in subgroup_analysis.items():\n",
    "                for m in result.keys():\n",
    "                    subgroup_analysis[name][m] = np.mean(subgroup_analysis[name][m])\n",
    "                    if self.wandb:\n",
    "                        self.wandb.log({'test_' + name + '_' + m: subgroup_analysis[name][m]})\n",
    "\n",
    "                    print_sys('test_' + name + '_' + m + ': ' + str(subgroup_analysis[name][m]))\n",
    "        print_sys('Done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DataLoading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Found local copy...\n",
      "Found local copy...\n",
      "These perturbations are not in the GO graph and their perturbation can thus not be predicted\n",
      "['RHOXF2BB+ctrl' 'LYL1+IER5L' 'ctrl+IER5L' 'KIAA1804+ctrl' 'IER5L+ctrl'\n",
      " 'RHOXF2BB+ZBTB25' 'RHOXF2BB+SET']\n",
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of split is detected. Loading...\n",
      "Simulation split test composition:\n",
      "combo_seen0:9\n",
      "combo_seen1:43\n",
      "combo_seen2:19\n",
      "unseen_single:36\n",
      "Done!\n",
      "Creating dataloaders....\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n"
     ]
    }
   ],
   "source": [
    "pert_data = PertData('./data')\n",
    "pert_data.load(data_name = 'norman')\n",
    "pert_data.prepare_split(split = 'simulation', seed = 1)\n",
    "pert_data.get_dataloader(batch_size = 32, test_batch_size = 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    }
   ],
   "source": [
    "gears_model_original = GEARS(pert_data, device = 'cpu', \n",
    "                        weight_bias_track = False, \n",
    "                        proj_name = 'pertnet', \n",
    "                        exp_name = 'pertnet')\n",
    "gears_model_original.model_initialize(hidden_size = 64,gears_model=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    }
   ],
   "source": [
    "gears_model_exprembedding = GEARS(pert_data, device = 'cpu', \n",
    "                        weight_bias_track = False, \n",
    "                        proj_name = 'pertnet', \n",
    "                        exp_name = 'pertnet')\n",
    "gears_model_exprembedding.model_initialize(hidden_size = 64,gears_model=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    }
   ],
   "source": [
    "gears_model_gat = GEARS(pert_data, device = 'cpu', \n",
    "                        weight_bias_track = False, \n",
    "                        proj_name = 'pertnet', \n",
    "                        exp_name = 'pertnet')\n",
    "gears_model_gat.model_initialize(hidden_size = 64,gears_model=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    }
   ],
   "source": [
    "gears_model_transformer = GEARS(pert_data, device = 'cpu', \n",
    "                        weight_bias_track = False, \n",
    "                        proj_name = 'pertnet', \n",
    "                        exp_name = 'pertnet')\n",
    "gears_model_transformer.model_initialize(hidden_size = 64,gears_model=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    }
   ],
   "source": [
    "gears_model_no_coexpress = GEARS(pert_data, device = 'cpu', \n",
    "                        weight_bias_track = False, \n",
    "                        proj_name = 'pertnet', \n",
    "                        exp_name = 'pertnet')\n",
    "gears_model_no_coexpress.model_initialize(hidden_size = 64,gears_model=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    }
   ],
   "source": [
    "gears_model_no_perturb = GEARS(pert_data, device = 'cpu', \n",
    "                        weight_bias_track = False, \n",
    "                        proj_name = 'pertnet', \n",
    "                        exp_name = 'pertnet')\n",
    "gears_model_no_perturb.model_initialize(hidden_size = 64,gears_model=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "Epoch 1 Step 1 Train Loss: 0.4549\n",
      "Epoch 1 Step 51 Train Loss: 0.4682\n",
      "Epoch 1 Step 101 Train Loss: 0.4308\n",
      "Epoch 1 Step 151 Train Loss: 0.5396\n",
      "Epoch 1 Step 201 Train Loss: 0.5218\n",
      "Epoch 1 Step 251 Train Loss: 0.5099\n",
      "Epoch 1 Step 301 Train Loss: 0.5208\n",
      "Epoch 1 Step 351 Train Loss: 0.5326\n",
      "Epoch 1 Step 401 Train Loss: 0.4655\n",
      "Epoch 1 Step 451 Train Loss: 0.5449\n",
      "Epoch 1 Step 501 Train Loss: 0.3963\n",
      "Epoch 1 Step 551 Train Loss: 0.4877\n",
      "Epoch 1 Step 601 Train Loss: 0.5946\n",
      "Epoch 1 Step 651 Train Loss: 0.4576\n",
      "Epoch 1 Step 701 Train Loss: 0.4583\n",
      "Epoch 1 Step 751 Train Loss: 0.4761\n",
      "Epoch 1 Step 801 Train Loss: 0.4404\n",
      "Epoch 1 Step 851 Train Loss: 0.4724\n",
      "Epoch 1 Step 901 Train Loss: 0.4676\n",
      "Epoch 1 Step 951 Train Loss: 0.4829\n",
      "Epoch 1 Step 1001 Train Loss: 0.4930\n",
      "Epoch 1 Step 1051 Train Loss: 0.3945\n",
      "Epoch 1 Step 1101 Train Loss: 0.4691\n",
      "Epoch 1 Step 1151 Train Loss: 0.4559\n",
      "Epoch 1 Step 1201 Train Loss: 0.4626\n",
      "Epoch 1 Step 1251 Train Loss: 0.5016\n",
      "Epoch 1 Step 1301 Train Loss: 0.4807\n",
      "Epoch 1 Step 1351 Train Loss: 0.4216\n",
      "Epoch 1 Step 1401 Train Loss: 0.4943\n",
      "Epoch 1 Step 1451 Train Loss: 0.4643\n",
      "Epoch 1 Step 1501 Train Loss: 0.4834\n",
      "Epoch 1 Step 1551 Train Loss: 0.4732\n",
      "Epoch 1: Train Overall MSE: 0.0138 Validation Overall MSE: 0.0141. \n",
      "Train Top 20 DE MSE: 0.2498 Validation Top 20 DE MSE: 0.4944. \n",
      "Epoch 2 Step 1 Train Loss: 0.5124\n",
      "Epoch 2 Step 51 Train Loss: 0.4817\n",
      "Epoch 2 Step 101 Train Loss: 0.5149\n",
      "Epoch 2 Step 151 Train Loss: 0.5251\n",
      "Epoch 2 Step 201 Train Loss: 0.5158\n",
      "Epoch 2 Step 251 Train Loss: 0.4217\n",
      "Epoch 2 Step 301 Train Loss: 0.4704\n",
      "Epoch 2 Step 351 Train Loss: 0.4694\n",
      "Epoch 2 Step 401 Train Loss: 0.4427\n",
      "Epoch 2 Step 451 Train Loss: 0.4801\n",
      "Epoch 2 Step 501 Train Loss: 0.5639\n",
      "Epoch 2 Step 551 Train Loss: 0.4626\n",
      "Epoch 2 Step 601 Train Loss: 0.5093\n",
      "Epoch 2 Step 651 Train Loss: 0.5637\n",
      "Epoch 2 Step 701 Train Loss: 0.4563\n",
      "Epoch 2 Step 751 Train Loss: 0.4870\n",
      "Epoch 2 Step 801 Train Loss: 0.5238\n",
      "Epoch 2 Step 851 Train Loss: 0.5961\n",
      "Epoch 2 Step 901 Train Loss: 0.4827\n",
      "Epoch 2 Step 951 Train Loss: 0.4965\n",
      "Epoch 2 Step 1001 Train Loss: 0.4983\n",
      "Epoch 2 Step 1051 Train Loss: 0.4984\n",
      "Epoch 2 Step 1101 Train Loss: 0.4689\n",
      "Epoch 2 Step 1151 Train Loss: 0.4461\n",
      "Epoch 2 Step 1201 Train Loss: 0.4689\n",
      "Epoch 2 Step 1251 Train Loss: 0.4490\n",
      "Epoch 2 Step 1301 Train Loss: 0.4965\n",
      "Epoch 2 Step 1351 Train Loss: 0.4340\n",
      "Epoch 2 Step 1401 Train Loss: 0.4250\n",
      "Epoch 2 Step 1451 Train Loss: 0.6028\n",
      "Epoch 2 Step 1501 Train Loss: 0.5149\n",
      "Epoch 2 Step 1551 Train Loss: 0.5116\n",
      "Epoch 2: Train Overall MSE: 0.0076 Validation Overall MSE: 0.0082. \n",
      "Train Top 20 DE MSE: 0.1395 Validation Top 20 DE MSE: 0.3306. \n",
      "Epoch 3 Step 1 Train Loss: 0.4242\n",
      "Epoch 3 Step 51 Train Loss: 0.4677\n",
      "Epoch 3 Step 101 Train Loss: 0.5161\n",
      "Epoch 3 Step 151 Train Loss: 0.4672\n",
      "Epoch 3 Step 201 Train Loss: 0.4756\n",
      "Epoch 3 Step 251 Train Loss: 0.5490\n",
      "Epoch 3 Step 301 Train Loss: 0.5186\n",
      "Epoch 3 Step 351 Train Loss: 0.4896\n",
      "Epoch 3 Step 401 Train Loss: 0.5021\n",
      "Epoch 3 Step 451 Train Loss: 0.4888\n",
      "Epoch 3 Step 501 Train Loss: 0.4552\n",
      "Epoch 3 Step 551 Train Loss: 0.5151\n",
      "Epoch 3 Step 601 Train Loss: 0.5568\n",
      "Epoch 3 Step 651 Train Loss: 0.5697\n",
      "Epoch 3 Step 701 Train Loss: 0.4721\n",
      "Epoch 3 Step 751 Train Loss: 0.4614\n",
      "Epoch 3 Step 801 Train Loss: 0.4811\n",
      "Epoch 3 Step 851 Train Loss: 0.5493\n",
      "Epoch 3 Step 901 Train Loss: 0.5346\n",
      "Epoch 3 Step 951 Train Loss: 0.5016\n",
      "Epoch 3 Step 1001 Train Loss: 0.5108\n",
      "Epoch 3 Step 1051 Train Loss: 0.5666\n",
      "Epoch 3 Step 1101 Train Loss: 0.5081\n",
      "Epoch 3 Step 1151 Train Loss: 0.4577\n",
      "Epoch 3 Step 1201 Train Loss: 0.4270\n",
      "Epoch 3 Step 1251 Train Loss: 0.5555\n",
      "Epoch 3 Step 1301 Train Loss: 0.4645\n",
      "Epoch 3 Step 1351 Train Loss: 0.5285\n",
      "Epoch 3 Step 1401 Train Loss: 0.4780\n",
      "Epoch 3 Step 1451 Train Loss: 0.4346\n",
      "Epoch 3 Step 1501 Train Loss: 0.5989\n",
      "Epoch 3 Step 1551 Train Loss: 0.5266\n",
      "Epoch 3: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0033. \n",
      "Train Top 20 DE MSE: 0.0880 Validation Top 20 DE MSE: 0.2409. \n",
      "Epoch 4 Step 1 Train Loss: 0.5339\n",
      "Epoch 4 Step 51 Train Loss: 0.4828\n",
      "Epoch 4 Step 101 Train Loss: 0.5090\n",
      "Epoch 4 Step 151 Train Loss: 0.4690\n",
      "Epoch 4 Step 201 Train Loss: 0.4431\n",
      "Epoch 4 Step 251 Train Loss: 0.5183\n",
      "Epoch 4 Step 301 Train Loss: 0.5146\n",
      "Epoch 4 Step 351 Train Loss: 0.5421\n",
      "Epoch 4 Step 401 Train Loss: 0.4682\n",
      "Epoch 4 Step 451 Train Loss: 0.4543\n",
      "Epoch 4 Step 501 Train Loss: 0.5793\n",
      "Epoch 4 Step 551 Train Loss: 0.4585\n",
      "Epoch 4 Step 601 Train Loss: 0.5308\n",
      "Epoch 4 Step 651 Train Loss: 0.4834\n",
      "Epoch 4 Step 701 Train Loss: 0.4731\n",
      "Epoch 4 Step 751 Train Loss: 0.4542\n",
      "Epoch 4 Step 801 Train Loss: 0.5077\n",
      "Epoch 4 Step 851 Train Loss: 0.4424\n",
      "Epoch 4 Step 901 Train Loss: 0.5252\n",
      "Epoch 4 Step 951 Train Loss: 0.5096\n",
      "Epoch 4 Step 1001 Train Loss: 0.4880\n",
      "Epoch 4 Step 1051 Train Loss: 0.4762\n",
      "Epoch 4 Step 1101 Train Loss: 0.5241\n",
      "Epoch 4 Step 1151 Train Loss: 0.5414\n",
      "Epoch 4 Step 1201 Train Loss: 0.5579\n",
      "Epoch 4 Step 1251 Train Loss: 0.5192\n",
      "Epoch 4 Step 1301 Train Loss: 0.4776\n",
      "Epoch 4 Step 1351 Train Loss: 0.4568\n",
      "Epoch 4 Step 1401 Train Loss: 0.5581\n",
      "Epoch 4 Step 1451 Train Loss: 0.5554\n",
      "Epoch 4 Step 1501 Train Loss: 0.6485\n",
      "Epoch 4 Step 1551 Train Loss: 0.4935\n",
      "Epoch 4: Train Overall MSE: 0.0028 Validation Overall MSE: 0.0039. \n",
      "Train Top 20 DE MSE: 0.0703 Validation Top 20 DE MSE: 0.1979. \n",
      "Done!\n",
      "Start Testing...\n",
      "Best performing model: Test Top 20 DE MSE: 0.1639\n",
      "Start doing subgroup analysis for simulation split...\n",
      "test_combo_seen0_mse: 0.005301675474685099\n",
      "test_combo_seen0_pearson: 0.7515807681613498\n",
      "test_combo_seen0_mse_de: 0.10349490731540653\n",
      "test_combo_seen0_pearson_de: 0.7515807681613498\n",
      "test_combo_seen1_mse: 0.006102753992647279\n",
      "test_combo_seen1_pearson: 0.8721250489700673\n",
      "test_combo_seen1_mse_de: 0.158836483717138\n",
      "test_combo_seen1_pearson_de: 0.8721250489700673\n",
      "test_combo_seen2_mse: 0.0057914224208185545\n",
      "test_combo_seen2_pearson: 0.9330695\n",
      "test_combo_seen2_mse_de: 0.15137752361203494\n",
      "test_combo_seen2_pearson_de: 0.9330695\n",
      "test_unseen_single_mse: 0.003130711126788002\n",
      "test_unseen_single_pearson: 0.8761091364754571\n",
      "test_unseen_single_mse_de: 0.19152039023336126\n",
      "test_unseen_single_pearson_de: 0.8761091364754571\n",
      "test_combo_seen0_pearson_delta: 0.67746043\n",
      "test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.09444444444444446\n",
      "test_combo_seen0_frac_sigma_below_1_non_dropout: 0.8944444444444445\n",
      "test_combo_seen0_mse_top20_de_non_dropout: 0.16358716040849686\n",
      "test_combo_seen1_pearson_delta: 0.6130863\n",
      "test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.12906976744186047\n",
      "test_combo_seen1_frac_sigma_below_1_non_dropout: 0.8325581395348838\n",
      "test_combo_seen1_mse_top20_de_non_dropout: 0.20803059187046316\n",
      "test_combo_seen2_pearson_delta: 0.6385222\n",
      "test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.07894736842105264\n",
      "test_combo_seen2_frac_sigma_below_1_non_dropout: 0.8789473684210526\n",
      "test_combo_seen2_mse_top20_de_non_dropout: 0.16224532417560877\n",
      "test_unseen_single_pearson_delta: 0.48910305\n",
      "test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.19861111111111115\n",
      "test_unseen_single_frac_sigma_below_1_non_dropout: 0.8652777777777777\n",
      "test_unseen_single_mse_top20_de_non_dropout: 0.20949580520391464\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "gears_model_original.train(epochs=4,lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Expression Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "Epoch 1 Step 1 Train Loss: 0.4909\n",
      "Epoch 1 Step 51 Train Loss: 0.4037\n",
      "Epoch 1 Step 101 Train Loss: 0.4006\n",
      "Epoch 1 Step 151 Train Loss: 0.2994\n",
      "Epoch 1 Step 201 Train Loss: 0.3457\n",
      "Epoch 1 Step 251 Train Loss: 0.2968\n",
      "Epoch 1 Step 301 Train Loss: 0.3123\n",
      "Epoch 1 Step 351 Train Loss: 0.3165\n",
      "Epoch 1 Step 401 Train Loss: 0.3432\n",
      "Epoch 1 Step 451 Train Loss: 0.3269\n",
      "Epoch 1 Step 501 Train Loss: 0.3431\n",
      "Epoch 1 Step 551 Train Loss: 0.3437\n",
      "Epoch 1 Step 601 Train Loss: 0.4599\n",
      "Epoch 1 Step 651 Train Loss: 0.3443\n",
      "Epoch 1 Step 701 Train Loss: 0.3729\n",
      "Epoch 1 Step 751 Train Loss: 0.3302\n",
      "Epoch 1 Step 801 Train Loss: 0.3857\n",
      "Epoch 1 Step 851 Train Loss: 0.3322\n",
      "Epoch 1 Step 901 Train Loss: 0.3192\n",
      "Epoch 1 Step 951 Train Loss: 0.3385\n",
      "Epoch 1 Step 1001 Train Loss: 0.3644\n",
      "Epoch 1 Step 1051 Train Loss: 0.3353\n",
      "Epoch 1 Step 1101 Train Loss: 0.3161\n",
      "Epoch 1 Step 1151 Train Loss: 0.3553\n",
      "Epoch 1 Step 1201 Train Loss: 0.3255\n",
      "Epoch 1 Step 1251 Train Loss: 0.3606\n",
      "Epoch 1 Step 1301 Train Loss: 0.3324\n",
      "Epoch 1 Step 1351 Train Loss: 0.3882\n",
      "Epoch 1 Step 1401 Train Loss: 0.3303\n",
      "Epoch 1 Step 1451 Train Loss: 0.3512\n",
      "Epoch 1 Step 1501 Train Loss: 0.3736\n",
      "Epoch 1 Step 1551 Train Loss: 0.3531\n",
      "Epoch 1: Train Overall MSE: 0.0055 Validation Overall MSE: 0.0064. \n",
      "Train Top 20 DE MSE: 0.1381 Validation Top 20 DE MSE: 0.2466. \n",
      "Epoch 2 Step 1 Train Loss: 0.3686\n",
      "Epoch 2 Step 51 Train Loss: 0.4125\n",
      "Epoch 2 Step 101 Train Loss: 0.3515\n",
      "Epoch 2 Step 151 Train Loss: 0.3791\n",
      "Epoch 2 Step 201 Train Loss: 0.3649\n",
      "Epoch 2 Step 251 Train Loss: 0.4119\n",
      "Epoch 2 Step 301 Train Loss: 0.3877\n",
      "Epoch 2 Step 351 Train Loss: 0.3610\n",
      "Epoch 2 Step 401 Train Loss: 0.3784\n",
      "Epoch 2 Step 451 Train Loss: 0.3745\n",
      "Epoch 2 Step 501 Train Loss: 0.4198\n",
      "Epoch 2 Step 551 Train Loss: 0.3544\n",
      "Epoch 2 Step 601 Train Loss: 0.3875\n",
      "Epoch 2 Step 651 Train Loss: 0.3765\n",
      "Epoch 2 Step 701 Train Loss: 0.4221\n",
      "Epoch 2 Step 751 Train Loss: 0.3576\n",
      "Epoch 2 Step 801 Train Loss: 0.3854\n",
      "Epoch 2 Step 851 Train Loss: 0.4268\n",
      "Epoch 2 Step 901 Train Loss: 0.3998\n",
      "Epoch 2 Step 951 Train Loss: 0.3935\n",
      "Epoch 2 Step 1001 Train Loss: 0.3845\n",
      "Epoch 2 Step 1051 Train Loss: 0.3703\n",
      "Epoch 2 Step 1101 Train Loss: 0.3526\n",
      "Epoch 2 Step 1151 Train Loss: 0.3957\n",
      "Epoch 2 Step 1201 Train Loss: 0.3950\n",
      "Epoch 2 Step 1251 Train Loss: 0.3820\n",
      "Epoch 2 Step 1301 Train Loss: 0.3962\n",
      "Epoch 2 Step 1351 Train Loss: 0.3980\n",
      "Epoch 2 Step 1401 Train Loss: 0.3685\n",
      "Epoch 2 Step 1451 Train Loss: 0.3864\n",
      "Epoch 2 Step 1501 Train Loss: 0.3590\n",
      "Epoch 2 Step 1551 Train Loss: 0.3468\n",
      "Epoch 2: Train Overall MSE: 0.0063 Validation Overall MSE: 0.0070. \n",
      "Train Top 20 DE MSE: 0.1183 Validation Top 20 DE MSE: 0.2325. \n",
      "Epoch 3 Step 1 Train Loss: 0.3686\n",
      "Epoch 3 Step 51 Train Loss: 0.4114\n",
      "Epoch 3 Step 101 Train Loss: 0.4162\n",
      "Epoch 3 Step 151 Train Loss: 0.4134\n",
      "Epoch 3 Step 201 Train Loss: 0.3841\n",
      "Epoch 3 Step 251 Train Loss: 0.4369\n",
      "Epoch 3 Step 301 Train Loss: 0.3507\n",
      "Epoch 3 Step 351 Train Loss: 0.3958\n",
      "Epoch 3 Step 401 Train Loss: 0.4283\n",
      "Epoch 3 Step 451 Train Loss: 0.3718\n",
      "Epoch 3 Step 501 Train Loss: 0.3665\n",
      "Epoch 3 Step 551 Train Loss: 0.3602\n",
      "Epoch 3 Step 601 Train Loss: 0.3631\n",
      "Epoch 3 Step 651 Train Loss: 0.4026\n",
      "Epoch 3 Step 701 Train Loss: 0.4234\n",
      "Epoch 3 Step 751 Train Loss: 0.3841\n",
      "Epoch 3 Step 801 Train Loss: 0.3949\n",
      "Epoch 3 Step 851 Train Loss: 0.3680\n",
      "Epoch 3 Step 901 Train Loss: 0.4196\n",
      "Epoch 3 Step 951 Train Loss: 0.3918\n",
      "Epoch 3 Step 1001 Train Loss: 0.3789\n",
      "Epoch 3 Step 1051 Train Loss: 0.4100\n",
      "Epoch 3 Step 1101 Train Loss: 0.3695\n",
      "Epoch 3 Step 1151 Train Loss: 0.4273\n",
      "Epoch 3 Step 1201 Train Loss: 0.4215\n",
      "Epoch 3 Step 1251 Train Loss: 0.4058\n",
      "Epoch 3 Step 1301 Train Loss: 0.3681\n",
      "Epoch 3 Step 1351 Train Loss: 0.4114\n",
      "Epoch 3 Step 1401 Train Loss: 0.3977\n",
      "Epoch 3 Step 1451 Train Loss: 0.4048\n",
      "Epoch 3 Step 1501 Train Loss: 0.3731\n",
      "Epoch 3 Step 1551 Train Loss: 0.3784\n",
      "Epoch 3: Train Overall MSE: 0.0097 Validation Overall MSE: 0.0117. \n",
      "Train Top 20 DE MSE: 0.1125 Validation Top 20 DE MSE: 0.2606. \n",
      "Epoch 4 Step 1 Train Loss: 0.3949\n",
      "Epoch 4 Step 51 Train Loss: 0.3735\n",
      "Epoch 4 Step 101 Train Loss: 0.3752\n",
      "Epoch 4 Step 151 Train Loss: 0.3729\n",
      "Epoch 4 Step 201 Train Loss: 0.3683\n",
      "Epoch 4 Step 251 Train Loss: 0.3925\n",
      "Epoch 4 Step 301 Train Loss: 0.4089\n",
      "Epoch 4 Step 351 Train Loss: 0.3710\n",
      "Epoch 4 Step 401 Train Loss: 0.3917\n",
      "Epoch 4 Step 451 Train Loss: 0.3879\n",
      "Epoch 4 Step 501 Train Loss: 0.3732\n",
      "Epoch 4 Step 551 Train Loss: 0.4462\n",
      "Epoch 4 Step 601 Train Loss: 0.3790\n",
      "Epoch 4 Step 651 Train Loss: 0.3907\n",
      "Epoch 4 Step 701 Train Loss: 0.3690\n",
      "Epoch 4 Step 751 Train Loss: 0.3746\n",
      "Epoch 4 Step 801 Train Loss: 0.3786\n",
      "Epoch 4 Step 851 Train Loss: 0.4556\n",
      "Epoch 4 Step 901 Train Loss: 0.3823\n",
      "Epoch 4 Step 951 Train Loss: 0.4281\n",
      "Epoch 4 Step 1001 Train Loss: 0.4098\n",
      "Epoch 4 Step 1051 Train Loss: 0.3924\n",
      "Epoch 4 Step 1101 Train Loss: 0.3774\n",
      "Epoch 4 Step 1151 Train Loss: 0.3900\n",
      "Epoch 4 Step 1201 Train Loss: 0.4105\n",
      "Epoch 4 Step 1251 Train Loss: 0.3792\n",
      "Epoch 4 Step 1301 Train Loss: 0.3933\n",
      "Epoch 4 Step 1351 Train Loss: 0.3713\n",
      "Epoch 4 Step 1401 Train Loss: 0.4761\n",
      "Epoch 4 Step 1451 Train Loss: 0.3951\n",
      "Epoch 4 Step 1501 Train Loss: 0.3967\n",
      "Epoch 4 Step 1551 Train Loss: 0.4161\n",
      "Epoch 4: Train Overall MSE: 0.0086 Validation Overall MSE: 0.0100. \n",
      "Train Top 20 DE MSE: 0.0993 Validation Top 20 DE MSE: 0.2480. \n",
      "Done!\n",
      "Start Testing...\n",
      "Best performing model: Test Top 20 DE MSE: 0.2054\n",
      "Start doing subgroup analysis for simulation split...\n",
      "test_combo_seen0_mse: 0.007993941143569019\n",
      "test_combo_seen0_pearson: 0.7475532624456618\n",
      "test_combo_seen0_mse_de: 0.13371550685001743\n",
      "test_combo_seen0_pearson_de: 0.7475532624456618\n",
      "test_combo_seen1_mse: 0.009673392632976174\n",
      "test_combo_seen1_pearson: 0.8604299481524977\n",
      "test_combo_seen1_mse_de: 0.21673324468090785\n",
      "test_combo_seen1_pearson_de: 0.8604299481524977\n",
      "test_combo_seen2_mse: 0.007266392857816659\n",
      "test_combo_seen2_pearson: 0.8963535\n",
      "test_combo_seen2_mse_de: 0.25416840926596995\n",
      "test_combo_seen2_pearson_de: 0.8963535\n",
      "test_unseen_single_mse: 0.006403351215542191\n",
      "test_unseen_single_pearson: 0.8733936688966222\n",
      "test_unseen_single_mse_de: 0.18408565860914272\n",
      "test_unseen_single_pearson_de: 0.8733936688966222\n",
      "test_combo_seen0_pearson_delta: 0.54515773\n",
      "test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.16111111111111112\n",
      "test_combo_seen0_frac_sigma_below_1_non_dropout: 0.7888888888888889\n",
      "test_combo_seen0_mse_top20_de_non_dropout: 0.20955060174067816\n",
      "test_combo_seen1_pearson_delta: 0.46297064\n",
      "test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.23255813953488377\n",
      "test_combo_seen1_frac_sigma_below_1_non_dropout: 0.686046511627907\n",
      "test_combo_seen1_mse_top20_de_non_dropout: 0.29208765326197755\n",
      "test_combo_seen2_pearson_delta: 0.4713061\n",
      "test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.20263157894736844\n",
      "test_combo_seen2_frac_sigma_below_1_non_dropout: 0.7236842105263158\n",
      "test_combo_seen2_mse_top20_de_non_dropout: 0.2741148013033365\n",
      "test_unseen_single_pearson_delta: 0.38665983\n",
      "test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.2555555555555555\n",
      "test_unseen_single_frac_sigma_below_1_non_dropout: 0.7944444444444445\n",
      "test_unseen_single_mse_top20_de_non_dropout: 0.20053560519590974\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "gears_model_exprembedding.train(epochs=4,lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "Epoch 1 Step 1 Train Loss: 0.5027\n",
      "Epoch 1 Step 51 Train Loss: 0.3970\n",
      "Epoch 1 Step 101 Train Loss: 0.5084\n",
      "Epoch 1 Step 151 Train Loss: 0.5018\n",
      "Epoch 1 Step 201 Train Loss: 0.4152\n",
      "Epoch 1 Step 251 Train Loss: 0.5072\n",
      "Epoch 1 Step 301 Train Loss: 0.6210\n",
      "Epoch 1 Step 351 Train Loss: 0.4149\n",
      "Epoch 1 Step 401 Train Loss: 0.5023\n",
      "Epoch 1 Step 451 Train Loss: 0.4648\n",
      "Epoch 1 Step 501 Train Loss: 0.5172\n",
      "Epoch 1 Step 551 Train Loss: 0.4702\n",
      "Epoch 1 Step 601 Train Loss: 0.4625\n",
      "Epoch 1 Step 651 Train Loss: 0.5701\n",
      "Epoch 1 Step 701 Train Loss: 0.5528\n",
      "Epoch 1 Step 751 Train Loss: 0.4484\n",
      "Epoch 1 Step 801 Train Loss: 0.4492\n",
      "Epoch 1 Step 851 Train Loss: 0.4501\n",
      "Epoch 1 Step 901 Train Loss: 0.5188\n",
      "Epoch 1 Step 951 Train Loss: 0.4629\n",
      "Epoch 1 Step 1001 Train Loss: 0.3945\n",
      "Epoch 1 Step 1051 Train Loss: 0.4462\n",
      "Epoch 1 Step 1101 Train Loss: 0.4917\n",
      "Epoch 1 Step 1151 Train Loss: 0.5141\n",
      "Epoch 1 Step 1201 Train Loss: 0.6313\n",
      "Epoch 1 Step 1251 Train Loss: 0.5562\n",
      "Epoch 1 Step 1301 Train Loss: 0.5516\n",
      "Epoch 1 Step 1351 Train Loss: 0.4664\n",
      "Epoch 1 Step 1401 Train Loss: 0.4769\n",
      "Epoch 1 Step 1451 Train Loss: 0.4946\n",
      "Epoch 1 Step 1501 Train Loss: 0.4479\n",
      "Epoch 1 Step 1551 Train Loss: 0.5329\n",
      "Epoch 1: Train Overall MSE: 0.0131 Validation Overall MSE: 0.0130. \n",
      "Train Top 20 DE MSE: 0.1865 Validation Top 20 DE MSE: 0.3850. \n",
      "Epoch 2 Step 1 Train Loss: 0.5034\n",
      "Epoch 2 Step 51 Train Loss: 0.5071\n",
      "Epoch 2 Step 101 Train Loss: 0.4768\n",
      "Epoch 2 Step 151 Train Loss: 0.5178\n",
      "Epoch 2 Step 201 Train Loss: 0.4860\n",
      "Epoch 2 Step 251 Train Loss: 0.4458\n",
      "Epoch 2 Step 301 Train Loss: 0.4963\n",
      "Epoch 2 Step 351 Train Loss: 0.4552\n",
      "Epoch 2 Step 401 Train Loss: 0.4758\n",
      "Epoch 2 Step 451 Train Loss: 0.4490\n",
      "Epoch 2 Step 501 Train Loss: 0.4760\n",
      "Epoch 2 Step 551 Train Loss: 0.5983\n",
      "Epoch 2 Step 601 Train Loss: 0.5950\n",
      "Epoch 2 Step 651 Train Loss: 0.4584\n",
      "Epoch 2 Step 701 Train Loss: 0.4070\n",
      "Epoch 2 Step 751 Train Loss: 0.4539\n",
      "Epoch 2 Step 801 Train Loss: 0.5493\n",
      "Epoch 2 Step 851 Train Loss: 0.4795\n",
      "Epoch 2 Step 901 Train Loss: 0.4665\n",
      "Epoch 2 Step 951 Train Loss: 0.5061\n",
      "Epoch 2 Step 1001 Train Loss: 0.4560\n",
      "Epoch 2 Step 1051 Train Loss: 0.4630\n",
      "Epoch 2 Step 1101 Train Loss: 0.4629\n",
      "Epoch 2 Step 1151 Train Loss: 0.4574\n",
      "Epoch 2 Step 1201 Train Loss: 0.5110\n",
      "Epoch 2 Step 1251 Train Loss: 0.4721\n",
      "Epoch 2 Step 1301 Train Loss: 0.4761\n",
      "Epoch 2 Step 1351 Train Loss: 0.4708\n",
      "Epoch 2 Step 1401 Train Loss: 0.5560\n",
      "Epoch 2 Step 1451 Train Loss: 0.4743\n",
      "Epoch 2 Step 1501 Train Loss: 0.4286\n",
      "Epoch 2 Step 1551 Train Loss: 0.5022\n",
      "Epoch 2: Train Overall MSE: 0.0037 Validation Overall MSE: 0.0039. \n",
      "Train Top 20 DE MSE: 0.1107 Validation Top 20 DE MSE: 0.2759. \n",
      "Epoch 3 Step 1 Train Loss: 0.4584\n",
      "Epoch 3 Step 51 Train Loss: 0.5525\n",
      "Epoch 3 Step 101 Train Loss: 0.4956\n",
      "Epoch 3 Step 151 Train Loss: 0.4657\n",
      "Epoch 3 Step 201 Train Loss: 0.4272\n",
      "Epoch 3 Step 251 Train Loss: 0.5405\n",
      "Epoch 3 Step 301 Train Loss: 0.4972\n",
      "Epoch 3 Step 351 Train Loss: 0.4688\n",
      "Epoch 3 Step 401 Train Loss: 0.5356\n",
      "Epoch 3 Step 451 Train Loss: 0.5986\n",
      "Epoch 3 Step 501 Train Loss: 0.5517\n",
      "Epoch 3 Step 551 Train Loss: 0.5219\n",
      "Epoch 3 Step 601 Train Loss: 0.5066\n",
      "Epoch 3 Step 651 Train Loss: 0.4493\n",
      "Epoch 3 Step 701 Train Loss: 0.4930\n",
      "Epoch 3 Step 751 Train Loss: 0.4820\n",
      "Epoch 3 Step 801 Train Loss: 0.5185\n",
      "Epoch 3 Step 851 Train Loss: 0.5498\n",
      "Epoch 3 Step 901 Train Loss: 0.5081\n",
      "Epoch 3 Step 951 Train Loss: 0.4566\n",
      "Epoch 3 Step 1001 Train Loss: 0.5156\n",
      "Epoch 3 Step 1051 Train Loss: 0.4615\n",
      "Epoch 3 Step 1101 Train Loss: 0.4284\n",
      "Epoch 3 Step 1151 Train Loss: 0.4868\n",
      "Epoch 3 Step 1201 Train Loss: 0.4832\n",
      "Epoch 3 Step 1251 Train Loss: 0.4687\n",
      "Epoch 3 Step 1301 Train Loss: 0.5224\n",
      "Epoch 3 Step 1351 Train Loss: 0.4791\n",
      "Epoch 3 Step 1401 Train Loss: 0.5259\n",
      "Epoch 3 Step 1451 Train Loss: 0.6368\n",
      "Epoch 3 Step 1501 Train Loss: 0.4559\n",
      "Epoch 3 Step 1551 Train Loss: 0.4483\n",
      "Epoch 3: Train Overall MSE: 0.0037 Validation Overall MSE: 0.0043. \n",
      "Train Top 20 DE MSE: 0.1428 Validation Top 20 DE MSE: 0.3145. \n",
      "Epoch 4 Step 1 Train Loss: 0.5393\n",
      "Epoch 4 Step 51 Train Loss: 0.5261\n",
      "Epoch 4 Step 101 Train Loss: 0.5632\n",
      "Epoch 4 Step 151 Train Loss: 0.4686\n",
      "Epoch 4 Step 201 Train Loss: 0.4790\n",
      "Epoch 4 Step 251 Train Loss: 0.5134\n",
      "Epoch 4 Step 301 Train Loss: 0.4472\n",
      "Epoch 4 Step 351 Train Loss: 0.4910\n",
      "Epoch 4 Step 401 Train Loss: 0.5566\n",
      "Epoch 4 Step 451 Train Loss: 0.4615\n",
      "Epoch 4 Step 501 Train Loss: 0.4652\n",
      "Epoch 4 Step 551 Train Loss: 0.5019\n",
      "Epoch 4 Step 601 Train Loss: 0.4644\n",
      "Epoch 4 Step 651 Train Loss: 0.4852\n",
      "Epoch 4 Step 701 Train Loss: 0.5497\n",
      "Epoch 4 Step 751 Train Loss: 0.4686\n",
      "Epoch 4 Step 801 Train Loss: 0.5592\n",
      "Epoch 4 Step 851 Train Loss: 0.4499\n",
      "Epoch 4 Step 901 Train Loss: 0.4541\n",
      "Epoch 4 Step 951 Train Loss: 0.5488\n",
      "Epoch 4 Step 1001 Train Loss: 0.5205\n",
      "Epoch 4 Step 1051 Train Loss: 0.4772\n",
      "Epoch 4 Step 1101 Train Loss: 0.4378\n",
      "Epoch 4 Step 1151 Train Loss: 0.5219\n",
      "Epoch 4 Step 1201 Train Loss: 0.5413\n",
      "Epoch 4 Step 1251 Train Loss: 0.6089\n",
      "Epoch 4 Step 1301 Train Loss: 0.5368\n",
      "Epoch 4 Step 1351 Train Loss: 0.4774\n",
      "Epoch 4 Step 1401 Train Loss: 0.5956\n",
      "Epoch 4 Step 1451 Train Loss: 0.5063\n",
      "Epoch 4 Step 1501 Train Loss: 0.4703\n",
      "Epoch 4 Step 1551 Train Loss: 0.5333\n",
      "Epoch 4: Train Overall MSE: 0.0028 Validation Overall MSE: 0.0036. \n",
      "Train Top 20 DE MSE: 0.0770 Validation Top 20 DE MSE: 0.2444. \n",
      "Done!\n",
      "Start Testing...\n",
      "Best performing model: Test Top 20 DE MSE: 0.1863\n",
      "Start doing subgroup analysis for simulation split...\n",
      "test_combo_seen0_mse: 0.005275714004205333\n",
      "test_combo_seen0_pearson: 0.7512137558725145\n",
      "test_combo_seen0_mse_de: 0.14856412220332357\n",
      "test_combo_seen0_pearson_de: 0.7512137558725145\n",
      "test_combo_seen1_mse: 0.006271200855587458\n",
      "test_combo_seen1_pearson: 0.8697640216627787\n",
      "test_combo_seen1_mse_de: 0.1943574512464016\n",
      "test_combo_seen1_pearson_de: 0.8697640216627787\n",
      "test_combo_seen2_mse: 0.00460730879690106\n",
      "test_combo_seen2_pearson: 0.9377901\n",
      "test_combo_seen2_mse_de: 0.16574050054738396\n",
      "test_combo_seen2_pearson_de: 0.9377901\n",
      "test_unseen_single_mse: 0.0036431638153670873\n",
      "test_unseen_single_pearson: 0.8787600033813052\n",
      "test_unseen_single_mse_de: 0.19689212593463404\n",
      "test_unseen_single_pearson_de: 0.8787600033813052\n",
      "test_combo_seen0_pearson_delta: 0.659149\n",
      "test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.08888888888888889\n",
      "test_combo_seen0_frac_sigma_below_1_non_dropout: 0.8944444444444444\n",
      "test_combo_seen0_mse_top20_de_non_dropout: 0.2190017799536387\n",
      "test_combo_seen1_pearson_delta: 0.5779262\n",
      "test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.14767441860465116\n",
      "test_combo_seen1_frac_sigma_below_1_non_dropout: 0.8069767441860466\n",
      "test_combo_seen1_mse_top20_de_non_dropout: 0.2533984579752351\n",
      "test_combo_seen2_pearson_delta: 0.6408327\n",
      "test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.1\n",
      "test_combo_seen2_frac_sigma_below_1_non_dropout: 0.881578947368421\n",
      "test_combo_seen2_mse_top20_de_non_dropout: 0.17335402573409833\n",
      "test_unseen_single_pearson_delta: 0.43762198\n",
      "test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.2152777777777778\n",
      "test_unseen_single_frac_sigma_below_1_non_dropout: 0.859722222222222\n",
      "test_unseen_single_mse_top20_de_non_dropout: 0.2162578011242052\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "gears_model_gat.train(epochs=4,lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start Training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (6364) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgears_model_transformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 524\u001b[0m, in \u001b[0;36mGEARS.train\u001b[0;34m(self, epochs, lr, weight_decay)\u001b[0m\n\u001b[1;32m    518\u001b[0m     loss \u001b[38;5;241m=\u001b[39m uncertainty_loss_fct(pred, logvar, y, batch\u001b[38;5;241m.\u001b[39mpert,\n\u001b[1;32m    519\u001b[0m                       reg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muncertainty_reg\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    520\u001b[0m                       ctrl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctrl_expression, \n\u001b[1;32m    521\u001b[0m                       dict_filter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdict_filter,\n\u001b[1;32m    522\u001b[0m                       direction_lambda \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirection_lambda\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fct(pred, y, batch\u001b[38;5;241m.\u001b[39mpert,\n\u001b[1;32m    526\u001b[0m                   ctrl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctrl_expression, \n\u001b[1;32m    527\u001b[0m                   dict_filter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdict_filter,\n\u001b[1;32m    528\u001b[0m                   direction_lambda \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirection_lambda\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    529\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/snap/snapd-desktop-integration/current/Desktop/PerturbationPredictionUsingGeneNetworks/perturb/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/snap/snapd-desktop-integration/current/Desktop/PerturbationPredictionUsingGeneNetworks/perturb/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 136\u001b[0m, in \u001b[0;36mGEARS_Model.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    133\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb_pos(torch\u001b[38;5;241m.\u001b[39mLongTensor(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_genes)))\u001b[38;5;241m.\u001b[39mrepeat(num_graphs, )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers_emb_pos):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# pass in the positional embegginfs through a gcn with the co-expression graph.\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m     pos_emb \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mG_coexpress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mG_coexpress_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers_emb_pos) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;66;03m# relu till the last layer.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m         pos_emb \u001b[38;5;241m=\u001b[39m pos_emb\u001b[38;5;241m.\u001b[39mrelu()\n",
      "File \u001b[0;32m~/snap/snapd-desktop-integration/current/Desktop/PerturbationPredictionUsingGeneNetworks/perturb/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/snap/snapd-desktop-integration/current/Desktop/PerturbationPredictionUsingGeneNetworks/perturb/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/snap/snapd-desktop-integration/current/Desktop/PerturbationPredictionUsingGeneNetworks/perturb/lib/python3.10/site-packages/torch_geometric/nn/conv/transformer_conv.py:229\u001b[0m, in \u001b[0;36mTransformerConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, return_attention_weights)\u001b[0m\n\u001b[1;32m    225\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_value(x[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, H, C)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# propagate_type: (query: Tensor, key:Tensor, value: Tensor,\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m#                  edge_attr: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m                     \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alpha\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/torch_geometric.nn.conv.transformer_conv_TransformerConv_propagate_99o7hcsm.py:237\u001b[0m, in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, query, key, value, edge_attr, size)\u001b[0m\n\u001b[1;32m    225\u001b[0m             kwargs \u001b[38;5;241m=\u001b[39m CollectArgs(\n\u001b[1;32m    226\u001b[0m                 query_i\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery_i\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    227\u001b[0m                 key_j\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey_j\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m                 dim_size\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mdim_size,\n\u001b[1;32m    234\u001b[0m             )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# End Message Forward Pre Hook #########################################\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_i\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_j\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_j\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_j\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_j\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize_i\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# Begin Message Forward Hook ###########################################\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[0;32m~/snap/snapd-desktop-integration/current/Desktop/PerturbationPredictionUsingGeneNetworks/perturb/lib/python3.10/site-packages/torch_geometric/nn/conv/transformer_conv.py:275\u001b[0m, in \u001b[0;36mTransformerConv.message\u001b[0;34m(self, query_i, key_j, value_j, edge_attr, index, ptr, size_i)\u001b[0m\n\u001b[1;32m    273\u001b[0m out \u001b[38;5;241m=\u001b[39m value_j\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 275\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\n\u001b[1;32m    277\u001b[0m out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m*\u001b[39m alpha\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (6364) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "gears_model_transformer.train(epochs=4,lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. No Coexpression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gears_model_no_coexpress.train(epochs=4,lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. No Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gears_model_no_perturb.train(epochs=4,lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "perturb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
